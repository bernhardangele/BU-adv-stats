---
title: "Advanced Statistics"
author: "Bernhard Angele"
date: "Transformations and Logistic Regression"
output: pdf_document
---

Transformations
========================================================
- In some cases, our dependent variable will not be normally distributed
- Example: reaction times -- you get a long right tail of slow responses
    - Fixation times in eye movements are very similar

Example
========================================================
- For example, the probability density function for fixation durations might look like this:

```{r, echo = FALSE}
curve(dlnorm(x, 5.5, .3), from = 0.1, to = 1000)
```

Example data
========================================================
- Example experiment: how long do people look at swear words vs. non-swear words?
    - Let's assume that the true means are 250 ms for non swear words and 300 ms for swear words
- Let's generate data based on this assumption
```{r, echo = FALSE}
set.seed("11233")
# 60 subjects
word_condition <- factor(c(rep("swear word", 30), rep("non swear word", 30)))
# rlnorm: Generate random samples from the lognormal distribution
fixation_time <- c(rlnorm(n = 30, 
                          mean = log(250), 
                          sd = .3), 
                   rlnorm(n = 30,
                          mean = log(265), 
                          sd = .3))
swear_exp <- data.frame(word_condition, fixation_time)
write.csv(swear_exp, file = "swear_exp.csv", row.names = FALSE)
```

Running a linear model
========================================================
```{r, echo = FALSE}
linear_model <- lm(data = swear_exp, fixation_time ~ word_condition)
summary(linear_model)
```

Plotting the fitted line
========================================================
```{r, echo = FALSE}
with(swear_exp, plot.default(y = fixation_time, x = word_condition))
abline(linear_model)
```

Histogram of the residuals
========================================================
```{r, echo = FALSE}
hist(resid(linear_model), 10)
```
- Clearly not normal - it's right-skewed!
- But notice how robust the analysis is. We still find the effect!

Logarithmic transformations
=========================================================
- Better to run a proper model where the values as the dependent variable
  - This is our first step into the world of generalised linear models (GLM)
- The most common transformation uses the logarithm
  - Remember the logarithm from school? 
    - I hope you remember exponentiation: $x \cdot x \cdot x \cdot x = x^4$
    - Every logarithm has a base
      - The logarithm of a number is the exponent to which the base needs to be raised to produce that number
      - For example, the base $x$ logarithm of $x^4$ is $4$, since $x$ needs to be raised to the 4th power to produce $x^4$: $log_x(x^4) = 4$

The natural logarithm
==========================================================
- It doesn't really matter which base you use for your logarithm
- In mathematics, the algorithm to the base $e = `r round(exp(1),3)`$ called the *natural logarithm*. It is used frequently, since it has some very convenient properties. The logarithm to the base $e$ is called $log_e$ or $ln$.
- Just remember that $ln(e^x) = x$ and $e^{ln(x)} = x$.
- Also, remember that $x^{a+b} = x^a \cdot x^b$
    - Because of this, $e^{ln(x) + ln(y)} = x\cdot y$
    - And, the other way around, $ln(e^x \cdot e^y) = x+y$

Running a log model
========================================================
- Use `Compute` in SPSS to transform the predicted variable

```{r, echo = FALSE}
log_model <- lm(data = swear_exp, log(fixation_time) ~ word_condition)
summary(log_model)
```

Histogram of the residuals of the transformed model
========================================================
```{r, echo = FALSE}
hist(resid(log_model), 10)
```

- Much better! No longer skewed.

How to interpret a log model
========================================================
- We are transforming the predicted values using the natural logarithm ($ln$) here. This is a logarithm to the base $e = `r exp(1)`$. You may remember from school that $e^{ln(x)} = x$ and $ln(e^x) = x$.
- Important: You can only log-transform positive and non-zero values. If you have zeroes or negative values in your dependent variable, you have to transform it.
  - For example, to eliminate zeroes, you might add a tiny amount to all values.
- Log models are *multiplicative* rather than *additive*:
  - $e^{x+y} = e^x \cdot e^y$, and $e^{ln(x) + ln(y)} = x\cdot y$
- Our model formula: $ln(y_{i}) = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \varepsilon_i$
  - Let's rewrite that: $y_{i} = e^{\alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \varepsilon_i} = e^{\alpha} \cdot e^{\beta_1x_{i1}} \cdot e^{\beta_2x_{i2}}$

How to interpret a log model (2)
========================================================
- Example: Our swear word fixation time study
    - Fitted model: $ln(y_{i}) = 5.683 - .195 \cdot x_i$
    - Remember: We're using treatment contrasts. $x_i$ is 0 for non swear words and 1 for swear words
    - Predicted value for non swear words: $e^{5.683} \cdot e^{-.195 \cdot 0} = e^{5.683} =  293.82$
    - Predicted value for swear words: $e^{5.683} \cdot e^{-.195 \cdot 1} = 293.82 \cdot e^{-.195} = 293.82 * .823 = 241.81$
- Conclusion: Fixation times on swear words were 17.7% lower than fixation times on non swear words (*b* = -.195, *SE* = .074, *t* = -2.63, *p* = .011).

Summary
========================================================
- If our data aren't normally distributed, we can sometimes transform them to get them closer to normality.
- A very common transformation uses the logarithm (usually the natural logarithm with base $e$, but others can be used to)
- You can't log-transform negative or zero values, so if your data contain any of those, you have to remove them or transform them (e.g. by adding a constant value) before doing the log-transformation
- The regression equation of a log-model is additive in its log format, but multiplicative when transformed back into raw values     - e.g. $log(y) = \alpha + \beta X_i \leftrightarrow y = e^{\alpha + \beta X_i} = e^{\alpha} * e^{\beta X_i}$

Logistic regression
========================================================
- What if we have a dichotomous dependent variable?
    - Yes vs. no, error vs. no error, alive vs. dead, pregnant vs. not pregnant
- Our example (from A. Johnson): Factors that make (or don't make) you fail your driving test
- 90 candidates
- Dependent variable: `Driving.Test`: Yes or No
- Predictor variables:
    - `Practice`: in hours
    - `Emergency.Stop`: Whether the candidate performed the emergency stop (yes or no)
    - `Examiner`: How difficult the examiner is (on a scale from 0 = easy to 100 = extremely difficult)
    - `Cold.Remedy`: How many doses of cold remedy the candidate had before the test

Examining the data
=======================================================
- These are our data (first 6 rows):

```{r, results='asis', echo =FALSE}
library(knitr)
driving_tests <- read.csv("driving_tests.csv")
kable(head(driving_tests))
```

Where to start?
=========================================================
- First, let's recode our dependent variable, replacing "No" with 0 and "Yes" with 1
```{r, results='asis', echo =FALSE}
driving_tests$Driving.Test <- as.numeric(driving_tests$Driving.Test) - 1
kable(head(driving_tests))
```

Let's plot it
=========================================================
- Now we can make a plot of the situation
- Let's just consider the number of practice hours right now and ignore the other predictors

```{r, echo = FALSE}
plot(x = driving_tests$Practice, y = driving_tests$Driving.Test, ylab = "Driving Test (+ jitter)",  xlab = "Practice (in hours)")
```

Adding some jitter
=========================================================
- Since the y-value is either 0 or 1, a lot of the points are plotted on top of each other (overplotting)
- To avoid this, we add or subtract a small amount to the y value ("jitter")
  - Now we can see the pattern a little better.

```{r, echo = FALSE}
plot(x = driving_tests$Practice, y = jitter(driving_tests$Driving.Test, factor = .25), ylab = "Driving Test (+ jitter)",  xlab = "Practice (in hours)")
```

What do we want to estimate?
=========================================================
- The regression line gives us the expected $\hat{y}$-value (the mean $y$-value) for each value $x_i$
- In this case, if we code our dependent variable as 0 and 1, the mean (expected value) at each x value will give us the conditional probability $E(Y|x_i) = \pi_i$ of our event. 
  - Note that we're talking about estimating the theoretical probability in the population, hence the Greek letter. This is not the mathematical constant $\pi = `r pi`$
- Then our model can give us predictions like this: What's the probability of passing the test given that I've had (at most) 20 hours of practice, did my emergency stop, had at most an average examiner (50) and had only one cup of cold remedy?
    
Predicting probabilities
==========================================================
- We can try a standard linear function
- This would be our model then: $P(Y_i = 1) = \alpha + \beta X_{i} + \varepsilon_i$

```{r, echo = FALSE}
plot(x = driving_tests$Practice, y = jitter(driving_tests$Driving.Test, factor = .25), ylab = "Driving Test (+ jitter)",  xlab = "Practice (in hours)")
lines(abline(lm(data = driving_tests, Driving.Test ~ Practice)))
```

Problems with a linear prediction
==========================================================
- But it's not very well suited!
  - It predicts probabilities $P(Y_i = 1) < 0$ and $P(Y_i = 1) > 1$ for some $x$-values!
  - Also, the error variance is clearly not equal for all $x$-values
  - And we really can't pretend that our data are normally distributed.

Using a different regression function
=========================================================
- We would like a function that makes sure our linear predictor $\alpha + \beta x_i$ stays between 0 and 1
 - This **link function** $P$ would apply to our linear predictor and transform it into a probability: $\pi_i = P(\alpha + \beta x_i)$
- If the function is monotone, meaning that it consistently increases as $\alpha + \beta x_i$ gets larger, that would be advantageous, since the inverse of the function $P^{-1}$ then links each value predicted by the linear model with the probability  $P^{-1}(\pi_i) =\alpha + \beta x_i$
- There are two link functions that are most commonly used: the cumulative probability function of the normal distribution and the logistic function

The cumulative probability function of the normal distribution
=========================================================
- Look up the function definition on Wikipedia, if you really care.

```{r, echo = FALSE}
curve(pnorm(x), -3,3)
```

The logistic distribution function
=========================================================
- Here, the definition is simpler: $\Lambda(z) = \frac{1}{1+e^{-z}}$
  - Where $z$ is a *z*-value, and $e$ is the constant $e = `r exp(1)`$. $\Lambda$ is the capital letter Lambda.

```{r, echo = FALSE}
curve(plogis(x), -3,3)
```

Applying the link functions
========================================================
- Applying the cumulative normal distribution function gives you the **linear probit model**.
- Applying the logistic distribution gives you the **linear logit model**, also called **linear logistic regression**:
$$\pi_i = \Lambda(\alpha + \beta x_i) = \frac{1}{1+e^{-(\alpha+\beta x_i)}}$$
- Both functions work equally well, but the logistic distribution function has the advantage that the transformed values are directly interpretable:
  - Rewrite the above equation and you get **odds**: $$\frac{\pi_i}{1-\pi_i} = e^{\alpha+\beta x_i}$$
    
Probability and odds
========================================================
- Very popular in betting, since they make it easy to estimate the payout
- $Odds = \frac{P}{1-P}$
- For example: 
      - $P = .5$ gives you even odds $\frac{.5}{.5} = 1/1$
      - $P = .25$ gives you $\frac{.25}{.75} = 1/3$
      - $P = .75$ gives you $\frac{.75}{.25} = 3/1$
- Odds are nice because they aren't bounded, but for high probabilities they get very large very quickly ($P = .99 \Leftrightarrow Odds = 99/1$) and for small probabilities, they get very small very quickly ($P = .01 \Leftrightarrow Odds = 1/99$)

Log odds (logits)
=========================================================
- Just transform our odds (just like we did with our continuous fixation time variable earlier) by taking the natural logarithm: $logit = ln(Odds) = ln(\frac{P}{1-P})$
- Now we have a dependent measure that is suitable for linear relationships
- Log odds is just what we happen to get if we apply the natural logarithm on both sides of our logistic regression equation from earlier:
$$
\begin{aligned}
\frac{\pi_i}{1-\pi_i} &= e^{\alpha+\beta x_i}\\
\leftrightarrow ln(\frac{\pi_i}{1-\pi_i}) &= ln(e^{\alpha+\beta x_i})\\
\leftrightarrow ln(\frac{\pi_i}{1-\pi_i}) &= \alpha+\beta x_i\\
\end{aligned}
$$

The logistic model
==========================================================
- So, our full model (including errors) is
$$logit_i = ln(\frac{\pi_i}{1-\pi}) = \alpha + \beta x_i + \varepsilon_i$$
- If we want to get back from logits to probabilities, we can do that by reversing the transformation:
$$\pi_i = \frac{1}{1+e^{-(logit_i)}}$$

How do you fit a logistic regression line?
=========================================================
- Sadly, you can't fit the logistic regression model using the least squares approach (the errors still aren't nromally distributed or homoscedastic)
- We can evaluate the **likelihood** of the parameters instead
- In statistics, probability and likelihood aren't the same:
  - Probability: observations given parameters
  - Likelihood: parameters given observations

Calculating likelihood
========================================================
- Very simple example: Let's assume we have the following data from flipping a coin: $Y = (H H H T H T H H)$
- Likelihood is the product of all the probabilities given a certain parameter value. In this case, we are trying different parameter values for the probability: 
    -We're interested in the population parameter, so again, our population probability of observing "heads" (ignoring order) will be called $\pi$:
- $\pi = .5$: $L(Y|\pi = .5) = .5 \cdot .5 \cdot .5 \cdot .5 \cdot .5 \cdot .5 \cdot .5 \cdot .5 = .5^8 = .00039$
- $\pi = .25$: $L(Y|\pi = .25) = .25 \cdot .25 \cdot .25 \cdot .75 \cdot .25 \cdot .75 \cdot .25 \cdot .25 = .25^6 \cdot .75^2 = .00014$
    - $\pi = .25$ has a lower likelihood than $\pi = .5$.
- Let's try $\pi = .75$: $L(Y|\pi = .75) = .75 \cdot .75 \cdot .75 \cdot .25 \cdot .75 \cdot .25 \cdot .75 \cdot .75 = .75^6 \cdot .25^2 = .00111$
    - This is the highest one yet.

The likelihood function for logistic regression
========================================================
- Do you see a pattern here? For each element $Y_i$ in $Y$, the likelihood of $\pi$ is either
    - $L(Y_i|\pi) = \pi$ if $Y_i = H$, (e.g. $.75$ for $\pi = .75$), or
    - $L(Y_i|\pi) = 1 - \pi$ if $Y_i = T$, (e.g. $.25$ for $\pi = .75$)
- Then you get the likelihood for the full data set $Y$ by multiplying all the individual likelihoods
    - $L(Y|\pi) = \prod_{i = 1}^N{L(Y_i|\pi)}$
- You can simplify this a bit if you replace H with 1 and T with 0:
    -  $L(Y_i|\pi) = \pi^{Y_i} \cdot (1-\pi)^{(1-Y_i)}$
    - And combining the two equations above:
        - $L(Y|\pi) = \prod_{i = 1}^N\pi^{Y_i} \cdot (1-\pi)^{(1-Y_i)}$

Maximum likelihood
========================================================
- Likelihood gets a little unwieldy -- lots of very small numbers
    - Solution: take the log (who would have thought?)
        - Added bonus: Now our multiplication becomes a sum (remember that from calculus?)
          $log\;likelihood = \sum_{i = 1}^N Y_i\cdot ln(\pi) + (1-Y_i)\cdot ln(1-\pi)$
- Now we can simply try different values of $\pi$ until we find the one with the maximum likelihood
    - Remember that in logistic regression, $\pi$ is defined by our regression equation: $\pi = \frac{1}{1 + e^{-(\alpha + \beta x_{i1} + \varepsilon_i)}}$ 
    - Instead of simply trying different values of $\pi$, we have to try different values for $\alpha$, $\beta_1$, etc. and compute $\pi$. This gets to be quite a lot of work.
    
Fitting the model through an iterative process
=======================================================
- Trying different values might not seem particularly elegant, but this is essentially what R or SPSS do -- no simple solution like with least-squares regression or ANOVA exists
- This is (relatively) processing-intensive, which is one reason why psychologists didn't use logistic regression in the statistics-by-hand era.
      
Log likelihood as an indicator of model fit
=======================================================
- The log likelihood ($LL$) of the final model is an indicator of how well the model fit the data, just like $R^2$.
    - In fact, there are several ways to estimate $R^2$ from the log likelihood
- Log likelihood also enables us to make model comparisons
    - The test statistic in that case is $\chi^2$ -- more about that later
- Another measure is *deviance*, which is simply $-2\cdot LL$
    - Conceptually, deviance is like the residual variance.
        - In the case of deviance, lower is better, of course.
- Closely related to this is Akaike's Information Criterion (AIC), which is $-2\cdot LL + 2\cdot \text{number of parameters}$ (lower is better, so adding parameters makes the AIC worse)

How to fit the model in SPSS
========================================================
- You can't do this in Excel anymore.

```{r, echo = FALSE}
driving_tests$Emergency.Stop <- factor(driving_tests$Emergency.Stop, levels = c(1,0), labels = c("Yes", "No"))

#driving_tests$Cold.Remedy <- factor(driving_tests$Cold.Remedy)

driving_glm <- glm(data = driving_tests, 
                   formula = Driving.Test ~ Practice + Emergency.Stop + Examiner + Cold.Remedy, 
                   family = binomial(link = "logit"))
```

- To see how it works, watch Andy Johnson's video on myBU.

Plotting the model fit
==========================================================
```{r, echo = FALSE}
plot(x = driving_tests$Practice, y = jitter(driving_tests$Driving.Test, factor = .25), ylab = "Driving Test (+ jitter)",  xlab = "Practice (in hours)")
curve(predict(driving_glm, newdata = data.frame(Practice = x, Emergency.Stop = driving_tests$Emergency.Stop[1], Examiner = mean(driving_tests$Examiner), Cold.Remedy = mean(driving_tests$Cold.Remedy)), type="resp"),add=TRUE)
```

Model summary
==========================================================
- The coefficients

```{r, echo=FALSE, results='asis'}
kable(coef(summary(driving_glm)))
```

Significance tests for coefficients
=============================================================
- Analogous to linear regression: *b* value, *SE*, significance test
- Using the **Wald** statistic instead of *t* tests
    - $z = \frac{b}{SE_b}$
    - We can use *z*-values because our dependent variable comes from the binomial distribution. In the binomial distribution, the variance is always a function of the mean, so we don't have to estimate it from the sample (and since we're not using the sample variance to estimate the population variance, we don't need to use a t-test).

Interpreting the b values
==========================================================
- If you take the exponential of the coefficients , you get an **odds ratio**
- Odds ratio = Odds after a unit change in the predictor divided by the original odds
- Example: According to our model, each hour of practice increases the odds of passing the test by a factor of $e^{0.12959} = `r exp(.12959)`$
    - e.g. if the odds were even (1/1) for X hours of practice, they would be slightly better than even (1.138/1) for X+1 hours of practice
- On the other hand, each "unit" of examiner difficulty decreases the odds of passing the test by a factor of $e^{-0.03485} = `r exp(-.03485)`$
    - e.g. if the odds were even (1/1) for an instructor with a difficulty of X, they would be slightly worse than even (0.9658/1) for an instructor with a difficulty of X+1

Model comparisons (LRT)
==========================================================
- Deviance has some neat properties
    - We can compare likelihoods just like we compared mean squares in the *F*-test: by dividing them
    - That is, we compute a likelihood ratio: $LR = \frac{L_{baseline}}{L_{new}}$, where *baseline* is the simpler model and *new* the more complex model.
    - Now we can convert this likelihood ratio into a deviance: $deviance_{LR} = -2\frac{ln(L_{baseline})}{ln(L_{new})} = deviance_{baseline} - deviance_{new}$
    - And now the most fun part: If the $H_0$ that the two models explain the data equally well is true, this likelihood-ratio deviance is distributed according to a $\chi^2$ distribution.
    - The $\chi^2$ distribution has one parameter, degrees of freedom
    - $df = k_{new}$ - $k_{baseline}$, where $k$ is the number of parameters (including the intercept)

Model comparisons
==========================================================
- Now we can get a *p*-value! This is called the **Likelihood ratio test (LRT)**
- So, if we want to test if the model is better than a model with just the intercept, we can do an LRT
- $\chi^2 = deviance_{null} - deviance_{model} = 124.366 - 82.572 = 41.794$
- $df = k_{null} - k_{model} = 89 - 85 = 4$ 
- $p(\chi^2(4) \geq 41.794) < .01$
- This is equivalent to the overall *F*-test for the model.
- SPSS calls this the "Omnibus test"

Model comparisons (2)
========================================================
- We can use model comparisons to test how specific predictors contribute to the whole model (analogous to the *F*-tests in linear regression)
```{r, echo = FALSE}
library(car)
Anova(driving_glm)
```

Model comparisons (3)
========================================================
- This analysis of deviance follows the same logic as the ANOVA in the linear regression case
- You can do Type I, Type II, and Type III LRT tests (they are not sums of squares in this case)
- The LRTs are better tests than the Wald tests, since the Wald tests might be prone to overinflating the SE, leading to Type II error.

Diagnostics and assumption tests
========================================================
- We do not assume normality (so nothing to test for that one)
- All the influence measures from linear regression work in logistic regression as well
- Multicollinearity:
  - You can get Variance Inflation Factors (VIFs) and Tolerance etc. in SPSS if you fit the standard linear model first
    - Of course, don't interpret the coefficients that you get!
- You can also take a look at the Hosmer-Lemeshow test (see Andy Johnson's video)


Looking at logistic regression residuals
========================================================
- Can't test if they are normally distributed (because they are not)
- But look out for very large residuals
- You can get still get standardised and studentised residuals.
    - Look out for cases that are far away from 0.
    
```{r, results='asis', echo = FALSE}
driving_residuals <- rstandard(driving_glm)
plot(driving_residuals)
```


Reporting it
=============================================================
A logistic regression was conducted where the dependent variable was passing a driving test and the predictor variables were hours of practice, whether an emergency stop was successfully executed, how much the examiner was difficult, and amount of 'cold remedy drinks' consumed.  90 cases were examined and the model was found to significantly predict whether the test was passed (omnibus chi-square = 41.79, df=4, p<.001). that practice and examiner were the only two variables that reliably predicted if the driving test was passed. Increases in practice was associated with increased rate of passing (odds of passing increased by 1.14 per hour of practice, *b* = .130, SE = .03, *z* = 4.28, *p* < .01). Increases in the examiner difficulty reduced the rate of passing (odds of passing decreased by 0.96 per unit of difficulty rating, *b* = -.00349, SE = .013, *z* = -2.679, *p* < .01). None of the other predictors reached significance (all *p*s > .05). There were no issues due to multicollinearity or influential cases.

Summary: Logistic regression
===============================================================
- If you have a dichotomous outcome variable (e.g. Yes vs. No, Heads vs. Tails, etc.), you cannot use standard linear regression
- Instead, you must use a $link function$ to convert your predicted values into probabilities and vice versa. The most commonly used are the cumulative distribution function (CDF) of the normal distribution and the CDF of the logistic distribution
    - if you use the CDF of the normal distribution, you are fitting a **probit** model
    - if you use the CDF of the logistic distribution, you are fitting a **logit** or logistic regression model
- The logit model has the advantage that you can interpret the coefficients directly as changes in log odds $= ln(frac{\pi_i}{1-\pi_1})$
- Just like when you are using a log transformation, the coefficients are additive as far as log odds are concerned, but multiplicative when you convert the log odds into odds.
- In both cases, you use the Wald test as a significance test for coefficients (analogous to *t*-tests in standard linear regression) and the likelihood ratio test (LRT, analogous to *F*-tests in standard linear regression) in order to test the significance of predictors.
    - Since we don't have to estimate the error variance from the data, the Wald statistic is $z$-distributed and the LRT statistic is $\chi^2$ distributed.
    - Just like the standard oneway/multiway ANOVA is a special case of linear regression with only categorical predictors, the $\chi^2$-test is a special case of logistic regression with only categorical predictors.
