Advanced Statistics
========================================================
author: Bernhard Angele
date: Dummy-Variable regression


Discrete variables
==================================================================
- Is it possible to perform a regression analysis with discrete (instead of continuous) variables?
  - **Yes!**
  - In fact, when you ask SPSS or R to perform an ANOVA, what it does behind the scenes is run a linear model and then do model comparisons using the F-Test
  - How can we put discrete (that is, non-numerical) variables into the regression model?
      - We make up numbers, of course.
      - This is called *dummy coding*.
      
Example
=================================================================
```{r, echo=FALSE}
library(car)
n <- 30
intercept <- 100
options(digits = 3, scipen = 4)

set.seed("126")
cat_weight <- rnorm(n, mean = 5, sd = 1) # cat weight in kg
cat_age <- rnorm(n, mean = 60, sd = 24) # cat age in months
cat_breed <- factor(c(rep("Shorthair",n/2), rep("Manx", n/2)))

food_eaten <- intercept + 20*(cat_weight - mean(cat_weight)) + 2 + .5*(cat_age - mean(cat_age)) + rnorm(n, mean = 0, sd = 10)
catfood <- data.frame(CatWeight = cat_weight, FoodEaten = food_eaten)
catfood_age <- data.frame(CatWeight = cat_weight, CatAge = cat_age, FoodEaten = food_eaten)


print_p <- function(p){
  if(p < .01) return("*p* < .01")
  if(p <= .05) return(paste("*p* =", p))
  if(p > .05) return("*p* > .05")
}

catfood_age2 <- catfood_age
catfood_age2$CatWeight_centered <- scale(catfood_age$CatWeight, scale = FALSE)
catfood_age2$CatAge_centered <- scale(catfood_age$CatAge, scale = FALSE)

catfood_breed <- with(catfood_age2, data.frame(CatWeight = CatWeight_centered, CatAge = CatAge_centered, CatBreed = cat_breed, FoodEaten = food_eaten))
catfood_breed$FoodEaten <- catfood_breed$FoodEaten + ifelse(cat_breed == "Manx", 15, -15)
```
- Let's just stay with the cat data for a little bit longer
- Let's say our cats came from two breeds, shorthair and manx.
    - Does breed have an influence on food eaten?
    - The corresponding file is on myBU (`catfood_breed.sav`; first 6 rows of the table shown)
    
```{r, results='asis', echo = FALSE}
kable(head(catfood_breed))
write.csv(catfood_breed, file = "catfood_breed.csv")
```

We could do a t-test
=================================================================
```{r, echo = FALSE}
t.test(formula = FoodEaten ~ CatBreed, data = catfood_breed, var.equal = TRUE)
```

Or we could assign dummy values and do a regression
==================================================================
- Let's recode our variable and assign "0" to all Shorthairs and "1" to all Manxes
- We can do this in SPSS under `Data` --> `Recode into Different Variables...`
    - We recode our cat breed variable into a variable called `dummy`, containing 0s and 1s

## Dummy analysis
```{r, echo = FALSE}
# assign 0 to all shorthairs and 1 to all manxs
catfood_breed$dummy <- ifelse(catfood_breed$CatBreed == "Shorthair", 0, 1)
kable(coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed))))
```

Let's plot the situation
=================================================================
```{r, echo = FALSE}
plot(x = catfood_breed$dummy, y = catfood_breed$FoodEaten, xlab = "Dummy variable", ylab = "Food Eaten")
abline(lm(formula = FoodEaten ~ dummy, data = catfood_breed))
```

Interpreting dummy variables
=================================================================
- Now, the intercept is the mean for group "shorthair" and the slope gives the difference between group "shorthair" and group "manx"
- Remember the regression equation: $y_i = \alpha + \beta_1 x_{i} + \varepsilon_i$
- If $x_i$ is 0 (shorthair group), the predicted value y is the intercept ($\alpha$)
- If $x_i$ is 1 (manx group), the predicted value is the sum of the intercept ($\alpha$) and the slope ($\beta_1$). 

Different dummy values
================================================================
- Nobody forces us to set the values to 0 and 1
- We could use any values we want, e.g. 99 and 23419 (although have fun interpreting *that* equation)
- -1 and 1 might be reasonable. We use `Data` --> `Recode into Different Variables...` to make a new dummy variable containing -1s for Shorthair and 1s for Manxes.
```{r, echo = FALSE}
catfood_breed$dummy <- ifelse(catfood_breed$CatBreed == "Shorthair", -1, 1)
```

Re-run the analysis
================================================================

```{r, results='asis', echo = FALSE}
kable(coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed))))
```

- Note that the numbers are different: now the intercept represents the grand (overall) mean.
- The slope tells you how far the means of shorthair and manx are from the grand mean.
     - The prediction for shorthair is $`r coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["(Intercept)","Estimate"]` - `r coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["dummy","Estimate"]` = `r coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["(Intercept)","Estimate"] - coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["dummy","Estimate"]`$
     - The prediction for manx is $`r coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["(Intercept)","Estimate"]` + `r coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["dummy","Estimate"]` = `r coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["(Intercept)","Estimate"] + coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["dummy","Estimate"]`$
    - The *t* and *p* values are exactly the same.

Continuous covariates
=================================================================
- For example, someone's IQ might influence their performance in a memory experiment
- By including the covariate in the model, we can explain more variance (and take it out of the error variance)

Discrete and continuous factors in one model
=================================================================
- Still with the cats! It's just a fantastic example, right?
- Now that we have breed coded as a dummy variable, there is no reason why we can't add other continuous predictor variables and make it a multiple regression.
- Let's see if there is an effect of cat breed if we enter cat age and cat weight into the analysis as covariates (we don't care about their interaction, which we have shown to not be significant in the first place).

Use the Linear Regression module
================================================================
```{r, echo = FALSE}
catfood_breed_lm <- lm(data = catfood_breed, formula = FoodEaten ~ CatWeight + CatAge + dummy)
kable(coef(summary(catfood_breed_lm)))
```

- We can reject the null hypothesis. Cat Breed (represented by our dummy variable) does indeed have a significant effect on food eaten.
- Note that, since we are coding Cat Breed as -1 and 1 (and the continuous predictors are centred), our intercept is still the grand mean of FoodEaten. Our dummy variable tells us the distance between the mean of the Shorthair group and the grand mean (on the left) and the distance between the Manx group and the grand mean (on the right)


Using the General Linear Model (Univariate) module
================================================================
- We can do an F-test by using the `Univariate` test from the `General Linear Model` module (in the `Analyze` menu)
```{r, echo = FALSE}
Anova(catfood_breed_lm, type = "III")
```

What does a model with a dummy variable and a covariate look like?
================================================================

- For simplicity, let's use a model that just has CatWeight and the dummy variable

```{r, echo = FALSE}
catfood_breed_weight_only_lm <- lm(data = catfood_breed, formula = FoodEaten ~ CatWeight + dummy)

kable(coef(summary(catfood_breed_weight_only_lm)))
```

Plot for a model with a dummy variable and a covariate
================================================================
```{r, echo = FALSE}
library(ggplot2)
qplot(data = catfood_breed, y = FoodEaten, x = CatWeight, colour = factor(dummy)) + labs(colour = "Dummy variable")
```

Fitting a line for all points
================================================================
```{r, echo = FALSE}
library(ggplot2)
lm_nodummy <- lm(data = catfood_breed, FoodEaten ~ CatWeight)
intercept <- lm_nodummy$coefficients[1]
slope <- lm_nodummy$coefficients[2]


qplot(data = catfood_breed, y = FoodEaten, x = CatWeight, colour = factor(dummy)) + labs(colour = "Dummy variable") + geom_abline(intercept = intercept, slope = slope)
```

The effect of the dummy variable
================================================================
```{r, echo = FALSE}
library(ggplot2)
lm_nodummy <- lm(data = catfood_breed, FoodEaten ~ CatWeight)
intercept <- lm_nodummy$coefficients[1]
intercept_dummy1 <- intercept + catfood_breed_weight_only_lm$coefficients[3]
intercept_dummy2 <- intercept - catfood_breed_weight_only_lm$coefficients[3]
slope <- lm_nodummy$coefficients[2]


qplot(data = catfood_breed, y = FoodEaten, x = CatWeight, colour = factor(dummy)) + labs(colour = "Dummy variable") + geom_abline(intercept =  intercept_dummy1, slope = slope, colour = 5) + geom_abline(intercept =  intercept_dummy2, slope = slope, colour = 2)
```

Allowing different slopes
================================================================
```{r, echo = FALSE}

qplot(data = catfood_breed, y = FoodEaten, x = CatWeight, colour = factor(dummy)) + labs(colour = "Dummy variable") + geom_smooth(method = "lm", se = FALSE)
```

How to fit a model with different slopes
==============================================================
- One slope per level of the dichotomous variable
- Add the interaction between dummy and covariate to the model

```{r, echo = FALSE}
catfood_breed_weight_interact_lm <- lm(data = catfood_breed, formula = FoodEaten ~ CatWeight * dummy)
kable(coef(summary(catfood_breed_weight_interact_lm)))
```

- In this case, the interaction is not significant. We can't reject the null hypothesis that the slopes are the same for each level of the dichotomous variable (despite the slight strend in the data).

Factors with more than two levels
=========================================================
- What if we have three (or more) groups that we want to compare in our regression model?
    - Let's start with three to keep things simple

        
Example
========================================================
- Enough about cats, let's talk about dogs!
- In this ficticious example, let's assume we are testing 45 dogs to see how many object names they know (e.g. when you tell them to bring you a "ball", "stick", etc., do they bring you the correct object or a random one?)
- Our sample contains 15 beagles, 15 border collies, and 15 terriers.
- Let's assume that the true means $\mu_i$ for each breed are:

| Breed        | Number of object names known|
|-------------:|----------------------------:|
| Beagle       |                           10|
| Border Collie|                           60|
| Terrier      |                           15|

Example (2)
========================================================
- In a regression model, we would want get three lines, one per group
    - Of course, there is no covariate in this model, so the lines should be horizontal
- The lines should reflect the differences between the means:

| Comparison              | Difference                  |
|------------------------:|----------------------------:|
| Border Collie -- Beagle |                           50|
| Border Collie -- Terrier|                           35|
| Terrier -- Beagle       |                            5|

Dummy contrasts for three groups
====================================================================
- How do we make contrasts in this situation?
    - Remember, the dummy contrasts for two groups had one level coded as 0 (the baseline) and one coded as 1
    - The coefficient for the dummy contrasts then told us how the level coded as 1 differed from the baseline
    - We can apply the same scheme here. Let's make Beagles the baseline, because they are fantastic dogs:
    
| Breed        | X1                          |X2                           |
|-------------:|----------------------------:|----------------------------:|
| Beagle       |                            0|                            0|
| Border Collie|                            1|                            0|
| Terrier      |                            0|                            1|
    
- Note that we need **two** contrasts here to describe **three** regression lines
    - The baseline is described by the intercept


```{r, echo = FALSE}
# Seed for random number generators, so that we all get the same results
set.seed("67")
# Column 1: Breed - repeat each breed name 15 times, then combine
breed <- c(rep("Beagle", 15), rep("Border Collie", 15), rep("Terrier", 15))
# Column 2: Objects - repeat each true group mean 15 times, then combine
objects <- c(rep(10, 15), rep(60, 15), rep(15, 15))

# Add centered covariate
dog_iq <-rnorm(n = 45, mean = 0, sd = 15)

# Add random noise to the objects scores
objects <- objects + 0.15 * dog_iq + rnorm(n = 45, mean = 0 , sd = 6)
# for more realism, round the objects scores to full integers
# (what does it mean if a dog knows a fraction of an object?)
objects <- round(objects, digits = 0)
objects[objects < 0] <- 0
# Combine into data frame
dogs <- data.frame(breed, objects, dog_iq)
write.csv(dogs, "dogs.csv")
```

The data
========================================================
- We're going to simulate getting a sample from the populations I just described.
    - Note that sample means are not neccessarily the same as the population means
        - Which is of course the reason why we do inferential statistics in the first place!
    - The data are in the file `dogs.sav` on myBU
- Let's calculate the means:

## Means table
```{r, echo = FALSE}
# get the means for each breed
means_table <- data.frame(tapply(X = dogs$objects, INDEX = dogs$breed, FUN = mean))
colnames(means_table) <- c("Mean number of objects known")
kable(means_table)
```

Adding contrasts
========================================================
- We can use `Transform -> Recode into Different Variables...` in SPSS to make the contrasts.

    - *x_1* will be 1 for all "Border Collie" cases, and 0 otherwise
    - * x_2* will be 1 for all "Terrier" cases, and 0 otherwise
        - This will make "Beagle" the baseline


```{r, results='asis', echo=FALSE}
library(knitr)
contrast_matrix <- contr.treatment(3)
colnames(contrast_matrix) <- c("x1","x2")
rownames(contrast_matrix) <- c("Beagle","Border Collie","Terrier")
kable(contrast_matrix)
```

How contrasts work
=========================================================
- Remember the linear regression equation:
- $y_{i} = \alpha + \beta_1 x_{1} + \beta_2 x_{2} + \varepsilon_i$
- i.e. the predicted value for $y_i$ is $\hat{y_i} = \alpha + \beta_1 x_{1} + \beta_2 x_{2}$
- Now let's substitute in the values from the table if breed is "Beagle":
```{r, results='asis', echo=FALSE}
contrast_matrix <- contr.treatment(3)
colnames(contrast_matrix) <- c("x1","x2")
rownames(contrast_matrix) <- c("Beagle","Border Collie","Terrier")
kable(contrast_matrix)
```

- $\hat{y_{i}} = \alpha + \beta_1 \times 0 + \beta_2 \times 0 = \alpha$
- The predicted value for the Beagle group is $\alpha$, the intercept
- That means that in this analysis, the intercept will reflect the mean for the Beagle group

How contrasts work (2)
=========================================================
- The predicted value for $y_i$ is still $\hat{y_i} = \alpha + \beta_1 x_{1} + \beta_2 x_{2}$
- Now let's substitute in the values from the table if breed is "Border Collie":

```{r, results='asis', echo=FALSE}
contrast_matrix <- contr.treatment(3)
colnames(contrast_matrix) <- c("x1","x2")
rownames(contrast_matrix) <- c("Beagle","Border Collie","Terrier")
kable(contrast_matrix)
```

- $\hat{y_{i}} = \alpha + \beta_1 \times 1 + \beta_2 \times 0 = \alpha + \beta_1$
- The predicted value for the Border Collie group is $\alpha + \beta_1$, i.e. the sum of the intercept and the first slope $\beta_1$
- That means that in this analysis, the slope $\beta_1$ will reflect the difference between the mean for the Border Collie group and the mean for the Beagle group ($60 - 10 = 50$)

How contrasts work (2)
=========================================================
- The predicted value for $y_i$ is still $\hat{y_i} = \alpha + \beta_1 x_{1} + \beta_2 x_{2}$
- Now let's substitute in the values from the table if breed is "Terrier":
```{r, results='asis', echo=FALSE}
contrast_matrix <- contr.treatment(3)
colnames(contrast_matrix) <- c("x1","x2")
rownames(contrast_matrix) <- c("Beagle","Border Collie","Terrier")
kable(contrast_matrix)
```

- $\hat{y_{i}} = \alpha + \beta_1 \times 0 + \beta_2 \times 1 = \alpha + \beta_2$
- The predicted value for the Border Collie group is $\alpha + \beta_2$, i.e. the sum of the intercept and the second slope $\beta_2$
- That means that in this analysis, the slope $\beta_1$ will reflect the difference between the mean for the Terrier group and the mean for the Beagle group ($15 - 10 = 5$)

Let's run the regression analysis
==========================================================

- In SPSS: `Analyze -> Regression -> Linear...`, then add x1 and x2 as predictors.

```{r, echo = FALSE}
kable(coef(summary(lm(data = dogs, objects ~ breed))))
```

- Note that `x1` is called `breedBorder Collie` here and `x2` is called `breedTerrier` as a reminder of what they stand for

- Looks just about right in terms of the intercept and the differences (remember, the means differ from the true population means because this is a (simulated) sample and contains random error)

Adding a covariate
=========================================================
- Let's assume that we have done a "Doggy IQ" test on each dog (no idea how that would work, but bear with me), and that there are no differences in "Doggy IQ" between breeds
- Let's fit a model that tests the effect of "Doggy IQ" and breed, assuming that the effect of "Doggy IQ" is the same across breeds (i.e. no interactions).
    - For our convenience, `dog_iq` is centred already.

```{r, echo = FALSE}
kable(coef(summary(dogs_iq_lm <- lm(data = dogs, objects ~ breed + dog_iq))))
```

Plotting this situation
=========================================================
```{r, echo = FALSE}
library(ggplot2)
intercept <- dogs_iq_lm$coefficients[1]
border_collie_intercept <- intercept + dogs_iq_lm$coefficients[2]
terrier_intercept <- intercept + dogs_iq_lm$coefficients[3]
slope <- dogs_iq_lm$coefficients[4]


qplot(data = dogs, y = objects, x = dog_iq, colour = breed) + geom_abline(intercept =  intercept, slope = slope, colour = 5) + geom_abline(intercept =  border_collie_intercept, slope = slope, colour = 2) + geom_abline(intercept =  terrier_intercept, slope = slope, colour = 3)
```

Contrasts
========================================================
- The link between (multiple) regression, t-tests, and ANOVA
- Using dummy coding to turn a discrete variable into a number of "continuous" contrasts
- Many possible contrasts -- you can make your own!
    - Not very many *sensible* contrasts.
- Basic principles: A factor with $k$ levels gets split into $k-1$ contrasts.
    - i.e. one contrast per degree of freedom
- Contrasts can be, but don't have to be, **orthogonal**
    - orthogonal contrasts don't share any variance
    - non-orthogonal contrasts are fine to use, but they may be correlated
        - remember the pitfalls of multicollinearity!


We can also do a standard ANOVA
==========================================================
```{r, echo = FALSE}
kable(anova(lm(data = dogs, objects ~ breed)))
```

- Note that the contrasts give us more information: they tell us which of the factor levels differ from the "Beagle" baseline
    - These are not multiple comparisons -- the ANOVA F-test simply compares a model without any of the contrasts (just the intercept) to a model with all the contrasts, while the *t*-tests compare the coefficient for each contrast to 0.
        - The classic ANOVA is simply a regression with (implicit) standard contrasts.
     - As a consequence, we get the contrasts "for free", but we can only have as many contrasts as the factor has degrees of freedom (i.e. $k-1$, where $k$ is the number of factor levels)

Interpreting the hypothesis tests
========================================================
- Note that we are testing the $H_0$ that $\alpha$, $\beta_1$, $\beta_2$ are 0.
- In this examples, we call the observed coefficients $b_1$ `breedBorder Collie` and $b_2$ `breedTerrier`.
- Remember what we said about the coefficients?
- $\alpha$ (the intercept) reflects the mean for the Beagle group
- If the intercept is significantly different from 0, that's not that interesting (but at least it is evidence that the beagles can learn more than 0 object names)
- The first slope $\beta_1$ reflects the difference between the Border Collie group and the Beagle group
- If this difference is significant, it means that there is evidence that Border Collies know more object names than Beagles

Interpreting the hypothesis tests (2)
========================================================
- The second slope $\beta_2$ reflects the difference between the Terrier group and the Beagle group
- If this difference is significant, it means that there is evidence that Terriers know more object names than Beagles
- Looking at the *t*-test results, $b_1$ is significantly different from 0, but $b_2$ isn't.
- There's a significant difference in terms of object names known between Beagles and Border Collies, but not between Beagles and Terriers
- Note that we are only doing two comparisons -- that's all we can do.

Trying different contrasts
=========================================================
- We can try some different contrast coding schemes to see how they work
- We can do this here because there are fake data and we know the actual means
- With real data, you need to plan your contrasts **before** you analyse your data (ideally, before you even collect them)
    - That's why they are called **planned** contrasts as opposed to **post hoc**.
- You can't even look at the means first!
- Otherwise, you're cheating. This is far worse than a small violation of normality or homoscedasticity!

Using contrasts in SPSS
==========================================================
- To be honest, SPSS is terrible with contrasts -- it's really inconsistent
- You can specify your own contrasts using the Univariate ANOVA module, but you can't do that for the General Linear Model module
    - Instead, you have to pick a number of standard contrasts
- In theory, you could define your own contrasts, but it's a bit tricky

Standard contrasts
========================================================
- Simple
    - Compares each level to the reference level, the intercept is the grand mean
- Deviation
    - Compares each level to the overall mean of the dependent variable (the reference level is not compared)
- Helmert
    - Compares each level to the mean of the subsequent ones
- Difference (reverse Helmert)
    - Compares each level to the mean of the previous ones
- Repeated (successive differences)
    - Compares each level to the subsequent level 

Applying deviation contrasts
==========================================================
```{r, results='asis', echo = FALSE}
contrasts(dogs$breed) <- contr.sum
kable(coef(summary(lm(data = dogs, objects ~ breed))))
```

Interpreting sum/deviation contrasts
===========================================================
- The intercept $\alpha$ is the grand mean of all the observations ($28.33$)
- $\beta_1$ is the difference between the grand mean and the mean of Beagle ($10 - 28.33 = -18.33$)
- $\beta_2$ is the difference between the grand mean and the mean of Border Collie ($60 - 28.33 = 31.67$)
- Terrier is never explicitly compared to the grand mean.
- In general: each level (except for the last level) is compared to the grand mean.

Applying difference (reverse Helmert) contrasts
==========================================================
```{r, results='asis', echo = FALSE}
contrasts(dogs$breed) <- contr.helmert
kable(coef(summary(lm(data = dogs, objects ~ breed))))
```

Interpreting (reverse) Helmert contrasts
========================================================
- The intercept $\alpha$ is the grand mean of all the observations ($28.33$)
- $beta_1$ is half of the difference between the mean of Beagle and the mean of Border Collie ($(60 - 10)/2 = 25$)
- $beta_2$ is half of the difference between the joint mean of Beagle and Border Collie and the mean of Terrier ($(15 - (60+10)/2)/2 = -10$)
- In general: each level is compared to the mean of the previous levels

Make your own contrasts?
==========================================================
- **DANGER**: If you apply your contrasts directly as dummy variables, you must use the **inverse** of your contrast matrix
- If your contrasts are not orthogonal, and you don't use the inverse of your matrix, you won't be comparing what you think you're comparing.
- If you don't know what this means, don't use your own contrasts until you do. 
- For more background information on regression and linear models, see John Fox's book below (Warning: it does involve matrix algebra). Check Chapter 6 for information about how the contrasts work and why you need to be careful.

>Fox, J. (2008). Applied regression analysis and generalized linear models. 2nd Edition. Sage Publications, Thousand Oaks, CA, USA.
or the newer version
>Fox, J. (2015). Applied regression analysis and generalized linear models. 3rd Edition. Sage Publications, Thousand Oaks, CA, USA.


Assumption tests: Homogeneity of regression slopes
===============================================================
- This is an ANCOVA-specific assumption: is there an interaction between the covariate(s) and the discrete factor(s)?
- If there is, you can still use the linear model, but you can't call it an "ANCOVA" anymore
    - Also, the interpretation of the results will be much more complicated
- How to check this? Use `Transform` --> `Compute Variable...` to make a interaction terms for the dummy variable and the covariates, then fit a new model using the `Linear Regression` module and see if the interaction terms reach siugnificance.

Assumption tests: Homogeneity of regression slopes (2)
===============================================================
```{r, echo = FALSE}
catfood_breed_interact_lm <- lm(data = catfood_breed, 
                                formula = FoodEaten ~ CatWeight * CatAge * dummy)
kable(coef(summary(catfood_breed_interact_lm)))
```

- No evidence for any interaction. (Check multicollinearity, of course!)
