---
output: pdf_document
---
Advanced Statistics
========================================================
author: Bernhard Angele
date: Regression basics


Where we are so far
========================================================
- We have talked about some very basic tests so far.
- We can test how unlikely it is to observe a sample mean given a simple null hypothesis (e.g. that the mean is 0) -- the t-test.
- We can also test how much the distribution of the levels a discrete variable differs from the predictions made by a theoretical distribution -- the chi-square test. If it differs too much, we can reject the null hypothesis!

Linear regression
========================================================
- What if we have a more complex hypothesis?
  - Does X predict Y?
  - Does the amount of cat food eaten predict the weight of my cat? (Probably!)
  - Does the size of your forehead predict your conscientousness? (Probably not!)
  - These questions involve *continuous* predictors (independent variables) and a *continuous* predicted (dependent) variable.

Example
======================================================
- Is the amount of cat food a cat eats related to its weight?
- In other words, do heavy cats eat more?
  - (are cats who eat more heavier???)

Cat food and weight
=========================================================
```{r, echo=FALSE}
library(knitr)
n <- 30
intercept <- 100
options(digits = 3, scipen = 4)

set.seed("126")
cat_weight <- rnorm(n, mean = 5, sd = 1) # cat weight in kg
cat_age <- rnorm(n, mean = 60, sd = 24) # cat age in months
cat_breed <- factor(c(rep("Shorthair",n/2), rep("Manx", n/2)))

food_eaten <- intercept + 20*(cat_weight - mean(cat_weight)) + 2 + .5*(cat_age - mean(cat_age)) + rnorm(n, mean = 0, sd = 10)
catfood <- data.frame(CatWeight = cat_weight, FoodEaten = food_eaten)
catfood_age <- data.frame(CatWeight = cat_weight, CatAge = cat_age, FoodEaten = food_eaten)


print_p <- function(p){
  if(p < .01) return("*p* < .01")
  if(p <= .05) return(paste("*p* =", p))
  if(p > .05) return("*p* > .05")
}
```

- This table (with completely ficticious cat data) actually has 30 rows, but I'm just showing you the first 6.
- You can find the full data set on myBU.
- You have cat weight in kg and cat food eaten in g.
- Looks like there might be a positive relationship here.

```{r, results='asis', echo = FALSE}
kable(head(catfood))
```

Let's plot it
==========================================================
```{r, echo = FALSE}
plot(x = catfood$CatWeight, y = catfood$FoodEaten, xlab = "Cat Weight", ylab = "Food Eaten")
```

How can we describe these data?
==========================================================
- There is a lot of variability.
  - But the heavier cats do seem to eat a little bit more.
    - That's not a terribly precise statement
    - Can we use maths to make a better one?
      - Maybe we could draw a line through the points and then describe the line?

Fitting a line to the data
==========================================================
```{r, echo = FALSE}
plot(x = catfood$CatWeight, y = catfood$FoodEaten, xlab = "Cat Weight", ylab = "Food Eaten")
abline(mod1 <- lm(catfood$FoodEaten ~ catfood$CatWeight))
```
- Looks like we have a strong positive relationship: The heavier the cat (variable $x$), the more food eaten (variable $y$).

Fitting a line to the data
==========================================================
- What is this line?
  - There are lots of possible lines to draw for data like these
      - What is the best line to describe these data?
      - Remember (from school) that a line is (exhaustively) described by an equation such as $$y = a+b\cdot x$$
          - $a$ is called the *intercept*. It describes where the line intersects the $y$-axis
          - $b$ is called the *slope*. It describes by how many units $y$ changes when $x$ changes
          - You may have learned this as $y = mx + c$ depending on where you went to school

Errors
=======================================================
- No line will ever fit the data perfectly (i.e. go through all the points)
  - The difference between what the line *predicts* for a certain Y value (the predition is called $\hat{y}$) and what the Y value actually is can be called the error $E$:
  - For point i: $$E_i = Y_i - a + b\cdot X_i = Y_i - \hat{Y_i}$$

The "best" line?
=========================================================
- How do we find the best $a$ and $b$ (i.e. the best line) for the data?
  - Errors should sum to 0 (otherwise we are way off!)
      - This means the line should go through the point $(\bar{X}|\bar{Y})$
  - Smallest errors?
  - The errors are the difference between the values predicted by the line and the actual values:
          - We want the line that minimises all the deviations (in both directions) of the values predicted by the equation ($\hat{Y}$) from the actual $y$ values
        
Minimising the errors
==============================================================
- Two possibilities here: We could just minimise the *absolute value* of the deviations: 
  $$\sum\limits_{i=1}^{n} \left|E_i\right| = \sum\limits_{i=1}^{n}\left|Y_i - \hat{Y}_i\right| =min$$
    - This is called *least absolute values regression*, but it is quite mathematically complex, so it's rarely used and we won't talk about it in this unit.
    - It's much easier to use squares instead of absolute values and perform a *least squares regression*, so that is what almost everyone uses. Also, this puts a special penalty on large deviations, which is nice.
  $$\sum\limits_{i=1}^{n} E_i^2 = \sum\limits_{i=1}^{n}(Y_i - \hat{Y}_i)^2 =min$$

The least-Squares regression line
=========================================================
- Here, I've plotted the errors, i.e. the deviations of the predicted values (on the line) from the actual values. These are also called the **residuals**.

```{r echo = FALSE}
plot(x = catfood$CatWeight, y = catfood$FoodEaten, xlab = "Cat Weight", ylab = "Food Eaten")
abline(mod1 <- lm(catfood$FoodEaten ~ catfood$CatWeight))
pre <- predict(mod1) # plot distances between points and the regression line
segments(catfood$CatWeight, catfood$FoodEaten, catfood$CatWeight, pre, col = "red")
```

- For the simple case where all we do is predict one $y$ from one $x$ variable, the $b$ value is really easy to calculate. We start with the covariance.

Covariance
=======================================================
- Defined as: $$cov(x, y) = \frac{\sum\limits_{i = 1}^n {(x_i - \bar{x})\cdot(y_i - \bar{y})}}{n-1}$$
  - The covariance gives us a measure of how much $x$ and $y$ vary together: 
      - if $x$-values that are far away from the mean co-occur with $y$ values that are also far from the mean, we get a large absolute covariance (it can be either positive or negative, depending on which way the relationship between $x$ and $y$ goes)
      - if $x$-values that are far away from the mean co-occur with $y$ values that are close to the mean (and vice-versa), we get a small absolute covariance (i.e. $x$ and $y$ don't change together)



Covariance, the regression slope, and correlation
======================================================
- Let's see if this is useful for our goal of getting a least-squares line:
    - We're still trying to minimise $\sum\limits_{i=1}^{n} E_i^2 = \sum\limits_{i=1}^{n}(Y_i - A - B \cdot X_i )^2$
        - To minimise we'll have to use derivatives of this function. See the Fox (2013) book if you are truely interested.

Estimating the coefficients
======================================================
- In any case, it turns out that if we only have on independent (predictor) variable, we can compute the regression slope from the covariance: $$b = \frac{cov(x, y)}{s_x^2}$$
  - We can then get the intercept using the means of X and Y:
  $$a = \bar{Y} - b\cdot\bar{X}$$
- That's it -- we have the function for our least squares regression line.

Evaluating the linear model
=======================================================
- What we really want to know: How closely are the predictor variable and the predicted variable related?
    - Two ways to think about this.
        - First way: Think about the two coefficients in the linear model: the intercept $a$ and the slope $b$
        - Which one is indicative of the relationship between predictor and predicted variables?
            - The slope of course!
            
Interpreting the slope
========================================================
- Can we interpret $b$ directly?
    - Clearly, the larger $b$ is the better the prediction
    - But $b$ depends on the scales of predicted and predictor variables
        - Tricky to compare!
        - Can we standardise somehow?
            - Yes! We get the **correlation** $r$ if we standardise the covariance by dividing it by the product of the standard deviations ($s_x \cdot s_y$). In short, $r = \frac{cov(x, y)}{s_x \cdot s_y}$.
            

Other approach: Sums of squares
===========================================================
- We can also express how well our model predicts the data as **sums of squares**
- The total sums of squares is simply the numerator of the variance of $y$: ${SS}_{total} = \sum\limits_{i=1}^{n}(y_i - \bar{y})^2$
- The **regression** or model sums of squares is the variance explained by the regression model: ${SS}_{model} = \sum\limits_{i=1}^{n}(\hat{y}_i - \bar{y})^2$
- The residual sums of squares is the squared differences $e_i$ between the actual $y$-values and the predicted $\hat{y}$-values: ${SS}_{residual} =  \sum\limits_{i=1}^{n}e_i = \sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2$


Coefficient of determination
============================================================
- How much of the variance of the data is determined (explained) by the model?
$$r^2 = \frac{{SS}_{model}}{{SS}_{total}}$$
- $\frac{{SS}_{model}}{{SS}_{total}}$ is the same as the square of the correlation coefficient $r$.
    - Fun activity at home: Work out why!
- In multiple regression, we call the coefficient of determination $R^2$ instead of $r^2$. Here, we have multiple predictors and multiple correlations, so $R^2$ doesn't correspond to the square of any one of them. It still tells you how much variance your model explains, though.
- In our example: $r^2 = `r cor(catfood$CatWeight, catfood$FoodEaten)^2`$

Multiple regression
=================================================================
- We can easily add another predictor to a linear model, making it a multiple regression model, where $x_{1i}$ is observation *i* on the first predictor and $x_{2i}$ is observation *i* on the second predictor:
    - $y_i = \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{1,i} \cdot x_{2,i} + \epsilon_i$
    - Note that we have an interaction term in this equation: $\beta_3 x_{1,i} \cdot x_{2,i}$
      - We could also specify the model without the interaction if we think there might be a possibility that the effects are just additive:
          - $y_i = \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i$
    - Which of the models is better?
      - That's exactly what we want to find out! Next week!
