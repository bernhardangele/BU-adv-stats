Advanced Statistics
========================================================
author: Bernhard Angele
date: Lecture 4

Linear regression
========================================================
- So far, we've figured out how to test if a *discrete* predictor (e.g. treatment group) can explain the variance in a *continuous* variable.
- What if our predictor is also *continuous*?

- Example:
    - More exciting data about cats and cat food!
    - Is the amount of cat food a cat eats related to its weight?
    - In other words, do heavy cats eat more?
    
Cat food and weight
=========================================================
```{r, echo=FALSE}
library(knitr)
n <- 30
intercept <- 100
options(digits = 3, scipen = 4)

set.seed("126810")
cat_weight <- rnorm(n, mean = 5, sd = 1) # cat weight in kg
cat_age <- rnorm(n, mean = 48, sd = 24) # cat age in months
cat_breed <- factor(c(rep("Shorthair",n/2), rep("Manx", n/2)))

food_eaten <- intercept + 20*(cat_weight - mean(cat_weight)) + 2 + .5*(cat_age - mean(cat_age)) + rnorm(n, mean = 0, sd = 10)
catfood <- data.frame(CatWeight = cat_weight, FoodEaten = food_eaten)
catfood_age <- data.frame(CatWeight = cat_weight, CatAge = cat_age, FoodEaten = food_eaten)
catfood_breed <- data.frame(CatWeight = cat_weight, CatAge = cat_age, CatBreed = cat_breed, FoodEaten = food_eaten)

print_p <- function(p){
  if(p < .01) return("*p* < .01")
  if(p <= .05) return(paste("*p* =", p))
  if(p > .05) return("*p* > .05")
}
```

- This table actually has 30 rows, but I'm just showing you the first 6.
- You can find the full data set on myBU.
- You have cat weight in kg and cat food eaten in g.
- Looks like there might be a positive relationship here.

```{r, results='asis', echo = FALSE}
kable(head(catfood))
```

Let's plot it
==========================================================
```{r, echo = FALSE}
plot(x = catfood$CatWeight, y = catfood$FoodEaten)
```

Fitting a line to the data
==========================================================
- Let's put a line through it

```{r, echo = FALSE}
plot(x = catfood$CatWeight, y = catfood$FoodEaten)
abline(mod1 <- lm(catfood$FoodEaten ~ catfood$CatWeight))
```

- Looks like we have a strong positive relationship:
    - The heavier the cat (variable $x$), the more food eaten (variable $y$).

Fitting a line to the data
==========================================================
- What is this line?
  - There are lots of possible lines to draw for data like these
      - What is the best line to describe these data?
      - Remember (from school) that a line is (exhaustively) described by an equation such as $$y = a+b\cdot x$$
          - $a$ is called the *intercept*. It describes where the line intersects the $x$-axis
          - $b$ is called the *slope*. It describes by how many units $y$ changes when $x$ changes
          - You may have learned this as $y = mx + c$ depending on where you went to school

The Least-Squares regression line
=========================================================
- How do we find the best $a$ and $b$ for the data?
  - Since we are (as always) taking samples from a larger population, no line will ever fit the data perfectly (i.e. go through all the points), even if there is a perfect linear relationship in the population
  - We can determine the line that best estimates the actual data points, though
      - What does "best" mean?
        - We want the line that minimises the deviations of the values predicted by the equation ($\hat{y}$) from the actual $y$ values
          - Actually, we want to penalise large deviations, so we use the square: $$\sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2 =min$$

The Least-Squares regression line
=========================================================
- Here, I've plotted the deviations of the predicted values (on the line) from the actual values. These are also called the **residuals**.

```{r echo = FALSE}
plot(x = catfood$CatWeight, y = catfood$FoodEaten)
abline(mod1 <- lm(catfood$FoodEaten ~ catfood$CatWeight))
pre <- predict(mod1) # plot distances between points and the regression line
segments(catfood$CatWeight, catfood$FoodEaten, catfood$CatWeight, pre, col="red")
```

- For the simple case where all we do is predict one $y$ from one $x$ variable, the $b$ value is really easy to calculate. We start with the covariance.

Covariance
=======================================================
- Defined as: $$cov(x, y) = \frac{\sum\limits_{i = 1}^n {(x_i - \bar{x})\cdot(y_i - \bar{y})}}{n-1}$$
  - The covariance gives us a measure of how much $x$ and $y$ vary together: 
      - if $x$-values that are far away from the mean co-occur with $y$ values that are also far from the mean, we get a large absolute covariance (it can be either positive or negative, depending on which way the relationship between $x$ and $y$ goes)
      - if $x$-values that are far away from the mean co-occur with $y$ values that are close to the mean (and vice-versa), we get a small absolute covariance (i.e. $x$ and $y$ don't change together)

Covariance, the regression slope, and correlation
======================================================
- In the simple regression case, we can compute the regression slope from the covariance: $$b = \frac{cov(x, y)}{s_x^2}$$
- Why? The explanation would involve derivatives, which you may not want to deal with right now.
- We get the **correlation** $r$ if we standardise the covariance by dividing it by the product of the standard deviations ($s_x \cdot s_y$). In short, $r = \frac{cov(x, y)}{s_x \cdot s_y}$.

Sums of squares
===========================================================
- We can also express how well our model predicts the data as **sums of squares**
- The total sums of squares is simply the numerator of the variance of $y$ (just like in the ANOVA): ${SS}_{total} = \sum\limits_{i=1}^{n}(y_i - \bar{y})^2$
- The **regression** or model sums of squares is the variance explained by the regression model: ${SS}_{model} = \sum\limits_{i=1}^{n}(\hat{y}_i - \bar{y})^2$
- The residual sums of squares is the squared differences $e_i$ between the actual $y$-values and the predicted $\hat{y}$-values: ${SS}_{residual} =  \sum\limits_{i=1}^{n}e_i = \sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2$
- Of course, these add up just like in the ANOVA: $SS_{total} = {SS}_{model} + {SS}_residual$


Coefficient of determination
============================================================
- Remember $\eta^2$ for the ANOVA? We can do the same thing for the sums of squares in the regression:
$$r^2 = \frac{{SS}_{model}}{{SS}_{residual}}$$
- $\frac{{SS}_{model}}{{SS}_{residual}}$ is the same as the square of the correlation coefficient $r$.
    - Fun activity at home: Work out why!
- In multiple regression, we call the coefficient of determination $R^2$ instead of $r^2$. Here, we have multiple predictors and multiple correlations, so $R^2$ doesn't correspond to the square of any one of them. It still tells you how much variance your model explains, though.
- In our example: $r^2 = `r cor(catfood$CatWeight, catfood$FoodEaten)^2`$

Hypothesis tests
==========================================================
- We assume that there is the following linear relationship in our population: $$Y_i = \alpha + \beta\cdot x_i + \epsilon_i$$
  - $\alpha$ and $\beta$ are our population coefficients. $\epsilon_i$ is the error for this particular observation.
- We will need to estimate our regression equation based on our sample, so that $a$ and $b$ are estimates of the population coefficients $\alpha$ and $\beta$.
- $\epsilon$ is the **error**. It represents all the influences (random or not) that we are not considering in the linear relationship.
    - If the relationship is actually linear, then $E(\epsilon) = 0$
- How can we interpret these estimated coefficients? Mostly, we care about whether $\beta$ is 0, i.e. whether there is a significant relationship between $x$ and $y$ in the population.

Assumptions
==========================================================
- First, we need to assume that $x$ and $y$ come from a **bivariate** normal distribution (i.e. that they are both normally distributed) with means $\mu_x$ and $\mu_y$, variances $\sigma^2_x$ and $\sigma^2_y$, and covariance $\sigma^2_{x,y}$.
- The bivariate (or, more generally, multivariate) normal distribution assumes that:
    - For each $x$-value $x_j$, the corresponding $y_{(i|x_j)}$-values are normally distributed: $\epsilon \sim N(0, \sigma_{\epsilon}^2$)
    - For each $x$-value $x_j$, the corresponding $y_{(i|x_j)}$-values have the same standard deviation (homoscedasticity assumption): $Var(\epsilon_i) = \sigma_{\epsilon_i}^2$
    - For each $x$-value $x_j$, the corresponding $y_{(i|x_j)}$-values are independent (i.e. all $\epsilon$s are independent)
- Pretty similar to the ANOVA assumptions, actually. I wonder why?

Distribution of the coefficient estimates
===========================================================
- If our assumptions are met, our coefficient estimates $a$ and $b$ will be themselves normally distributed
- What do their distributions look like?
- What about the means (or rather, the expected values?): $E(a) = \alpha$; $E(b) = \beta$ (no derivations here, just the end results)
- What about the variance?
  - We don't really care about $a$, but $b$ is important: $$Var(b) = \frac{\sigma_{\epsilon}^2}{\sum\limits_{i=1}^{n}(x_i-\bar{x})^2} = \frac{\sigma_{\epsilon}^2}{(n-1)\cdot s^2_{x}}$$
- This means that the sampling variance of our estimate of $\beta$ will be smaller when:
    - the overall error variance is small
    - our $x$-values are spread out (covering a wide range)

Doing a hypothesis test on b
==========================================================
- We now know the sampling distribution of $b$: $$ b \sim N\left(\beta, \frac{\sigma_{\epsilon}^2}{\sqrt{\sum\limits_{i=1}^{n}(x_i-\bar{x})^2}}\right)$$
- All we need to do is find an estimate for $\sigma_{\epsilon}$
- We can get this by calculating the variance of the residuals: $$s^2_e = \frac{\sum\limits_{i = 1}^{n} e_i^2}{n-2} = \frac{\sum\limits_{i = 1}^{n} (y_i - \hat{y}_i)^2}{n-2}$$

Testing correlations
==========================================================
- Important: correlations calculated from a sample are **random variables**
- That means they will be different each time we repeat the experiment and whether they reflect the true correlation in the population depends on our luck of the draw.
- Luckily, Pearson figured out that if you randomly take *uncorrelated* samples from a normal distribution and compute the correlation coefficient, it will be *t*-distributed.
    - Tthis is exactly what you are doing if the null hypothesis that there is no relationship between the two variables you are studying is true.
    - So all we have to do is to see if the correlation coefficient is extreme enough that it would only occur 5% of the time or less if the $H_0$ (no correlation in the population) were true.

Testing correlations (2)
==========================================================
- R can do this *t*-test very easily using the `cor.test` function:
```{r, echo = FALSE}
cor.test(catfood$CatWeight, catfood$FoodEaten)
```

Moving beyond correlations
============================================================
- So far, so good. But what if we have multiple continuous predictors?
- Let's look at the function we used to draw the line again:
```{r, eval=FALSE}
abline(lm(catfood$FoodEaten ~ catfood$CatWeight))
```
- The `abline` part just does the drawing. The real work is done by `lm`.
- `lm` stands for "Linear Models"
- Let's see what `lm` does if we don't use `abline` on it

Linear models
===========================================================
- Remember the formula interface? It's `Dependent variable ~ Independent variable(s)`
- Also, I can use `data = catfood` to avoid having to write `catfood$` every time.
```{r, echo = FALSE}
lm(formula = FoodEaten ~ CatWeight, data = catfood)
```

What do these values mean (2)?
============================================================
- So, in our case, the best fitting line for the cat food data intersects the $y$-axis at the point (0, `r lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[1]`).
  - Not all x-values are sensible for all data. Saying that a cat with 0 kg weight would eat `r lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[1]` g of food makes no sense, since a cat with 0 kg weight is not a cat anymore.
  - The linear function doesn't care, of course. It knows nothing about our data and just specifies a line.
- The slope might be more useful: It says that for each kg of extra weight, a cat will eat `r lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[2]` more grammes of food.
    - Using this information, we can predict that a giant 8 kg cat would eat $`r lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[1]` + `r lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[2]` \cdot 8 = `r lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[1] + lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[2] * 8`$ g of food.
    
Predictions and residual errors
===============================================================
- Of course, our prediction is likely to be at least a little off.
- If we had an 8 kg cat in our data and its actual amount of food consumed was 170 g, we'd have an error of `r 170 - (lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[1] + lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[2] * 8)`.
  - This is called the residual error.
- More formally, the regression equation looks like this (where $x_i$ are the individual values for the $x$ variable, and $y_i$ are the corresponding values for the $Y$ variable):
    - $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$
    - Here, we've simply renamed the intercept to $\beta_0$ and the slope to $\beta_1$.
    - $\epsilon_i$ is the residual error for each data point.
    - Important: $\epsilon_i$ is assumed to be normally distributed
      - This doesn't matter for the line fitting, but it does for the hypothesis tests!
      
Back to hypothesis testing
=================================================================
- Important: Note that the $\beta$ variables are greek letters, which means they are the *population parameters*
- For each $\beta$ coefficient in the regression formula, we can propose the $H_0$ that the true value of that $\beta$ coefficient is 0
- The $\beta$ that are estimated from our sample are simply called $b$
- We can once again test if our $b$ values are extreme enough so they would only occur 5% of the time or less given the $H_0$.
- We test this separately for each $b$ value. Guess what, it's a *t*-test!

Hypothesis testing using lm
=================================================================
- In R, the `summary` function will usually give us the hypothesis tests corresponding to a linear model:
```{r, echo = FALSE}
summary(lm(formula = FoodEaten ~ CatWeight, data = catfood))
```

Hypothesis testing using lm (2)
=================================================================
- This is essentially the same test as the Pearson test for the correlations before, just without standardising the slope
- We also test whether the intercept is 0
    - This is usually not particularly interesting unless you have a very specific hypothesis about the intercept.
- In our cat example, the intercept is so small that it essentially doesn't matter for the quality of the prediction if the intercept is 0
- What definitely matters is whether the slope is 0 or not. Based on the sample data, we can conclude that if the $H_0$ were true, we would see an effect of this size far less than 5% of the time.

Hypothesis testing using lm (2)
=================================================================
- Unlike simply running a hypothesis test on a correlation, we can easily add another predictor to a linear model, making it a multiple regression model, where $x_{1i}$ is observation *i* on the first predictor and $x_{2i}$ is observation *i* on the second predictor:
    - $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{1,i} \cdot x_{2,i} + \epsilon_i$
    - Note that we have an interaction term in this equation: $\beta_3 x_{1,i} \cdot x_{2,i}$
      - We could also specify the model without the interaction if we think there might be a possibility that the effects are just additive:
          - $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i$
    - Which of the models is better?
      - That's exactly what the significance test tells us!
      
Example
==================================================================
- Let's assume that, apart from each cat's weight in kg, we also have its age in months:

```{r, results='asis'}
kable(head(catfood_age))
```

- Does adding age (and the interaction between age and weight) to the model improve it?


Example (2)
==================================================================
- In a formula, the `:` symbol symbolises the interaction term

```{r, echo = FALSE}
summary(lm(formula = FoodEaten ~ CatWeight + CatAge + CatWeight:CatAge, data = catfood_age))
```

Example (3)
==================================================================
- You can also use the shorthand `*` to add both a main effect and an interaction.
  - This model is exactly the same as the previous one:

```{r, echo = FALSE}
lm_catfood_interaction <- lm(formula = FoodEaten ~ CatWeight * CatAge, data = catfood_age)
summary(lm_catfood_interaction)
```

Interpreting the coefficients
=================================================================
- Let's look at just the coefficients:

```{r, results='asis'}
kable(summary(lm_catfood_interaction)$coefficients)
```

- Looks like only the coefficient for CatWeight is significantly different from 0, while the coefficients for CatAge and the interaction aren't.
- Important: these *t*-tests measure how much of the variance each predictor explains **given that all the other predictors are in the model as well**.

The rest of the results
=================================================================
- Residuals: Gives you a quick overview of the distribution of residuals
    - Is the median (roughly) 0?
    - Is quartile 1 roughly as far away from the median as quartile 3?
    - Are the minimum or maximum extremely far away from the median?
- Residual standard error: 
    - This is an estimate of the error variance: On average, how far away are the true values from the predicted values?

The rest of the results (2)
=================================================================
- Multiple R-squared:
    - $R^2 = \frac{SS_{Model}}{SS_{Total}}$
    - What is the percentage of the total variance that can be explained by the model?
    - This is a meaasure of effect size (exactly the same as $\eta^2$)
- Adjusted R-squared:
    - This is an adjustment to $R^2$ that takes into account the number of predictors used
    - With enough predictors, you can fit anything
    - We want a model that explains as much of the variance as possible with as few predictors as possible

The rest of the results (2)
=================================================================
- F-statistic: This compares the variance explained by the model ($MS_{Model}$) with the unexplained variance ($MS_{Error}$) -- just like an ANOVA.
    - The dfs work just like in ANOVA: you have one Model df for each parameter that you estimate
    - $df_{Error} = df_{Total} - df_{Model}$
    - $df_{Total} = n - 1$, where $n$ is the number of observations; you lose one df for computing the intercept.
- You can also conceptualise this as testing whether the model with the 3 predictors is better than a model using only the intercept.

Regression and F-Tests
=================================================================
- You can use *F*-Tests to compare any regression models that you want
  - Although the comparison only makes sense if they share predictors
- For example, is a model containing the CatWeight by CatAge interaction better than a model without it?
```{r, echo = FALSE}
anova(lm_catfood_interaction, lm(formula = FoodEaten ~ CatWeight + CatAge, data = catfood_age))
```

Regression and F-Tests (2)
=================================================================
- You can use the `anova` command for this. If you only give one linear model to the ANOVA command, it will run an *F*-Test for each predictor
```{r, echo = FALSE}
anova(lm_catfood_interaction)
```

Why do we get conflicting results?
=================================================================
- This is weird: We didn't have an effect for CatAge when we looked at the *t*-values, but we have when doing F-tests?

```{r, results='asis'}
kable(anova(lm_catfood_interaction))
```

- The answer lies in **what each test compares**. This may sound nitpicky, but it's really important!

Model comparisons
=================================================================
- The *t*-values compare a model containing the predictor *along with* all the other predictors to a model containing *all the other predictors* **except** the predictor in question.
- The `anova` command compares a model containing the predictor **and all other predictors** entered *so far* with a model only containing the predictors entered **so far**
    - This means that **order matters**. In this case, the CatWeight by CatAge interaction isn't entered yet.
    - This is called **Type I sums of squares**
- The `anova` command is dumb and can only do Type I SS, but there is an alternative in the `car` package, cleverly labeled `Anova`
  - Use `install.packages("car")` to get the `car` package if you don't have it already

Sums of squares
=================================================================
- Type I: Compare a model containing the predictor **and all other predictors** entered *so far* with a model only containing the predictors entered **so far** (order matters).
    - This is what the `anova` command does.
- Type II: The *t*-values compare a model containing the predictor *along with* all the other predictors to a model containing *all the other predictors* **except** the predictor in question **and its interactions**.
    - This is the standard in ezANOVA.
- Type III: The *t*-values compare a model containing the predictor *along with* all the other predictors to a model containing *all the other predictors* **except** the predictor in question, but **including its interactions**.
    - This is the equivalent of the *t*-tests.
    - This is the standard in SPSS.


Example: Type I sum of squares
=================================================================
```{r, results='asis'}
kable(anova(lm_catfood_interaction))
```

Example: Type II sum of squares
=================================================================
```{r, results='asis'}
library(car)
kable(Anova(lm_catfood_interaction,type = "II"))
```

Example: Type III sum of squares
=================================================================
```{r, results='asis'}
library(car)
kable(Anova(lm_catfood_interaction,type = "III"))
```

Summary: Sums of square types
==================================================================
- ANOVA results (both classic ANOVA and regression model tests) can vary depending on which SS you use
    - Make sure that you know which ones you are using
    - If you are using SPSS, the answer is *probably* III.
- In standard ANOVA (with only discrete predictors), all SS types give the same result *as long as your design is balanced*
    - An unbalanced design will lead to differing sums of squares.
- In multiple regression, all SS types give the same result *as long as your predictor variables are not correlated*

Back to the current model
=================================================================
- **Important**: The above model has a **critical flaw**, so read on before you run off and perform multiple regressions.
- The flaw is related to correlations between predictors (multicollinearity).
- Specifically, we seem to have the issue that CatAge is significant on its own, but loses significance if the interaction CatWeight:CatAge is already in the model
- Can we test this idea in a more formal way?

Multicollinearity
=================================================================
- Once again: Type III SS and *t*-tests measure how much of the variance each predictor explains **given that all the other predictors are in the model as well**.
- This may not seem important at first, but often, the predictors will be correlated themselves
- This is called *multicollinearity*.
```{r, echo = FALSE}
cor(catfood_age$CatWeight, catfood_age$CatAge)
```
- In this case, they aren't. So where is the problem?

Multicollinearity (2)
=================================================================
- Imagine what would happen if CatWeight and CatAge were somehow (almost) perfectly correlated
- Actually, let's just try it
```{r, echo = FALSE}
# make a copy of catfood_age
catfood_age_bizarre <- catfood_age
catfood_age_bizarre$CatAge <- catfood_age_bizarre$CatWeight/10 + 
                                  rnorm(nrow(catfood_age_bizarre), mean = 0, sd = .0001)
```
- We're entering a bizarre world where each cat's age is exactly 1/10th of its weight
- I added a tiny little bit of random variance because perfect correlations really mess up regression

Multicollinearity (3)
=================================================================
- Neat little trick: If you give `plot` just a data frame object instead of columns, it will make every possible scatter plot from the data (a scatter plot matrix).
```{r, echo = FALSE}
plot(catfood_age_bizarre)
```

Multicollinearity (4)
=================================================================
```{r, echo = FALSE}
summary(lm(formula = FoodEaten ~ CatWeight + CatAge, data = catfood_age_bizarre))
```

What a mess!
=================================================================

```{r, results='asis'}
kable(summary(lm(formula = FoodEaten ~ CatWeight + CatAge, data = catfood_age_bizarre))$coefficients)
```

- Neither CatWeight nor CatAge are significant  -- nothing is!
- If you compare the standard errors of the estimates to the "sane" model, they are huge.
    - That is because the model has a hard time determining the estimates
      - If the predictors are perfectly correlated, you can make equally good predictions no matter what the value of coefficient 1 is -- you just have to adjust coefficient 2 to compensate!

What a mess! (2)
=================================================================
- This model is really hard to interpret
    - CatWeight or CatAge would have a strong effect individually
    - But neither predictor adds anything if the other one is already in the model
- Make sure to check for correlations between predictors before running a regression
- Start by using the scatter plot matrix, then you can run `cor` on individual predictors
- Important: Some multicollinearity is unavoidable, but the higher values are correlated, the more problems you get


Diagnosing Multicollinearity
================================================================
- You can use `vif` from the `car` package
- First the sane data, where cat age isn't almost perfectly correlated with cat weight (note that I'm taking the interaction out for the time being)
```{r, echo = FALSE}
library(car)
vif(lm(formula = FoodEaten ~ CatWeight + CatAge, data = catfood_age))
```

Diagnosing Multicollinearity
================================================================
- Now the insane data, where all cats gain almost exactly 100 grammes per month for some bizarre reason.
```{r, echo = FALSE}
library(car)
vif(lm(formula = FoodEaten ~ CatWeight + CatAge, data = catfood_age_bizarre))
```

Interpreting the VIF
=================================================================
- You have a problem with mulitcollinearity if
    - The largest VIF is greater than 10 and/or
    - The average VIF is substantially greater than 1
- Multicollinearity seriously affects the interpretability of your model
- Both criteria are true for the bizarre data
- Neither is true for the non-bizarre data

Now what about that problematic interaction model?
==================================================================
- We use the non-bizarre data (of course)
```{r, echo = FALSE}
vif(lm_catfood_interaction)
```
- Those are some seriously high VIFs!
- What is going on?

Where does the multicollinearity come from?
==================================================================
- Here's the problem: The interaction effect is equivalent to CatWeight x CatAge
- Unfortunately, the product of the two variables is highly correlated with at least one of the two variables:
```{r, echo = FALSE}
with(catfood_age, cor(CatWeight, CatWeight*CatAge))
with(catfood_age, cor(CatAge, CatWeight*CatAge))
```

What to do?
===================================================================
- Luckily, there is an easy solution:
  - If we **center** both variables (i.e. subtract the mean from each observation), the correlation will disappear
  - You can center variables either by hand `x - mean(x)` or by using `scale(x, scale = FALSE)` (one of the least intuitive commands I've seen so far!)
```{r, echo = FALSE}
catfood_age$CatWeight <- catfood_age$CatWeight - mean(catfood_age$CatWeight)
catfood_age$CatAge <- catfood_age$CatAge - mean(catfood_age$CatAge)
```

Did this help?
===================================================================
- Yes, it did:
```{r, echo = FALSE}
with(catfood_age, cor(CatWeight, CatWeight*CatAge))
with(catfood_age, cor(CatAge, CatWeight*CatAge))
# re-fit the model
lm_catfood_interaction <- lm(formula = FoodEaten ~ CatWeight * CatAge, data = catfood_age)
vif(lm_catfood_interaction)
```

Let's look at the coefficients again
===================================================================

```{r, results='asis'}
kable(summary(lm_catfood_interaction)$coefficients)
```

- Look at that: Now CatAge is significant, too!
- We would have made a Type II error if we hadn't centered the variables.
- Lesson of this story: When testing for interactions with continuous variables, **always center the continuous variables**.

Influential cases
====================================================================
- Meet Goliath, the cat

- He's very heavy (20 kg more than the average) and isn't very old (4 months more than the average) but doesn't eat much (101 g).

Influential cases (2)
====================================================================
- Let's add him to the data:
```{r, echo = FALSE}
catfood_goliath <- rbind(catfood_age, c(20, 4, 101))
# don't forget to re-center the predictors!
catfood_goliath$CatWeight <- scale(catfood_goliath$CatWeight, scale = FALSE)
catfood_goliath$CatAge <- scale(catfood_goliath$CatAge, scale = FALSE)
```

Influential cases (3)
====================================================================
- Make a plot of the situation. See Goliath out there?
```{r, echo = FALSE}
plot(catfood_goliath)
```

Influential cases (4)
====================================================================
- What will he do to the model?

```{r, results='asis'}
lm_goliath <- lm(formula = FoodEaten ~ CatWeight * CatAge, data = catfood_goliath)
kable(coef(summary(lm_goliath)))
```

- That doesn't look too good!

Let's plot it
=====================================================================
```{r, echo = FALSE}
with(catfood_goliath, plot(x = CatWeight, y = FoodEaten))
abline(lm_goliath)
```

Diagnosing influential cases
======================================================================
- R has a convenient function called `influence.measures` giving you all the statistics you need.
  - To make this more readable here I'm using `kable` to only print the last rows of the statistics table (in order to see Goliath's case), but you can just use `influence.measures(your_lm_here)` to get a fairly comprehensive printout of all the cases.

Diagnosing influential cases (2)
======================================================================
```{r, results='asis'}
kable(tail(influence.measures(lm_goliath)$infmat))
```

Diagnosing influential cases (3)
======================================================================
- One row per observation/case
- Columns:
- DFBetas (one for each predictor, including the intercept) 
    - Difference between the parameter when this case is excluded and when it is included
    - Should not be much larger than that of the other cases
- DFFit
    - Difference between the predicted value for a case when this case is excluded and when it is included
    - Should not be much larger than that of the other cases
- Covariance ratio
    - The farther it is from 1 and the smaller it is the more this case increases the error variance

Diagnosing influential cases (4)
========================================================================
- Cook's distance
    - Measure of the overall influence of a case on the model
      - Values > 1 are possibly problematic
- Hat = leverage
    - How strongly does this case influence the prediction?
    - Average value is $p/n$, where $p$ is the number of predictors (including the intercept, so 4 for our model) and $n$ is the number of observations(31 for our model, so our average should be .13)
    - Values over 2 or 3 times the average should be cause for concern

General strategy for diagnosing influential cases
==========================================================================
- Check if any cases have a Cook's distance of > 1
- Then check if they have an undue effect on the model using the other statistics
- If called directly, influence.measures gives you a column called "Inf", flagging potentially influential cases. 
  - If Cook's distance for these cases is > 1, consider removing them.

More assumption tests
===========================================================================
- Normality of the residuals (we already know that one)
```{r, echo = FALSE}
shapiro.test(resid(lm_catfood_interaction))
```

More assumption tests (2)
===========================================================================
- Durbin-Watson test for autocorrelated errors (in `car` package)
- Are adjacent residuals correlated?
```{r, echo = FALSE}
durbinWatsonTest(lm_catfood_interaction)
```
- Not in this case.

More assumption tests (3)
===========================================================================
- Heteroskedasticity
    - Look at the plot of residuals by predicted values
    - Does it look like there is more variance for certain predicted values?
```{r, echo = FALSE}
plot(x = predict(lm_catfood_interaction), y = resid(lm_catfood_interaction))
```

Reporting the regression results
===================================================================
- Make a table with the model coefficients:

```{r, results='asis'}
lm_catfood_interaction_table <- coef(summary(lm_catfood_interaction))
kable(lm_catfood_interaction_table)
```

- Introductory paragraph: In order to test the hypothesis that cat weight and cat age can predict how much food a cat eats, we performed a multiple regression analysis  with food eaten (in g) as the dependent variable and cat weight and cat age as well aas their interactions as continuous independent variables. The model explained a very high amount of the variance in the dependent variable, with an adjusted $R^2$ of `r summary(lm_catfood_interaction)$adj.r.squared`.


Reporting the regression results
===================================================================
Our results show that both cat weight (b = `r lm_catfood_interaction_table["CatWeight","Estimate"]`, SE = `r lm_catfood_interaction_table["CatWeight","Std. Error"]`, t = `r lm_catfood_interaction_table["CatWeight","t value"]`, `r print_p(lm_catfood_interaction_table["CatWeight","Pr(>|t|)"])`) and cat age (b = `r lm_catfood_interaction_table["CatAge","Estimate"]`, SE = `r lm_catfood_interaction_table["CatAge","Std. Error"]`, t = `r lm_catfood_interaction_table["CatAge","t value"]`, `r print_p(lm_catfood_interaction_table["CatAge","Pr(>|t|)"])`)) had a significant effect on the food eaten. On average, an increase in weight by one kg went along with an increase in food eaten of `r lm_catfood_interaction_table["CatWeight","Estimate"]` g. Similarly, an increase in age by one month went along with an increase in food eaten of `r lm_catfood_interaction_table["CatAge","Estimate"]` g.

There was no significant interaction between cat weight and cat age (p > .05), suggesting that the effects of cat weight and cat age were additive. Assumption tests and visual inspection of residual plots showed that there was no evidence of violations of normality (p > .05), independence, or homoscedasticity. [If you removed influential cases, say this here.]

Discrete variables
==================================================================
- Is it possible to perform a regression analysis with discrete (instead of continuous) variables?
  - **Yes!**
  - In fact, when you ask SPSS or R to perform an ANOVA, what it does behind the scenes is run a linear model and then do model comparisons using the F-Test
      - Minds blown (maybe)!
  - How can we put discrete (that is, non-numerical) variables into the regression model?
      - We make up numbers, of course.
      - This is called *dummy coding*.
      
Example
=================================================================
```{r, echo=FALSE}
catfood_breed$FoodEaten <- catfood_breed$FoodEaten + ifelse(cat_breed == "Manx", 15, -15)
```
- Let's just stay with the cat data for a little bit longer
- Let's say our cats came from two breeds, shorthair and manx.
    - Does breed have an influence on food eaten?
    
```{r, results='asis'}
kable(head(catfood_breed))
```

We could do a t-test
=================================================================
```{r, echo = FALSE}
t.test(formula = FoodEaten ~ CatBreed, data = catfood_breed)
```

Or an ANOVA
==================================================================
```{r, echo = FALSE}
summary(aov(formula = FoodEaten ~ CatBreed, data = catfood_breed))
```

Or we could assign dummy values and do a regression
==================================================================
```{r, echo = FALSE}
# assign 0 to all shorthairs and 1 to all manxs
catfood_breed$dummy <- ifelse(catfood_breed$CatBreed == "Shorthair", 0, 1)
summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed))
```

Let's plot the situation
=================================================================
```{r, echo = FALSE}
plot(x = catfood_breed$dummy, y = catfood_breed$FoodEaten)
abline(lm(formula = FoodEaten ~ dummy, data = catfood_breed))
```

Interpreting dummy variables
=================================================================
- Now, the intercept is the mean for group "shorthair" and the slope gives the difference between group "shorthair" and group "manx"
- Remember the regression equation: $y_i = \beta_0 + \beta_1 x_{i} + \epsilon_i$
- If $x_i$ is 0 (shorthair group), the predicted value y is the intercept ($\beta_0$)
- If $x_i$ is 1 (manx group), the predicted value is the sum of the intercept ($\beta_0$) and the slope ($\beta_1$). 

Different dummy values
================================================================
- Nobody forces us to set the values to 0 and 1
- We could use any values we want, e.g. 99 and 23419 (although have fun interpreting *that* equation)
- -1 and 1 might be more reasonable:
```{r, echo = FALSE}
catfood_breed$dummy <- ifelse(catfood_breed$CatBreed == "Shorthair", -1, 1)
```

Re-run the analysis
================================================================

```{r, results='asis'}
kable(coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed))))
```

- Note that the numbers are different: now the intercept represents the grand mean.
- The slope tells you how far the means of shorthair and manx are from the grand mean.
  - The prediction for shorthair is $`r coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["(Intercept)","Estimate"]` - `r coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["dummy","Estimate"]` = `r coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["(Intercept)","Estimate"] - coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["dummy","Estimate"]`$
  - The prediction for manx is $`r coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["(Intercept)","Estimate"]` + `r coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["dummy","Estimate"]` = `r coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["(Intercept)","Estimate"] + coef(summary(lm(formula = FoodEaten ~ dummy, data = catfood_breed)))["dummy","Estimate"]`$
- The *t* and *p* values are exactly the same.

Contrasts
================================================================
- Coming up with these dummy contrasts gets quite involved, especially when you have many factor levels
    - More details on that next week
- Fortunately, R is ready to help!
- When you define a variable as a factor (i.e. tell R that it is discrete), R automatically assigns a type of factor dummy coding to it.
    - The default contrast is treatment coding (for 2 levels, that's 0 vs. 1). SPSS call this a *simple* contrast.
      - Which level will be 0 (that is, the baseline)?
      - By default, the alphabetically first factor (in this case, "Manx")

Contrasts (2)
================================================================
- Take a look at the contrasts for a factor using `contrasts`:
```{r, echo = FALSE}
contrasts(catfood_breed$CatBreed)
```
- We want shorthair as the baseline. You can use `relevel` to change the baseline (or reference level):
```{r, echo = FALSE}
catfood_breed$CatBreed <- relevel(x = catfood_breed$CatBreed, ref = "Shorthair")
contrasts(catfood_breed$CatBreed)
```

Contrasts (3)
================================================================
- You can also assign other contrasts. For example, for 2 levels, *sum* or *effect* coding is (-1 vs. 1).
- To assign a different contrast type, you use `contrasts` together with a function to generate the contrast matrix (e.g. `contr.sum`)
```{r, echo = FALSE}
contr.sum(2)
contrasts(catfood_breed$CatBreed) <- contr.sum(2)
contrasts(catfood_breed$CatBreed)
```

Analysis of Covariance (ANCOVA)
=================================================================
- Now we have all the elements in place to perform a simple ANCOVA
- The idea behind the ANCOVA is that sometimes our dependent variable in the ANOVA is influenced by non-discrete covariates
- For example, someone's IQ might influence their performance in a memory experiment
- By including the covariate in the model, we can explain more variance (and take it out of the error variance)

Analysis of Covariance (ANCOVA)
=================================================================
- Again with the cats! Sorry, I'm just too lazy to make up a new data set...
- Let's see if there is an effect of cat breed if we enter cat age and cat weight into the analysis as covariates.
- In theory, we could use `ezANOVA` for this, but it does the ANCOVA slightly differently from how most statistics programs do it.
- Better to use `lm` for this, which will give us exactly the same results as SPSS and other statistics programmes.

Use lm to perform the ANCOVA (1)
================================================================
```{r, echo = FALSE}
catfood_breed_lm <- lm(data = catfood_breed, formula = FoodEaten ~ CatWeight + CatAge + CatBreed)
summary(catfood_breed_lm)
```

Use lm to perform the ANCOVA (2)
================================================================
- Get F values using `Anova` from the `car` package (Type II or III SS doesn't matter since we have no interactions):
```{r, echo = FALSE}
Anova(catfood_breed_lm, type = "III")
```

Assumption tests: Homogeneity of regression slopes
===============================================================
- This is an ANCOVA-specific assumption: is there an interaction between the covariate(s) and the discrete factor(s)?
- If there is, you can still use the linear model, but you can't call it an "ANCOVA" anymore
    - Also, the interpretation of the results will be much more complicated
- How to check this? Fit a new model that allows the covariate(s) and factor(s) to interact, then test it against the old model

Assumption tests: Homogeneity of regression slopes (2)
===============================================================
- We need to use the lower-case `anova` function to compare the two models:
```{r, echo = FALSE}
catfood_breed_interact_lm <- lm(data = catfood_breed, 
                                formula = FoodEaten ~ CatWeight * CatAge * CatBreed)
anova(catfood_breed_lm, catfood_breed_interact_lm)
```
- No evidence for any interaction.

Assumption checks: Multicollinearity
==============================================================
```{r, echo = FALSE}
vif(catfood_breed_lm)
```
- All good.

Assumption checks: Normality
===============================================================
Again, check for normality violations:
```{r, echo = FALSE}
shapiro.test(resid(catfood_breed_lm))
```

Assumption checks: Homogeneity of variance
===============================================================
- Just like in the between-subjects ANOVA, we have to check homogeneity of variances for the factor.
- Unlike `ezANOVA`, `lm` doesn't do this for us automatically.
    - We can use the function `leveneTest` from the `car` package (that's what `ezANOVA` does internally, anyway):
```{r, echo = FALSE}
leveneTest(y = FoodEaten ~ CatBreed, data = catfood_breed)
```

Assumption checks: Influential cases
==============================================================
- We've already done those above, but if this were a new data set, you'd have to do them again!

Assumption checks: Homoscedasticity
==============================================================
No evidence for autocorrelation:
```{r, echo = FALSE}
durbinWatsonTest(catfood_breed_lm)
```

Last class
========================================================
- Time really flies when you are having fun, right?
- On the agenda for today:
    - Contrasts
    - Transformations
    - Logistic regression
    - Linear mixed models
    - Power
    - Non-parametric tests (and why we didn't talk much about them)
- Also: Assignment 2
- Assignment 1 marks:
    - By Dec 1st -- plenty of time to incorporate my feedback for Assignment 2.

Contrasts
========================================================
- The link between multiple regression and ANOVA
- Using dummy coding to turn a discrete variable into a number of "continuous" contrasts
- Many possible contrasts -- you can make your own!
    - Not very many *sensible* contrasts.
- Basic principles: A factor with $k$ levels gets split into $k-1$ contrasts.
    - i.e. one contrast per degree of freedom
- Contrasts can be, but don't have to be, **orthogonal**
    - orthogonal contrasts don't share any variance
    - non-orthogonal contrasts are fine to use, but they may be correlated
        - remember the pitfalls of multicollinearity!
        
Example
========================================================
- Enough about cats, let's talk about dogs!
- In this ficticious example, let's assume we are testing 45 dogs to see how many object names they know (e.g. when you tell them to bring you a "ball", "stick", etc., do they bring you the correct object or a random one?)
- Our sample contains 15 beagles, 15 border collies, and 15 terriers.
- Let's assume that the true means for each breed are:

| Breed        | Number of object names known|
|-------------:|----------------------------:|
| Beagle       |                           10|
| Border Collie|                           60|
| Terrier      |                           15|

Example (2)
========================================================
- With 3 means, there are (at least) 3 comparisons we can make

| Comparison              | Difference                  |
|------------------------:|----------------------------:|
| Border Collie -- Beagle |                           50|
| Border Collie -- Terrier|                           35|
| Terrier -- Beagle       |                            5|

- Let's see how the contrasts reflect these comparisons
  - But remember -- we can only make 2.
  
Generating the data
========================================================
- Feel free to skip over this if you don't care about how we generate the fake data
```{r, echo = FALSE}
# Seed for random number generators, so that we all get the same results
set.seed("6")
# Column 1: Breed - repeat each breed name 15 times, then combine
breed <- c(rep("Beagle", 15), rep("Border Collie", 15), rep("Terrier", 15))
# Column 2: Objects - repeat each true group mean 15 times, then combine
objects <- c(rep(10, 15), rep(60, 15), rep(15, 15))
```

Generating the data (2)
========================================================
- You can still skip this if you must...
```{r, echo = FALSE}
# Add random noise to the objects scores
objects <- objects + rnorm(n = 45, mean = 0 , sd = 6)
# for more realism, round the objects scores to full integers
# (what does it mean if a dog knows a fraction of an object?)
objects <- round(objects, digits = 0)
# Combine into data frame
dogs <- data.frame(breed, objects)
```

The data
========================================================
```{r, echo = FALSE}
# get the means for each breed
mean(dogs[dogs$breed == "Beagle",]$objects)
mean(dogs[dogs$breed == "Border Collie",]$objects)
mean(dogs[dogs$breed == "Terrier",]$objects)
# or do it all in one go:
tapply(X = dogs$objects, INDEX = dogs$breed, FUN = mean)
```

Comparing the means
========================================================
- We can always do pairwise *t*-tests. Those give us all the comparisons, but at the cost of making multiple comparisons.
```{r, echo = FALSE}
pairwise.t.test(x = dogs$objects, g = dogs$breed)
```

Adding contrasts
========================================================
- Let's make "breed" into a factor
```{r, echo = FALSE}
dogs$breed <- factor(dogs$breed)
```
- R automatically assigns *treatment* contrasts to each factor, which you can look at using the `contrasts` command:
```{r, echo = FALSE}
contrasts(dogs$breed)
```
- "Beagle" is the baseline level here. Why? Because it comes first alphabetically and R really has no way to know if there is another baseline level that would suit you more.

What does this contrast matrix mean?
========================================================
```{r, results='asis', echo=FALSE}
library(knitr)
contrast_matrix <- contr.treatment(3)
colnames(contrast_matrix) <- c("x1","x2")
rownames(contrast_matrix) <- c("Beagle","Border Collie","Terrier")
kable(contrast_matrix)
```

- When doing a regression analysis, R will replace the factor "breed" with two contrasts, $x_1$ and $x_2$
- $x_1$ will be 1 for all "Border Collie" cases, and 0 otherwise
- $x_2$ will be 1 for all "Terrier" cases, and 0 otherwise
- Why is this a good idea?

How contrasts work
=========================================================
- Remember the linear regression equation:
- $y_{i} = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \epsilon_i$
- i.e. the predicted value for $y_i$ is $\hat{y_i} = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2}$
- Now let's substitute in the values from the table if breed is "Beagle":
```{r, results='asis', echo=FALSE}
contrast_matrix <- contr.treatment(3)
colnames(contrast_matrix) <- c("x1","x2")
rownames(contrast_matrix) <- c("Beagle","Border Collie","Terrier")
kable(contrast_matrix)
```

- $\hat{y_{i}} = \beta_0 + \beta_1 \times 0 + \beta_2 \times 0 = \beta_0$
- The predicted value for the Beagle group is $\beta_0$, the intercept
- That means that in this analysis, the intercept will reflect the mean for the Beagle group (10)

How contrasts work (2)
=========================================================
- The predicted value for $y_i$ is still $\hat{y_i} = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2}$
- Now let's substitute in the values from the table if breed is "Border Collie":
```{r, results='asis', echo=FALSE}
contrast_matrix <- contr.treatment(3)
colnames(contrast_matrix) <- c("x1","x2")
rownames(contrast_matrix) <- c("Beagle","Border Collie","Terrier")
kable(contrast_matrix)
```

- $\hat{y_{i}} = \beta_0 + \beta_1 \times 1 + \beta_2 \times 0 = \beta_0 + \beta_1$
- The predicted value for the Border Collie group is $\beta_0 + \beta_1$, i.e. the sum of the intercept and the first slope $\beta_1$
- That means that in this analysis, the slope $\beta_1$ will reflect the difference between the mean for the Border Collie group and the mean for the Beagle group ($60 - 10 = 50$)

How contrasts work (2)
=========================================================
- The predicted value for $y_i$ is still $\hat{y_i} = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2}$
- Now let's substitute in the values from the table if breed is "Terrier":
```{r, results='asis', echo=FALSE}
contrast_matrix <- contr.treatment(3)
colnames(contrast_matrix) <- c("x1","x2")
rownames(contrast_matrix) <- c("Beagle","Border Collie","Terrier")
kable(contrast_matrix)
```

- $\hat{y_{i}} = \beta_0 + \beta_1 \times 0 + \beta_2 \times 1 = \beta_0 + \beta_2$
- The predicted value for the Border Collie group is $\beta_0 + \beta_2$, i.e. the sum of the intercept and the second slope $\beta_2$
- That means that in this analysis, the slope $\beta_1$ will reflect the difference between the mean for the Terrier group and the mean for the Beagle group ($15 - 10 = 5$)

Let's try it
==========================================================
```{r, echo = FALSE}
lm(data = dogs, objects ~ breed)
```
- Looks just about right (remember, the means differ from the true population means because this is a -- simulated -- sample and contains random error)

Let's do the hypothesis tests
==========================================================
- First, the ANOVA:
```{r, echo = FALSE}
anova(lm(data = dogs, objects ~ breed))
```

Now, the contrasts
==========================================================
```{r, echo = FALSE}
summary(lm(data = dogs, objects ~ breed))
```

Interpreting the hypothesis tests
========================================================
- Note that we are testing the $H_0$ that $\beta_0$, $\beta_1$, $\beta_2$ are 0.
- R helpfully calls the observed coefficients $b_1$ `breedBorder Collie` and $b_2$ `breedTerrier`.
- Remember what we said about the coefficients?
- $\beta_0$ (the intercept) reflects the mean for the Beagle group
- If the intercept is significantly different from 0, that's not that interesting (but at least it is evidence that the beagles can learn more than 0 object names)
- The first slope $\beta_1$ reflects the difference between the Border Collie group and the Beagle group
- If this difference is significant, it means that there is evidence that Border Collies know more object names than Beagles

Interpreting the hypothesis tests (2)
========================================================
- The second slope $\beta_2$ reflects the difference between the Terrier group and the Beagle group
- If this difference is significant, it means that there is evidence that Terriers know more object names than Beagles
- Looking at the *t*-test results, $b_1$ is significantly different from 0, but $b_2$ isn't.
- There's a significant difference in terms of object names known between Beagles and Border Collies, but not between Beagles and Terriers
- Note that we are only doing two comparisons -- that's all we can do.

Trying different contrasts
=========================================================
- We can try some different contrast coding schemes to see how they work
- We can do this here because there are fake data and we know the actual means
- With real data, you need to plan your contrasts **before** you analyse your data (ideally, before you even collect them)
    - That's why they are called **planned** contrasts as opposed to **post hoc**.
- You can't even look at the means first!
- Otherwise, you're cheating. This is far worse than a small violation of normality or homoscedasticity!

What other contrasts does R have?
========================================================
- Sum/deviation contrasts
- (Reverse) Helmert contrasts
- many more
- Make your own!

Sum (or deviation) contrasts
==========================================================
```{r, results='asis'}
contrasts(dogs$breed) <- contr.sum
kable(coef(summary(lm(data = dogs, objects ~ breed))))
```

Interpreting sum/deviation contrasts
===========================================================
- The intercept $\beta_0$ is the grand mean of all the observations ($28.33$)
- $\beta_1$ is the difference between the grand mean and the mean of Beagle ($10 - 28.33 = -18.33$)
- $\beta_2$ is the difference between the grand mean and the mean of Border Collie ($60 - 28.33 = 31.67$)
- Terrier is never explicitly compared to the grand mean.
- In general: each level (except for the last level) is compared to the grand mean.

(Reverse) Helmert contrasts
==========================================================
```{r, results='asis'}
contrasts(dogs$breed) <- contr.helmert
kable(coef(summary(lm(data = dogs, objects ~ breed))))
```

Interpreting (reverse) Helmert contrasts
========================================================
- The intercept $\beta_0$ is the grand mean of all the observations ($28.33$)
- $beta_1$ is half of the difference between the mean of Beagle and the mean of Border Collie ($(60 - 10)/2 = 25$)
- $beta_2$ is half of the difference between the joint mean of Beagle and Border Collie and the mean of Terrier ($(15 - (60+10)/2)/2 = -10$)
- In general: each level is compared to the mean of the previous levels

Make your own contrasts
==========================================================
- General rules: You have one contrast per degree of freedom
- The dummy values in each contrast should sum to 0 (so that your intercept will be the grand mean)
- The sum of the absolute values of the dummy values in each contrast should be 2
- If you want to compare two levels, set one to be -1 and the other to be 1
- Factor levels that you don't want to compare should be set to 0


Example
==========================================================
- I want to compare level 1 (Beagle) to level 3 (Terrier):

|             | x1 | x2|
|------------:|---:|--:|
|Beagle       |   1|TBD|
|Border Collie|   0|TBD|
|Terrier      |  -1|TBD|

- This contrast sums to 0, so the intercept should be the grand mean (unless the other contrast is something really crazy)
- The absolute values sum to 2
- The coefficient will be $Mean(Terrier) - Mean(Beagle)$, so it will be positive if the mean for Terrier is greater than that for Beagle and negative if $Mean(Beagle) > Mean(Terrier)$

More complex comparisons
==========================================================
- To compare a mean of two factor levels to the mean of another factor (e.g. the mean of Beagle and Terrier vs. the mean of Border Collie), split them up: 
    - Set the two levels that you want to take the mean of both to .5 or -.5
    - Then set the third level to -1 or 1, respectively
    - The rules still hold:
        - The dummy values in each contrast should sum to 0 (so that your intercept will be the grand mean)
        - The absolute values should sum to 2
        - Factor levels that you don't want to compare should be set to 0

Example (continued)
==========================================================
- I want to compare level 1 (Beagle) to level 3 (Terrier):

|             | x1 | x2|
|------------:|---:|--:|
|Beagle       |   1|-.5|
|Border Collie|   0|  1|
|Terrier      |  -1|-.5|

- Both contrasts sum to 0, so the intercept should be the grand mean
- The absolute values sum to 2
- The coefficient for x2 will be $Mean(Terrier) - Mean(Beagle)$, so it will be positive if the mean for Terrier is greater than that for Beagle and negative if $Mean(Beagle) > Mean(Terrier)$

Defining your own contrast coding
========================================================
- You'll need the library "MASS"
    - If you don't have it yet, install it: `install.packages("MASS")`
- Put together the contrast matrix:
```{r, echo = FALSE}
x1 <- c(1, 0, -1)
x2 <- c(-.5, 1 , -.5)
# cbind: bind the vectors together as columns in a matrix
my_contrasts <- cbind(x1, x2)
```

Defining your own contrast coding (2)
========================================================
- Now you can assign the contrasts
- Important: you don't actually assign your home-made contrast matrix itself, but rather the transposed generalised inverse of the matrix
    - Why? That's just how R expects to get the contrasts...
- The only thing you need to be aware of is that `ginv` requires the `MASS` package to be loaded:
```{r, echo = FALSE}
library(MASS)
contrasts(dogs$breed) <- t(ginv(my_contrasts))
```

Interpreting the results
=======================================================
```{r, results='asis'}
kable(coef(summary(lm(data = dogs, objects ~ breed))))
```

- The intercept $\beta_0$ is the grand mean of all the observations ($28.33$)
- $\beta_1$ is the difference between the mean of Beagle and the mean of Terrier ($10 - 15 = -5$)
- $\beta_2$ is the difference between the mean of Beagle and Terrier together and the mean of Border Collie ($60 - (10+15)/2 = 47.5$)
- Once again, notice that the sample coefficients $b_0$, $b_1$, and $b_2$ are not *exactly* the same as the population coefficients $\beta_0$, $\beta_1$, and $\beta_2$.

Orthogonality of contrasts
=======================================================
- If contrasts are orthogonal, that means they do not share any variance
    - i.e. they are not correlated
- You can find out if your hand-made contrasts are orthogonal:
- Calculate the product of each row
- If the row products sum up to 0, the contrast is orthogonal:


|             | x1 | x2| x1 $\times$ x2|
|------------:|---:|--:|--------------:|
|Beagle       |   1|-.5|            -.5|
|Border Collie|   0|  1|              0|
|Terrier      |  -1|-.5|             .5|
|Total        |   0|  0|              0|


- Are our contrasts orthogonal? Yes!

Orthogonality of contrasts (2)
=======================================================
- Contrasts do not have to be orthogonal (also known as *independent*).
- But be aware that correlated contrasts may cause multicollinearity issues
    - Especially if you have an unbalanced design
    - Especially if you have an interaction design
- As long as you are aware of what exactly you are doing, you'll be fine
- As soon as you no longer know what you're doing, ask for help!

Transformations
========================================================
- In some cases, our dependent variable will not be normally distributed
- Example: reaction times -- you get a long right tail of slow responses
    - Fixation times in eye movements are very similar

Example
========================================================
- For example, the probability density function for fixation durations might look like this:

```{r, echo = FALSE}
curve(dlnorm(x, 5.5, .3), from = 0.1, to = 1000)
```

Example data
========================================================
- Example experiment: how long do people look at swear words vs. non-swear words?
    - Let's assume that the true means are 250 ms for non swear words and 300 ms for swear words
- Let's generate data based on this assumption
```{r, echo = FALSE}
set.seed("11233")
# 60 subjects
word_condition <- factor(c(rep("swear word", 30), rep("non swear word", 30)))
# rlnorm: Generate random samples from the lognormal distribution
fixation_time <- c(rlnorm(n = 30, 
                          mean = log(250), 
                          sd = .3), 
                   rlnorm(n = 30,
                          mean = log(265), 
                          sd = .3))
swear_exp <- data.frame(word_condition, fixation_time)
```

Running a linear model
========================================================
```{r, echo = FALSE}
linear_model <- lm(data = swear_exp, fixation_time ~ word_condition)
summary(linear_model)
```

Assumption test
========================================================
```{r, echo = FALSE}
shapiro.test(resid(linear_model))
```
- Clearly not normal!
- But notice how robust the analysis is. We still find the effect!
- Nevertheless, better to run a proper model with log transformed values as the dependent vairable


Running a log model
========================================================
```{r, echo = FALSE}
log_model <- lm(data = swear_exp, log(fixation_time) ~ word_condition)
summary(log_model)
```

Assumption test
========================================================
```{r, echo = FALSE}
shapiro.test(resid(log_model))
```

How to interpret a log model
========================================================
- Formula: $ln(y_{i}) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i$
- Let's rewrite that: $y_{i} = e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i} = e^{\beta_0} \times e^{\beta_1x_{i1}} \times e^{\beta_2x_{i2}}$
    - If this confuses you: we are using the natural logarithm ($ln$; R somewhat confusingly calls it `log`) here. This is a logarithm to the base $e = `r exp(1)`$. Maybe you'll remember that $e^{ln(x)} = x$
- Log models are *multiplicative* rather than *additive*

How to interpret a log model (2)
========================================================
- Example: Our swear word fixation time study
    - Fitted model: $ln(y_{i}) = 5.683 - .195 \times x_i$
    - Remember: We're using treatment contrasts. $x_i$ is 0 for non swear words and 1 for swear words
    - Predicted value for non swear words: $e^{5.683} \times e^{-.195 \times 0} = e^{5.683} =  293.82$
    - Predicted value for swear words: $e^{5.683} \times e^{-.195 \times 1} = 293.82 \times e^{-.195} = 293.82 * .823 = 241.81$
- Conclusion: Fixation times on swear words were 17.7% lower than fixation times on non swear words (*b* = -.195, *SE* = .074, *t* = -2.63, *p* = .011).

Logistic regression
========================================================
- What if we have a dichotomous dependent variable?
    - Yes vs. no, error vs. no error, alive vs. dead, pregnant vs. not pregnant
- Our example (from A. Johnson): Factors that make (or don't make) you fail your driving test
- 90 candidates
- Dependent variable: `Driving.Test`
- Predictor variables:
    - `Practice`: in hours
    - `Emergency.Stop`: Whether the candidate performed the emergency stop (yes or no)
    - `Examiner`: How difficult the examiner is (on a scale from 0 = easy to 100 = extremely difficult)
    - `Cold.Remedy`: How many doses of cold remedy the candidate had before the test

Where to start?
=========================================================
- We would like to use our standard regression model $y_{i} = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i$
- But our data are definitely not normally distributed (they aren't even continuous)
- First step: represent the data as probabilities rather than Yes or No
    - What's the probability of passing the test given that I've had (at most) 20 hours of practice, did my emergency stop, had at most an average examiner (50) and had only one cup of cold remedy?
    - Probabilities are still weird: They only go from 0 to 1, and they also aren't great for linear relationships
    - Let's try something else: odds
    
Probability and odds
========================================================
- Very popular in betting, since they make it easy to estimate the payout
- $Odds = \frac{P}{1-P}$
- For example: 
      - $P = .5$ gives you even odds $\frac{.5}{.5} = 1/1$
      - $P = .25$ gives you $\frac{.25}{.75} = 1/3$
      - $P = .75$ gives you $\frac{.75}{.25} = 3/1$
- Odds are nice because they aren't bounded, but for high probabilities they get very large very quickly ($P = .99 \Leftrightarrow Odds = 99/1$) and for small probabilities, they get very small very quickly ($P = .01 \Leftrightarrow Odds = 1/99$)
- What to do?

Log odds (logits)
=========================================================
- Just transform our odds (just like we did with our continuous fixation time variable earlier) by taking the natural logarithm: $logit = ln(Odds) = ln(\frac{P}{1-P})$
- Now we have a dependent measure that is suitable for linear relationships
- Our new logistic regression model is $ln(\frac{P}{1-P}) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i$
- If we want to get back to probabilities, we can exponentiate both sides of the equation: $\frac{P}{1-P} = e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i}$
- Solving this for $P$: $P = \frac{e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i}}{1 + e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i}} = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i)}}$

How do you fit a logistic regression line?
=========================================================
- Least squares won't work
- We can evaluate the **likelihood** of the parameters instead
- Probability: observations given parameters
- Likelihood: parameters given observations

Calculating likelihood
========================================================
- Very simple example: Let's assume we have the following data from flipping a coin: $Y = (H H H T H T H H)$
- Likelihood is the product of all the probabilities given a certain parameter value. In this case, we are trying different parameter values for the probability: 
    - We usually call the observed probability $p$, but since we are trying different population parameters, we'll give it a greek letter and call our population probability of observing "heads" (ignoring order) $\pi$ (that is, pi)
- $\pi = .5$: $L(Y|\pi = .5) = .5 \times .5 \times .5 \times .5 \times .5 \times .5 \times .5 \times .5 = .5^8 = .00039$
- $\pi = .25$: $L(Y|\pi = .35) = .25 \times .25 \times .25 \times .75 \times .25 \times .75 \times .25 \times .25 = .25^6 \times .75^2 = .00014$
    - $\pi = .25$ has a lower likelihood than $\pi = .5$.
- Let's try $\pi = .75$: $L(Y|\pi = .75) = .75 \times .75 \times .75 \times .25 \times .75 \times .25 \times .75 \times .75 = .75^6 \times .25^2 = .00111$
    - This is the highest one yet.

The likelihood function for logistic regression
========================================================
- Do you see a pattern here? For each element $Y_i$ in $Y$, the likelihood of $\pi$ is either
    - $L(Y_i|\pi) = \pi$ if $Y_i = H$, (e.g. $.75$ for $\pi = .75$), or
    - $L(Y_i|\pi) = 1 - \pi$ if $Y_i = T$, (e.g. $.25$ for $\pi = .75$)
- Then you get the likelihood for the full data set $Y$ by multiplying all the individual likelihoods
    - $L(Y|\pi) = \prod_{i = 1}^N{L(Y_i|\pi)}$
- You can simplify this a bit if you replace H with 1 and T with 0:
    -  $L(Y_i|\pi) = \pi^{Y_i} \times (1-\pi)^{(1-Y_i)}$
    - And combining the two equations above:
        - $L(Y|\pi) = \prod_{i = 1}^N\pi^{Y_i} \times (1-\pi)^{(1-Y_i)}$

Maximum likelihood
========================================================
- Likelihood gets a little unwieldy -- lots of very small numbers
    - Solution: take the log (who would have thought?)
        - Added bonus: Now our multiplication becomes a sum (remember that from calculus?)
          $log\;likelihood = \sum_{i = 1}^N Y_i\times ln(\pi) + (1-Y_i)\times ln(1-\pi)$
- Now we can simply try different values of $\pi$ until we find the one with the maximum likelihood
    - Remember that in logistic regression, $\pi$ is defined by our regression equation: $\pi = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i)}}$ 
    - Instead of simply trying different values of $\pi$, we have to try different values for $\beta_0$, $\beta_1$, etc. and compute $\pi$. This gets to be quite a lot of work.
    - Trying different values might not seem particularly elegant, but this is essentially what R or SPSS do -- no simple solution like with least-squares regression or ANOVA exists
      - This is (relatively) processing-intensive. One reason why psychologists didn't use logistic regression in the statistics-by-hand era.
      
Log likelihood as an indicator of model fit
=======================================================
- The log likelihood (LL) of the final model is an indicator of how well the model fit the data, just like $R^2$.
    - In fact, there are several ways to estimate $R^2$ from the log likelihood
- Log likelihood also enables us to make model comparisons
    - The test statistic in that case is $\chi^2$ -- more about that later
- Another measure is *deviance*, which is simply $-2$*LL
    - Conceptually, deviance is like the residual variance.
        - In the case of deviance, lower is better, of course.
- Closely related to this is Akaike's Information Criterion (AIC), which is -2LL+2*number of parameters (lower is better, so adding parameters makes the AIC worse)

Enough maths, let's just run this model
=======================================================
- These are our data (first 6 rows):
```{r, results='asis'}
driving_tests <- read.csv("driving_tests.csv")
kable(head(driving_tests))
```

Fitting the model
========================================================
- We use `glm` instead of `lm` (short for generalised linear model)
- We tell `glm` that our data are binomial and we want to use the logit function as the link
- Note that, for simplicity, we're not investigating interactions here (let's assume that in this example we simply aren't interested in them)
```{r, echo = FALSE}
driving_glm <- glm(data = driving_tests, 
                   formula = Driving.Test ~ Practice + Emergency.Stop + Examiner + Cold.Remedy, 
                   family = binomial(link = "logit"))
```

Model summary
========================================================
```{r, echo = FALSE}
summary(driving_glm)
```

Model summary explained
========================================================
- Deviance residuals:
    - For these, we calculate the deviance separately for each case and then take the square root. We also change the sign depending on whether the observed case was 1 or 0:
    - $d_i = s_i \sqrt{-2(Y_i\times ln(\pi) + (1-Y_i)\times ln(1-\pi))}$
        - with $s_i = 1$ if $Y_i = 1$ and $s_i = -1$ if $Y_i = 0$

How to interpret logistic regression residuals
========================================================
- Can't test if they are normally distributed (because they are not)
- But look out for very large residuals
- You can get standardised residuals using `rstandard`.
    - Look out for cases that are far away from 0.
```{r, results='asis'}
driving_residuals <- rstandard(driving_glm)
plot(driving_residuals)
```

Model summary explained (2)
==========================================================
- The coefficients

```{r, echo=FALSE, results='asis'}
kable(coef(summary(driving_glm)))
```

- Analogous to linear regression: *b* value, *SE*, significance test
- Using the **Wald** statistic instead of *t* tests
    - $z = \frac{b}{SE_b}$
    - Can be interpreted like *z* values from a standard normal distribution (that's how the *p*-value is computed)

Interpreting the b values
==========================================================
- If you take the exponential of the coefficients (using `exp()` or `e^` in R), you get an **odds ratio**
- Odds ratio = Odds after a unit change in the predictor divided by the original odds
- Example: According to our model, each hour of practice increases the odds of passing the test by a factor of $e^{0.12959} = `r exp(.12959)`$
    - e.g. if the odds were even (1/1) for X hours of practice, they would be slightly better than even (1.138/1) for X+1 hours of practice
- On the other hand, each "unit" of examiner difficulty decreases the odds of passing the test by a factor of $e^{-0.03485} = `r exp(-.03485)`$
    - e.g. if the odds were even (1/1) for an instructor with a difficulty of X, they would be slightly worse than even (0.9658/1) for an instructor with a difficulty of X+1

Model summary explained (2)
==========================================================
- Null deviance: deviance from a model that estimates all values using the overall probability of passing the test (without taking into account the 4 predictors)
- Residual deviance: deviance from the current model (note that we're losing 4 df due to the 4 predictors)
- AIC: residual deviance + 2*model df (note that the model has 5 df, including one for the intercept)

Model comparisons (LRT)
==========================================================
- Deviance has some neat properties
    - We can compare likelihoods just like we compared mean squares in the *F*-test: by dividing them
    - That is, we compute a likelihood ratio: $LR = \frac{L_{baseline}}{L_{new}}$, where *baseline* is the simpler model and *new* the more complex model.
    - Now we can convert this likelihood ratio into a deviance: $deviance_{LR} = -2\frac{ln(L_{baseline})}{ln(L_{new})} = deviance_{baseline} - deviance_{new}$
    - And now the most fun part: If the $H_0$ that the two models explain the data equally well is true, this likelihood-ratio deviance is distributed according to a $\chi^2$ distribution.
    - The $\chi^2$ distribution has one parameter, degrees of freedom
    - $df = k_{new}$ - $k_{baseline}$, where $k$ is the number of parameters (including the intercept)

Model comparisons
==========================================================
- Now we can get a *p*-value! This is called the **Likelihood ratio test (LRT)**
- So, if we want to test if the model is better than a model with just the intercept, we can do an LRT
- $\chi^2 = deviance_{null} - deviance_{model} = 124.366 - 82.572 = 41.794$
- $df = k_{null} - k_{model} = 89 - 85 = 4$ 
- $p(\chi^2(4) \geq 41.794) < .01$
- This is equivalent to the overall *F*-test for the model.

Model comparisons (2)
========================================================
- We can use model comparisons to test how specific predictors contribute to the whole model (analogous to the *F*-tests in linear regression)
- For this, you can use the `Anova` command from the `car` package:
```{r, echo = FALSE}
library(car)
Anova(driving_glm)
```

Model comparisons (3)
========================================================
- This analysis of deviance follows the same logic as the ANOVA in the linear regression case
- You can do Type I, Type II, and Type III LRT tests (they are not sums of squares in this case)
- The LRTs are better tests than the Wald tests, since the Wald tests might be prone to overinflating the SE, leading to Type II error.
- You can also use the LRT to directly compare a model to another (just like in the linear regression case). For this, you can use the `anova` command (lower case `a`).

Diagnostics and assumption tests
========================================================
- We do not assume normality (so nothing to test for that one)
- All the influence measures from linear regression work in logistic regression as well
    - See Class 5 notes for explanations and criteria
    
```{r,results='asis'}
kable(head(influence.measures(driving_glm)$infmat))
```

Diagnostics and assumption tests (2)
========================================================
- Multicollinearity:
- You can get Variance Inflation Factors (VIFs)
    - Again, see Class 5 notes for explanations and criteria
```{r, echo = FALSE}
vif(driving_glm)
```

Linearity
=============================================================
- This is a new one: Test if the effects of the predictors on the logits are actually linear
- How to do it: run a model that includes interactions between each *continuous* predictor and its logarithm
- Test each interaction in a separate model
- If you have 0s in your data, you might need to replace them by a very small value since log(0) = -Inf
- If the interaction is significant, linearity is violated

Linearity (2)
=============================================================
```{r, results='asis'}
# remove 0s by adding a very small number to Cold.Remedy
# test each factor in a separate model
driving_tests$Cold.Remedy_no_0 <- driving_tests$Cold.Remedy + 1
driving_glm_linearity <- glm(data = driving_tests, 
                             formula = Driving.Test ~ Practice + Emergency.Stop + 
                               Examiner + Cold.Remedy_no_0 +
                               Practice:log(Practice) + Examiner:log(Examiner) +
                               Cold.Remedy_no_0:log(Cold.Remedy_no_0), 
                             family = binomial(link = "logit"))
```


Linearity(3)
=============================================================
```{r, results='asis'}
kable(coef(summary(driving_glm_linearity)))
```

- There is an issue with the linearity of `Cold.Remedy`, suggesting that this variable might have to be transformed/coded as a factor/etc.
    - Maybe this is why we don't see a significant effect?

Reporting it
=============================================================
A logistic regression was conducted where the dependent variable was passing a driving test and the predictor variables were hours of practice, whether an emergency stop was successfully executed, how much the examiner was difficult, and amount of 'cold remedy drinks' consumed.  90 cases were examined and the model was found to significantly predict whether the test was passed (omnibus chi-square = 41.79, df=4, p<.001). that practice and examiner were the only two variables that reliably predicted if the driving test was passed. Increases in practice was associated with increased rate of passing (odds of passing increased by 1.14 per hour of practice, *b* = .130, SE = .03, *z* = 4.28, *p* < .01). Increases in the examiner being an difficult reduced the rate of passing (odds of passing decreased by 0.96 per unit of difficulty rating, *b* = -.00349, SE = .013, *z* = -2.679, *p* < .01). None of the other predictors reached significance (all *p*s > .05). There were no issues due to multicollinearity or influential cases, however, the linearity assumption was violated for the cold remedy drinks predictor. 

Linear mixed models (LMMs)
============================================================
- The final step to greatness!
    - Note that we can really only scratch the surface here.
- Main issue:
    - We know how to to regressions for continuous and discrete DVs now
    - We know what the regression equivalent of a between-subjects ANOVA is and we can take the regression analysis much further than an ANOVA or ANCOVA would let us
    - However: 
    - What if we have a within-subject or repeated measures design?
    - What if there is some other underlying correlation in the data 
    - e.g. data collected from students in different classes in different schools
    - Surely the classes and schools share some variance -- how to account for that?
                
Moving from linear regression to linear mixed models
=============================================================
- In repeated-measures ANOVA, we've dealt with within-subjects effects by removing the variance due to subject differences from the error
    - Essentially, we have added a "subject" factor to the model
    - Linear mixed models enable us to do the same thing for regression analyses

Problem: how to add subject as a factor
=============================================================
- We could simply add a "subject" factor to the predictors
    - This would reflect the systematic differences between subjects
        - But that's not quite right: how do we deal with a factor with 40 levels?
        - Also, we want to generalise our model to more than those 40 subjects that are in the analysis
        - How do we do that?
    - Subject is really like a random variable: we get a different set each time we run the experiment
    - Instead of analysing the subject effect in a generalisable way, we really just want to get rid of the subject variance in the most efficient way possible

Problem: how to add subject as a factor (2)
=============================================================
- Fixed effects vs. random effects
    - Fixed effects: repeatable, generalisable (e.g. experiment condition)
    - Random effects: non-repeatable, sampled from a general population
    - Mixed effects models include both fixed and random effects
- Another issue with including subject as a fixed effect:
    - Each subject would take up a degree of freedom
    - That would majorly impact the power of our analysis
    - LMMs solve this issue by a procedure called **shrinkage**
    
Shrinkage
===============================================================
- Conceptually, LMMs allow subjects to have individual effects (e.g. in an eye-tracking experiment subject 1 might have an intercept of 200 ms, while subject 2 might have an intercept of 210 ms), but they pull each subject's effects
towards an underlying distribution of subject effects
- This reflects the idea that if 20 other subjects have intercepts between 180 and 220 ms, the current subject is unlikely to have an intercept of 400 ms, even though it looks like that from the data
- Shrinkage also helps majorly with missing data issues (although it won't fix them for you!)
- The downside of shrinkage is that it isn't clear what the df_{Error} should be
    - This leads to some issues later on.
    
Example
==============================================================
A PhD student wants to investigate whether our mood affects how we react to visual scenes. In order to do this, she showed 40 subjects a total of 40 scenes. There are two version of each scene: one contains people, the other one doesn't -- everything else is identical. The PhD student spent a considerable amount of time taking photos to ensure this (until her supervisor got a bit impatient). Before the experiment, all subjects were asked to rate their current mood on a scale from 0 (very sad) to 100 (very happy). They then looked at each scene and rated how much they liked it on a scale from 0 (hate it) to 20 (love it). The student's hypothesis is that if you are happy, you should want to see scenes with people. If you are unhappy, you should prefer scenes without people. The data are given below. Will the student find what she is looking for? Or will she have to start from scratch and be in even more trouble with her supervisor?

Example Data
==============================================================
- Subject: Subject ID (1-40)
- Item: Item ID (1-40)
- Scene Type: within-item factor ("no people" vs. "people")
- Mood: between-subject factor (scale from 0--100)
- Rating: Dependent variable (scale from 0 to 20)
```{r, echo = FALSE}
# Start by loading the data
scene_liking <- read.csv("Class 6 exercise data.csv")
```

Looking at the data
=========================================================
```{r, results='asis'}
kable(head(scene_liking))
```

Looking at the data (2)
=========================================================
```{r, echo = FALSE}
str(scene_liking)
# We should set subject and item to be factors
scene_liking$subject <- factor(scene_liking$subject)
scene_liking$item <- factor(scene_liking$item)
```

Calculating means
=========================================================
- Let's get condition means for scene type
- We can't really use `ez` for this, since we have two random variables
- In theory, we could report means by subject or means by item
- Either one would be fine, but usually people report subject means.
    - We use `melt` and `cast` from the `reshape` package to calculate the means
```{r, echo = FALSE}
library(reshape)
# set rating as the dependent (measure) variable
scene_liking.m <- melt(scene_liking, measure = "rating")
# collapse over item; calculate means
scene_liking.c <- cast(scene_liking.m, subject + mood + scene ~ variable, mean)
```

Calculating means (2)
=========================================================
```{r, echo = FALSE}
head(scene_liking.c)
```

Calculating means (2)
=========================================================
- Now we can use this to calculate our means for the scene condition
```{r, echo = FALSE}
(mean_people <- mean(subset(scene_liking.c, scene == "people")$rating))
(mean_no_people <- mean(subset(scene_liking.c, scene == "no people")$rating))
```

Calculating means (3)
=========================================================
- Let's also get sd, N, and SE
```{r, echo = FALSE}
(sd_people <- sd(subset(scene_liking.c, scene == "people")$rating))
(sd_no_people <- sd(subset(scene_liking.c, scene == "no people")$rating))
(N_people <- length(subset(scene_liking.c, scene == "people")$rating))
(N_no_people <- length(subset(scene_liking.c, scene == "no people")$rating))
(SE_people <- sd_people/sqrt(N_people))
(SE_no_people <- sd_no_people/sqrt(N_no_people))
```

Plotting the interaction
==========================================================
- We're really interested in the interaction between `scene` and `mood`.
    - Unfortunately, mood is a continuous variable
    - How to plot this?
- Use `qplot` from `ggplot2` with `geom = "smooth"`
    - This will give you a plot showing a smoothed conditional mean for each value of mood

Plotting the interaction (2)
=========================================================
```{r, message=FALSE, warning=FALSE}
library(ggplot2)
qplot(data = scene_liking, y = rating, x = mood, colour = scene, geom = "smooth")
```

Plotting the interaction (2)
=========================================================
- Looks like the student was right!
- Also looks like the effect is not really completely linear
    - Maybe this is due to the subject and item effects in the data?
    - Let's find out!

Start with linear regression
=========================================================
- Let's check our contrasts for `scene`
```{r, echo = FALSE}
contrasts(scene_liking$scene)
```
- Are we happy with this?
    - Sure -- we just have to be aware of the coding when we interpret the coefficients

Run the model
=========================================================
```{r, results='asis'}
scene_lm <- lm(data = scene_liking, rating ~ mood * scene)
kable(coef(summary(scene_lm)))
```

- Where did the interaction go?
- Let's do some quick diagnostics

Regression diagnostics
=========================================================
- Multicollinearity?
```{r, echo = FALSE}
vif(scene_lm)
```
- Aha! Those VIFs are quite  a bit larger than 1. That spells trouble.
- What is wrong?

Addressing multicollinearity
=========================================================
- What is wrong?
- We forgot to center the continuous predictor `mood`
- Let's fix this:
```{r, echo = FALSE}
scene_liking$mood <- scale(scene_liking$mood, scale = FALSE) # See Class 5
```

Run the model again
=========================================================
```{r, results='asis'}
scene_lm <- lm(data = scene_liking, rating ~ mood * scene)
kable(coef(summary(scene_lm)))
```

- Still not quite there...
- Let's do more diagnostics

Regression diagnostics -- again
=========================================================
- Multicollinearity?
```{r, echo = FALSE}
vif(scene_lm)
```
- The VIFs are fine now.

Influential cases
=========================================================
```{r, results='asis'}
kable(head(influence.measures(scene_lm)$infmat))
```

- Any Cook's d greater than 1?

```{r, echo = FALSE}
sum(cooks.distance(scene_lm) > 1 )
```

- Doesn't look like it, so we should be fine here.

Normality
=========================================================
```{r, echo = FALSE}
shapiro.test(resid(scene_lm))
```
- Whoa! But we saw this in the plot already

Q-Q Plots
===========================================================
- Here's a visual way to assess normality
- Quantile-Quantile Plot: Split data into a number of quantiles and plot them against the quantiles of a hypothetical normal distribution

```{r, echo = FALSE}
qqnorm(resid(scene_lm))
# if the distribution is normal, the points should be on this line
qqline(resid(scene_lm))
```

How to fix this?
===========================================================
- As a first step, remember that there are subject and item effects in these data
- `lm` can't account for them, so we need something more powerful
- Linear Mixed Models!
- We use the function `lmer` ("Linear mixed effects regression") from the `lme4` package
- If you don't have `lme4` yet, install it by typing `install.packages("lme4")` in the Console

Adding random subject and item effects
===========================================================
- As a first step, we want our model to allow subjects and items to have different intercepts
    - For example, Subject 1 might just really dislike the whole experiment and rate all scenes lower
    - Or Item 33 might be particularly ugly and be disliked by all subjects
- Formally, our model will look like this: $y_{ij} = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \gamma_{0i} + \gamma_{0j} + \epsilon_{ij}$, where $y_{ij}$ is the response of subject $i$ to item $j$, $\gamma_{0i}$ is the intercept for subject $i$ and $\gamma_{0j}$ is the intercept for item $j$

Running the model
============================================================
- In `lmer`, we specify the model in a formula just like in `lm`, but we add random effects terms, e.g. `(1|subject)`
    - The left side of the pipe stands for the random effect, the right side stands for the group for which we want to define the random effect
    - `1` stands for the intercept. It is implicitly added, *except* when there is no other predictor
```{r, echo = FALSE}
library(lme4)
scene_lmm <- lmer(data = scene_liking, rating ~ scene * mood + (1|subject) + (1|item))
```

Examining the model
============================================================
```{r, echo = FALSE}
summary(scene_lmm)
```

Understanding the model output
===========================================================
- Just like in logistic regression, LMMs are fitted in an iterative procedure using Maximum Likelihood (ML)
    - Actually, `lmer` uses a slightly modified criterion called Restricted Maximum Likelihood (REML)
- Residuals can be interpreted just like in a regular linear model
- Random effects: Here we get an estimate of the variance (and standard deviation) explained by the random intercepts
    - We also get an estimate of the residual variance
- Check the number of observations to see if there are any missing that shouldn't be missing

Coefficients
===========================================================
```{r, results='asis'}
kable(coef(summary(scene_lmm)))
```

- First thing you notice: There's no *p* value!
- That's because, due to the shrinkage procedure, it isn't clear what the df of that *t*-value should be
- In general, if the number of subjects is > 30, we should be able to interpret the *t* value like a *z* value, meaning that any *t* > 2 or < -2 should be significant

Correlation of fixed effects
==========================================================
- These are the estimated correlations of the fixed effects
    - If any of these is > .8, you're in multicollinearity trouble!
    
Model comparisons
==========================================================
- Unfortunately, *F*-tests won't work, because we don't know what the $df_{Error}$ would be
- But we can do the likelihood ratio test (LRT)
- As always, we use `Anova` from `car`. This one gives us *p* values!
```{r, echo = FALSE}
library(car)
Anova(scene_lmm)
```

More model diagnostics
=========================================================
- Something still seems to be wrong with this model. How about testing the normality assumption again?
```{r, echo = FALSE}
shapiro.test(resid(scene_lmm))
```
- Still significant? Maybe there still is a random effect that we haven't accounted for.

Random slopes
=========================================================
- We can also allow the regression slopes to vary by subject or item.
- What are possible random slopes that we could consider?
    - Important: in theory, you could add random slopes for all fixed effects, but in practice, your data might not have enough information to fit these
    - In this case, there simply isn't enough data to fit random slopes for the interaction
      - How do you know this?
      - Well, your model will simply fail to converge if there is not enough data for a solution!
      - Even if there *is* enough data, multicollinearity can cause convergence failures, too.
    - In our case, some reasonable random slopes would be `scene` for subjects (do some people react more strongly to scenes with people than others) and `mood` for items (are some items really hated by people in a bad mood?)
    
Random slopes (2)
=========================================================
- If we include a random slope for subjects for $beta_1$, our model looks like this:
$y_{ij} = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \gamma_{0i} + \gamma_{1i} x_{1} + \gamma_{0j} + \epsilon_{ij}$
- We can tell `lmer` to fit such models like this (note that the intercept is implicit again in `(mood|item)` and `(scene|subject)`.
    - Note that we don't have enough data to include both random effects in one model.
```{r, echo = FALSE}
scene_lmm_mood <- lmer(data = scene_liking, 
                       rating ~ scene * mood + (1|subject) + (mood|item))
scene_lmm_scene <- lmer(data = scene_liking, 
                        rating ~ scene * mood + (scene|subject) + (1|item))
```

Testing the effect of random slopes
===========================================================
- We can use the LRT to test whether the slopes actually improve the models.
    - We use the `anova` command (lower case `a`) to compare each random slope model with the random intercept model we fitted earlier
```{r, echo = FALSE}
anova(scene_lmm, scene_lmm_mood)
```
- Seems to improve the model! (Note that R automatically uses ML instead of REML for model comparison)

Testing the effect of random slopes
===========================================================
```{r, echo = FALSE}
anova(scene_lmm, scene_lmm_scene)
```
- No improvement here.

Diagnostics -- yet again!
==========================================================
```{r, echo = FALSE}
shapiro.test(resid(scene_lmm_mood))
```
- Looks like adding a random slope for `mood` also (mostly) fixed our normality problem

Interpreting the coefficients
==========================================================
```{r, results='asis'}
kable(coef(summary(scene_lmm_mood)))
```

- Now we have significant effects!
- Remember that scene was coded as 0 = no people, 1 = people
    - Looks like, on average, subjects gave the scene a rating that was -.26 lower when it contained people than when it didn't.
- The interaction is also significant. When the scene contained no people, there was a very weak, non-significant negative effect of mood. 
    - When the scene did contain people, there was a significant change in the effect of mood (with each point on the mood scale increasing the picture rating by -.007 + .0101 = .0031). Not a huge effect, but significant.

Writing it up
==========================================================
- See the exercise!

Thank you!
==========================================================
- I know this was (and still is) a massive effort
- Thank you for staying motivated and engaging with the material.
- As always, come see me if you have questions!