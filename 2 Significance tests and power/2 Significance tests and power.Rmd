---
title: "Advanced Statistics Lecture 2"
author: "Significance tests and power"
date: "Press F for fullscreen"
output: 
  revealjs::revealjs_presentation:
    self_contained: true
    transition: fade
    css: "robot-lung.css"
---  

```{r prepare, echo = F, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, out.height = "50%", fig.width = 8, fig.height = 4.5, cache = TRUE)
library(tidyverse)
```
# Making conclusions about the world

- Last time, we talked about using *inference* to learn about the world. One way of doing this is to use null-hypothesis significance testing (NHST). Another one is Bayesian Inference, which we will talk about later.
- NHST is one answer to the question of how we can go from observing the world (collecting a sample of data) to making conclusions about it.
    - In other words, it is an answer to the question of what our *sample* can tell us about the *population* we took the sample from
    
# Using NHST
- What we do in NHST is to propose a hypothesis about something in the world
- How about this one: 

> "There is a genetic disposition towards depression."

- Is this a good hypothesis? Remember that Popper said that a strong hypothesis needs to be falsifiable.
    - In order to falsify this hypothesis, we would need to investigate essentially every single gene to see if there is a relationship before we can falsify it.

# Coming up with a better hypothesis

- How about narrowing it down to one specific gene?
    - You might remember that depression is often treated using Selective Serotonin Reuptake Inhibitors (SSRIs)
    - What about the gene that encodes the serotonin (also called *5-HT*) transporter protein (*5-HTT*) which takes serotonin back into the presynaptic neuron (i.e. performs the reuptake)? If there is a mutation in this gene, maybe the effect of serotonin is decreased?
        
    - There is a particular region in the gene that encodes *5-HTT* (*SLC6A4*) called *5-HTTLPR*, of which there are two main variants, "short" and "long".
        - So how about this hypothesis?
        
> "There is a relationship between the *5-HTTLPR* variant that a person has and depression."

# An even more specific hypothesis

- How can we measure "depression"?
    - Many possibilities:
        - Dichotomous (or binary) outcome: 
            - Diagnosis of depression: (yes/no)
        - Ordinal outcome:
            - Strength of depression symptoms: Scale from 1-9
        - Continuous outcome:
            - Sum of 30 depression outcome questions -- i.e., a depression inventory score (this is not perfectly continuous, but close enough)

- Let's pick one hypothesis:

> "People with the short *5-HTTLPR* variant score lower on Beck's Depression Inventory (BDI) than people with the long variant."

- That's very easily falsifiable.

# A slightly less specific hypothesis

- But we have a problem here: We actually have no intuition which of the variants would lead to less severe depression
    - So let's make this a *difference* rather than a *directional* hypothesis:

> "There is a difference in BDI score between people with the short *5-HTTPLR* variant and people with the long variant.

- This still seems sort of easy to falsify, but we run into a problem:
    - Researcher X finds an average difference of 5 between the groups he has tested and claims that this is evidence in favour of the hypothesis
    - Researcher Y says that she has done the BDI on lots of patients, and there is so much variability in the results that you could easily find a difference this large or even larger between any groups of the same size regardless of their *5-HTTPLR* variant.
- How can we tell who is right?
    
# The logic of NHST

- Imagine what the world would be like if your hypothesis was false (i.e. Researcher Y was right)
- In other words, what would the world look like your hypothesis was false?

> Null hypothesis: "On average, there is no difference in BDI score between people with the short *5-HTTPLR* variant and people with the long variant."

- Imagine a world in which people with the two variations are exactly identical -- that is, they are all part of a single population
- This "imagination" needs to be very precise, so let's rather call it a model of the world
- This model needs to make precise predictions about our observations given that the null hypothesis is true
- For this, we need maths -- there is no other way.
    
# Making a model of the world (with a calculator)

- There are many ways of making mathematical models of the world, but the mathematically simplest way is to use a probability distribution
- Different hypotheses require different distributions. In this case, we want to model a continuous outcome (the BDI scores).
    - The simplest way to do this, for most situations, is the normal distribution
      - We said last week that, when we're taking random samples from *any* distribution (with a mean and a variance), the means (and sums) of these samples will approximately follow a **normal distribution** if the sample size is large enough (in general, $n \geq 30$ is a good rule of thumb).
- So how can we use a normal distribution to simulate what would happen if we just take two samples from the same population and calculate the average (or *mean*).
- How often would we see a mean of 5 or an even larger mean?

# Finding the right normal distribution for our situation

- We want a normal distribution that enables us to calculate how likely it is to observe a mean of 5 if the null hypothesis is true 
    - that is, the distribution that we get when we take two samples, calculate the mean for each, and then subtract them to get the difference
- Such a distribution is called a *sampling distribution of the mean*
    - We can model it using a normal distribution. Every normal distribution is characterised by just two *parameters*: Its mean, $\mu$, and its variance, $\sigma^2$.
- Using maths (you can read up on it if you don't believe me), we can find out how the mean of the sampling distribution is related to the mean of the population we are sampling from:
    - It's the same!
$$\mu_{\bar{x}} = \mu$$ 

# Parameters for our normal distribution

- Even better, since the null hypothesis says that both samples come from the same population, the average difference will be 0
    - That takes care of one parameter very easily!
- Now for the variance:
    - The variance of the sampling distribution of the mean is closely related to the variance of the population we're sampling: It's the population variance divided by the sample size:
$$\sigma_{\bar{x}}^2 = \frac{\sigma^2}{n}$$
- Again, I'm not going to explain why, but I have some extra slides that do if you are interested/doubtful!


# Doing a hypothesis test

- Let's say we somehow know exactly what the population variance for the BDI scale is
- Let's say we just somehow know that it's 100
    - Is a difference of 5 impressive in that case?
- What would our sampling distribution of the mean look like if we plug this in?
    - The mean is still 0 (because we're still considering the null hypothesis): $\mu_{\bar{x}} = 0$
    - The variance is $\sigma_{\bar{x}}^2 = \frac{100}{n}$
- We are missing a value: the sample size!
- Let's say we have just 2 patients in each group, i.e. 4 patients total
  - Then the variance is $\sigma_{\bar{x}}^2 = \frac{100}{4} = 25$
- Now we have our distribution and can use it to make a plot!

# Making a plot of the probability density

- We can highlight the part of the curve that corresponds to observations greater than 5

```{r, echo = F}
cord.x <- c(5,seq(5,50,0.01),50)
cord.y <- c(0,dnorm(seq(5,50,0.01),0,5),0)
curve(dnorm(x,0,5),xlim=c(-15,15),main='Normal distribution with mean of 0 and variance of 25')
polygon(cord.x,cord.y,col='skyblue')
```

# Getting a probability

- So 5 (or more) is not the most likely outcome, but how likely is it
- Normal distributions are very simple, but not so simple that you can easily calculate probabilities by hand (you need integrals)
    - Luckily, we have probability tables and computer programmes for that
- What is the probability of getting a value of 5 or greater in this distribution?
- In order to use a table, we need to standardise our data (i.e. the observed mean difference of 5)
- We can do that by just subtracting the mean from it (remember that in our case the mean is 0) and dividing it by the standard deviation (the square root of the variance)
- The resulting standardised value is called a *z*-value

$$
z(5) = \frac{5 - 0}{\sqrt{25}} = \frac{5}{5} = 1
$$

# Getting a probability 
- What is the probability of getting a *z*-value of 1 given that the null hypothesis is true?
    - Check the standard normal distribution -- you can easily find a table

```{r, echo = F}
cord.x <- c(1,seq(1,3,0.01),3)
cord.y <- c(0,dnorm(seq(1,3,0.01)),0)
curve(dnorm(x,0,1),xlim=c(-3,3),main='Standard Normal')
polygon(cord.x,cord.y,col='skyblue')
```

# Get the p-value

- Get Excel (or another software) to give you the area under the curve.
- $p(z > 1) = 1 - p(z < 1)$, so `=1-NORM.S.DIST(1,TRUE)`
- Result: `r 1-pnorm(1)`
- So there is only a `r (1-pnorm(1))*100 %>% round(2)`% chance that we will observe a difference greater than 5 if the null hypothesis (the $H_0$) is true.
- Is that low?
    - Is that high?
- Ronald Fisher suggested hat a probability of .05 is so low that we can reject the null hypothesis and corroborate the alternative hypothesis
    - Nobody else had a better idea (although see your coursework!)
- According to this criterion, we would not be able to reject the null hypothesis in this case.

# Confidence intervals

- We could also construct a *confidence interval*
- It is another way of expressing how far our sample mean is from the mean expected by the null hypothesis (in our case, 0)
- Let's start with a really simple case: let's say the population mean $\mu$ is `r (pop.mean <- 0)` and the population sd $\sigma$ is `r (pop.sd <- 1)`.
    - We can calculate an interval that represents 95% of the area under the curve, i.e. in 95 out of 100 samples from this population we would get a value within this interval
    
# Confidence intervals (2)

- So, let's get the interval that $\mu_{\bar{x}}$ is going to be in 95% of the time.
- We want something like this:

```{r, echo = F}
cord.x <- c(-3,seq(-1.96,1.96,0.01),1.96)
cord.y <- c(-3,dnorm(seq(-1.96,1.96,0.01)),0)
curve(dnorm(x,0,1),xlim=c(-3,3),main='Standard Normal')
polygon(cord.x,cord.y,col='skyblue')
```

# Confidence intervals (3)

- Let's start with the standard normal distribution (**z-scores**)
- We want to get an interval that includes 95% of the area under the curve
  - That means we need to take off 2.5% on every side
  - For the left interval boundary, we want the x value that is greater than or equal to 2.5% of x values
  - ask Excel for the *z*-value: For this, we use the *inverse* of the standard normal distribution: `=NORM.S.INV(0.025)`
  - Result: `r qnorm(.025)`

# Confidence intervals (4)

- For the right interval boundary, we want the x value that is greater than or equal to 97.5% of x values.
- ask Excel for the *z*-value: For this, we use the *inverse* of the standard normal distribution: `=NORM.S.INV(0.975)`
  - Result: `r qnorm(.975)`
- If you've done statistics before, these numbers should be pretty familiar to you.
- Now what if we want to do this for another distribution? Just multiply the standard deviation with 1.96 and add and subtract it to/from the mean:
$\bar{x} = \mu \pm 1.96 \times \sigma_{\bar{x}}$
- If we are doing it for the sampling distribution of the mean:
  - $\bar{x} = \mu \pm 1.96 \times \frac{\sigma}{\sqrt{n}}$

# Confidence intervals (5)

- If you construct a confidence interval (CI) around the sample mean, it can tell you how far it is away from the population mean proposed by the null hypothesis.
- Let's do a confidence interval for our sample of 4 participants with mean of 5 and the population variance of 100 (or population standard deviation of 10)
- Plug the values into the formula: $\bar{x} = \mu \pm 1.96 \times \frac{\sigma}{\sqrt{n}}$
- $\bar{x} = 5 \pm 1.96 \times \frac{10}{\sqrt{4}}= 5 \pm 9.8$
- So the 95% confidence interval goes from -4.8 to 14.8
- If you take 100 samples and construct confidence intervals from them, the true population mean will be inside the confidence interval 95% of the time
    - This means that if the CI does not contain 0, you can reject the $H_0$

# Making things more realistic

- Usually, you do not know the population variance -- all you have is your sample
- So how can we do our hypothesis tests and make our confidence intervals?
- Can you just use the sample standard deviation ($s$) to estimate population standard deviation ($\sigma$)?
- Or the equivalent question: is sample variance ($s^2$) a good estimator of population variance ($\sigma^2$)?
- The maths here are a bit more tricky, but we can make it easier and just simulate the problem!
- If you want to know more, you can watch [Julian Parris' video](https://www.youtube.com/watch?v=Juo5NJSHlMM) (see also on Brightspace).

# Population variance and sample variance: plots

- Let's take 1000 samples of size 2 from the standard normal distribution: $X \sim N(0,1)$ and calculate the variance of each sample, then plot the resulting distribution:

```{r, echo = FALSE}
nsim = 1000
run_variance_simulation <- function(sample_size = 2, number_of_simulations = 1000, population_mean = 0, population_sd = 1)
  {

sample_variances <- replicate(number_of_simulations, var(rnorm(n = sample_size, mean = population_mean, sd = population_sd)))
}

#Define a new plot function so that the plot titles are correct
make_variance_hist_and_plot <- function(sample_variance){
  par(mfrow=c(1,2)) # 
  hist(sample_variance,freq=F, breaks = 30, main = "Sample variance")
  plot(density(sample_variance), main = paste("Mean = ", round(mean(sample_variance),2) , "SD = ", round(sd(sample_variance),2)))} 

make_variance_hist_and_plot(run_variance_simulation(sample_size = 2, number_of_simulations = nsim, population_mean = 0, population_sd = 1))
```


# Population variance and sample variance: plots

- Let's do the same with 1000 samples of size 4:

```{r, echo = FALSE}
make_variance_hist_and_plot(run_variance_simulation(sample_size = 4, number_of_simulations = nsim, population_mean = 0, population_sd = 1))
```


# Population variance and sample variance: plots

- Let's do the same with 1000 samples of size 10:

```{r, echo = FALSE}
make_variance_hist_and_plot(run_variance_simulation(sample_size = 10, number_of_simulations = nsim, population_mean = 0, population_sd = 1))
```

# Population variance and sample variance: plots

- Taking samples of size 100 from the standard normal distribution: $X \sim N(0,1)$ and calculating the variance:

```{r, echo = FALSE}
make_variance_hist_and_plot(run_variance_simulation(sample_size = 100, number_of_simulations = nsim, population_mean = 0, population_sd = 1))
```

# Sample variance as an estimator of population variance

- On average, we keep getting an sd (and a variance of 1)
- Looks like sample variance (at least if we calculate it dividing by $n-1$ instead of $n$ is a pretty good estimator (unbiased actually)
- We can plug the sd of the sample into the equation for the SD of the sampling distribution (also called the *standard error*):
$$
\begin{aligned}
SE_{\bar{x}} = \frac{s}{\sqrt{n}}
\end{aligned}
$$
(Note that we are ignoring the question if the relationship between $s$ and $s^2$ is really the same as the relationship between $\sigma$ and $\sigma^2$. Feel free to simulate that, if you are really curious.)

# The problem with $\sigma$
- But as you saw in Julian's video, the estimate of $\sigma$ from $s$ is sometimes quite far away from the correct $\sigma$, especially for small sample sizes.
- This means that our estimate for $\sigma$ is going to vary. Its accuracy will depend on the sample size.

# The chi-square distribution

- But there's another striking thing going on here. Look again at the distribution of variances for sample size 2:

```{r, echo = FALSE}
make_variance_hist_and_plot(run_variance_simulation(sample_size = 2, number_of_simulations = nsim, population_mean = 0, population_sd = 1))
```

# The chi-square distribution (2)

- This is definitely not a normal distribution, and it is skewed right, meaning that the errors are not symmetrical: a small error to the left of the mean is more likely than a small error to the right

# The chi-square distribution

- What is a variance again?
    - Definition: $s^2 = \frac{\sum\limits_{i=1}^{n}(x_i - \mu)^2}{n-1}$
    - If we have a standard normal distribution ($\mu = 0$ and $\sigma = 1$): $s^2 = \frac{\sum\limits_{i=1}^{n}z_i^2}{n-1}$
    - If $n = 2$: $s^2 = \sum\limits_{i=1}^{n}z_i^2$
    - The $\chi_1^2$ distribution is the distribution of the square of a random variable following the standard normal distribution
        - i.e. squares of *z*-values are $\chi_1^2$ distributed
        
# Chi-square distributions

- There is more than one $\chi^2$ distribution:
    - The sum of the squares of two **independent**, squared random variables following the standard normal distribution (i.e. *z*-values) follows the $\chi_2^2$ distribution: $\chi_2^2 = z_1^2 + z_2^2$
    - In general, $\chi_n^2 = \sum\limits_{i=1}^{n}z_i^2$
      - Here, $n$, the number of independent $z^2$ variables is also known as the **degrees of freedom** of the $\chi^2$ distribution.
      
# Chi-square distributions plotted

```{r, echo = F}
library(ggplot2)
x <- seq(from = 0.1,by = .01,to = 20)
ggplot(data.frame(x = x), aes(x = x)) + 
  stat_function(fun = dchisq, args=list(df = 1), aes(linetype = "df = 1")) +
  stat_function(fun = dchisq, args=list(df = 2), aes(linetype = "df = 2")) +
  stat_function(fun = dchisq, args=list(df = 4), aes(linetype = "df = 4")) +
  stat_function(fun = dchisq, args=list(df = 10), aes(linetype = "df = 10")) +
  scale_linetype_discrete(limits = paste("df", "=", c(1,2,4,10)))+ labs(linetype = NULL, y = "f(x)")
```

# What can we do with chi-square?

- Many different things:
  - We could approach our genetic causes of depression problem in a different way
  - Instead of looking at means from scales, we could look at how many individuals are in particular categories (e.g. no depression, mild depression, severe depression)
  - We can model these outcomes using a distribution called the **multinomial** distribution
  - Let's start with just two categories, though, because that way we can use the **binomial** distribution

# The binomial distribution

- This is the distribution of number of successes in a sequence of n independent yes/no experiments
  - This could be coin flips, or the number of patients with a diagnosis of depression in the short *5-HTTPLR* group compared to the long *5-HTTPLR* group
- Definition: $$f(X = k|n, p) = \binom{n}{k}\cdot p^k\cdot (1-p)^{n-k}$$
    - Where $k$ is the number of successes (e.g. number of patients with depression or coins that come up heads), $n$ is the total number of experiments (each participant is a "coin flip" experiment), and $p$ is the probability of the success (e.g. 0.5 for a fair coin, or 0.045 for depression in the UK).
- You almost definitely did this in school, but we won't go into the details of this distribution much. Instead, we'll just look at what happens when we increase the sample size 

# Plotting the binomial distribution

```{r echo = F}
par(mfcol=c(3, 1))
p = .5
for(n in c(5,10,60))
{
    x <- dbinom(0:(n), size=n, p=p)
    barplot(x, names.arg=0:(n), space=0, main=paste('n = ',n,", p = ",p,sep=''), xlab = "Number of successes (X)", ylab = ("p(X)"))
}
```

# Binomial and normal distribution

- For large sample sizes, the binomial distribution approximates the normal distribution
- Because of this, there is an easy way of calculating a *z*-value (or rather, its square -- you can read up on the details if you want): 
    - First, get the frequencies of successes $f_{o(1)}$ and non-successes $f_{o(2)}$.
        - For example, if you had $f_{o(1)} = 40$ times Heads and $f_{o(2)} = 60$ times Tails, can we conclude that the coin is not fair?
    - Then get the expected frequencies given the null hypothesis. If we have a fair coin, our p(Heads) should be .5, so we're expecting $f_{e(1)}=50$ times Heads and $f_{e(2)}=50$ times tails.

# The chi-square test

- If the sample size is large enough (more than 10 per category), the binomial distribution approximates the normal distribution and the squared differences between the observed ($f_{o(j)}$) and the expected ($f_{e(j)}$) are $z^2$-values (again, if you want to know why, I can point you to where you can read more).
$$z^2 = \chi_1^2 = \frac{\sum\limits_{j = 1}^{n}(f_{o(j)}-f_{e(j)})^2}{f_{e(j)}}$$

# The chi-square test (2)

- In this case, we have two groups (Heads and Tails), so $n = 2$. We can rewrite the sum as:
$$z^2 = \chi_1^2 = \frac{(f_{o(1)}-f_{e(1)})^2}{f_{e(1)}} + \frac{(f_{o(2)}-f_{e(2)})^2}{f_{e(2)}}$$
- Plug in our values ($f_{o(1)} = 40$, $f_{o(2)} = 60$, $f_{e(1)} = f_{e(2)} = 50$):
$$z^2 = \chi_1^2 = \frac{(40-50)^2}{50} + \frac{(60-50)^2}{50} = \frac{100}{50} + \frac{100}{50} = 4$$
- We can look up the probability of getting a value this extreme based on the $\chi^2$-value: `=1-CHISQ.DIST(4,1,TRUE)`, which is `r 1-pchisq(4,1)`
- Conclusion: if the null hypothesis (fair coin, p(H) = .5) is true, we would expect to find an outcome like H: 40, T:60 in less than 5% of samples.

# Degrees of freedom

- Wait, what is the `1` in `=1-CHISQ.DIST(4,1,TRUE)`?
    - That's the degrees of freedom. Remember, the degrees of freedom are the number of independent $z^2$ variables we are summing up.
    - Why only one, when we are summing two terms?
$$z^2 = \chi_1^2 = \frac{(f_{o(1)}-f_{e(1)})^2}{f_{e(1)}} + \frac{(f_{o(2)}-f_{e(2)})^2}{f_{e(2)}}$$
- In this expression, the second term is determined by the first, since $f_{o(2)} = n - f_{o(1)}$.
    - There is only one term that can vary freely, hence $df(\chi^2) = 1$.
- This is a really tricky concept! Don't worry if you don't quite understand what a degree of freedom is at this point. We'll come back to it.
    
# Generalising the chi-square test

- Why use $\chi^2$ here at all, when we could just take the square root and do a *z*-test?
- The answer is that this whole principle generalises to the **multinomial** distribution, i.e. cases where we have more than two groups.
- In the multinomial distribution, we have more than $n=2$ groups, but the general equation stays the same:

$$\chi_{n-1}^2 = \frac{\sum\limits_{j = 1}^{n}(f_{o(j)}-f_{e(j)})^2}{f_{e(j)}}$$    

- Our $\chi^2$ is distributed with $n-1$ degrees of freedom, where $n$ is the group size.


# More things to do with chi-square

- Remember, when we have a low sample size and we want to use sample standard deviation to estimate the population standard deviation, we have the problem that the sample standard deviations are not symmetrically distributed:

```{r chisq_df_1_plot_2}
make_variance_hist_and_plot(run_variance_simulation(sample_size = 2, number_of_simulations = nsim, population_mean = 0, population_sd = 1))
```

# Correcting for the skew

- We can do this by correcting the standard normal distribution using the chi-square distribution to account for the skew in the variance estimate
- The result of this correction is the *t*-distribution (this was the work that William Sealy Gossett published under the pseudonym "Student")
- The *t*-distribution can be expressed in terms of the *z* (standard normal) and the chi-square distributions
$$t_n = \frac{z}{\sqrt{\chi_n^2/n}}$$
- In other words, if we divide a *z*-value by the square root of an *independent* $\chi^2$ value divided by *n*, we get a *t*-value 
    
# The *t*-distribution

- The *t*-value has **degrees of freedom** as well -- it inherits them from the $\chi^2$ value in its denominator.
- Practically, the denominator makes the distribution have "heavier" tails (it makes extreme values more likely) -- exactly what we need for our problem of the sample variance often underestimating the variance

# Let's plot some *t*-distributions

```{r, echo = F}
library(ggplot2)
x <- seq(from = -5,by = .01,to = 5)
ggplot(data.frame(x = x), aes(x = x)) + 
  stat_function(fun = dt, args=list(df = 1), aes(linetype = "df = 1")) +
  stat_function(fun = dt, args=list(df = 2), aes(linetype = "df = 2")) +
  stat_function(fun = dt, args=list(df = 4), aes(linetype = "df = 4")) +
  stat_function(fun = dt, args=list(df = 10), aes(linetype = "df = 10")) +
  scale_linetype_discrete(limits = paste("df", "=", c(1,2,4,10)))+ labs(linetype = NULL, y = "f(x)")
```

The *t*-distribution vs. the normal distribution

- Solid = normal distribution, dashed = *t*-distribution

```{r, echo = F}
## plot multiple figures:
## replace ugly par... specification with 
## something easier to remember:
multiplot <- function(row,col){
     par(mfrow= c(row,col), pty = "s")
   }

range <- seq(-4,4,.01)  
 
multiplot(2,2)

 for(i in c(2,5,15,20)){
   plot(range,dnorm(range),type="l",lty=1,
        xlab="",ylab="",
        cex.axis=1)
   lines(range,dt(range,df=i),lty=2,lwd=1)
   mtext(paste("df=",i),cex=1.2)
 }
```

# The *t*-test

- Solution: assume that the sample means aren't normally distributed, but rather *t*-distributed
- Why *t*?
  - The *t*-distribution is like the standard normal distribution, but it has an additional parameter that we call *df* (for degrees of freedom, but don't worry about the name yet).
  - The higher df, the closer the *t*-distribution is to the standard normal distribution
  - For lower df, the *t*-distribution has "heavy tails", meaning that it's wider
    - This reflects greater uncertainty.

# t as a test statistic

$$t_{n-1} = \frac{\bar{x} - \mu_0}{\hat{\sigma}_{\bar{x}}}$$, where $\mu_0$ is the mean according to the null hypothesis and $\hat{\sigma}_{\bar{x}}$ is the estimate of the standard error of the mean (based on the sample standard deviation) is *t*-distributed with $df = n-1$ degrees of freedom (look it up if you don't believe me, but I think most of you don't want the mathematical proof...)

# Back to the depression and genes example

- Let's assume this time we have 50 participants in each group and the variance within each group is 625 (i.e., the sd is 25). The means for the groups are 100 and 105, so we still have a 5 point difference in means.

# Computing the 95% CI

- Using the *t*-distribution, we can compute CIs from samples as follows: get the lower and upper bounds from the *t*-distribution (which one depends on the sample size, e.g. in this we have $n = 100$, so we will use a *t*-distribution with $df = n-1 = 99$):

- How far from the mean is the lower bound (remember, we want to exclude the extreme low 2.5%)? `=T.INV(0.025, 99)` (where 99 is the df)
    - Result: `r qt(.025, 99)`
- How far from the mean is the upper bound (remember, we want to exclude the extreme high 2.5%)? `=T.INV(0.975, 99)` (where 99 is the df)
    - Result: `r qt(.975, 99)`
- No surprise: the *t*-distribution is symmetrical
    
# Finish computing the CI

- Then take the upper and lower bounds and compute the CIs as follows:
$\bar{x} = \mu_{\bar{x}} \pm `r qt(.975, 99)` \cdot \frac{s}{\sqrt{n}}$
- We plug in our sample mean of 5 for $\mu_{\bar{x}}$ using the sample mean 
- Then we estimate the population SD from the sample SD (25) and use that to calculate the *standard error of the mean*: $s_{\bar{x}} = \frac{s}{\sqrt{n}} =\frac{25}{\sqrt{100}} = \frac{25}{10} = 2.5$
- Now plug it in for the CI: $5 \pm `r qt(.975, 99)` \cdot{2.5} = 5 \pm `r qt(.975, 99) * 2.5`$
- Lower bound: `r 5-qt(.975, 99) * 2.5`
- Upper bound: `r 5+qt(.975, 99) * 2.5`
- As 0 is not part of the 95% CI, we can in this case reject the null hypothesis that there is no difference between the groups


# Interpreting the CI of the sample mean

- Remember, we are reversing the idea that the sample mean has a 95% probability to be within the 95% confidence interval around the population mean.
- When we calculate a 95% CI from a *sample* this **DOES NOT MEAN** that there is a 95% probability that the population mean is within this 95% CI.
- The true mean either is or is not in this particular CI.
- Rather, it means that if you take a lot of samples and compute the CI around the sample mean, 95% of those CIs will contain the true population mean.
- In other words, the CI bounds are random variables, but the population mean isn't.
- (In Bayesian statistics, you can actually get something equivalent to the first definition -- a 95% credible interval.)

# Hypothesis tests

- In a way, by calculating the CI we already have a way to test hypotheses
- Let's say we got a 95% CI from our sample with a lower bound of 2 and an upper bound of 3.
- Let's use the simplest null hypothesis possible
  - Null hypothesis: the mean of the population that the sample came from is 0
  - $H_0: \mu = 0$
  - Given the 95% CI above, can we reject the null hypothesis?
    - And if so, what is the chance that we're wrong?
  - Answer: Yes, we can, since 0 is not part of the CI.
    - There is the possibility that we are wrong, though, since only 95% of the CIs will contain the true population mean.
    - This is called the type I error, and its probability here (called $\alpha$)is 5%.

# Two-tailed *t*-tests

- Remember that we actually wanted to test a null hypothesis about a difference because we weren't sure which way the effect was going to go
- Instead of computing the CI from the *t*-value, we can also just take the *t*-value itself as a measure of how far the sample mean is away from the mean specified in the null hypothesis.
- We just consider the most extreme 5% of the distribution around the mean proposed by the null hypothesis (usually 0)
- If our observation is within the extreme 5% (i.e. the lowest 2.5% or the highest 2.5%) of the distribution, we can reject the null hypothesis
- We can determine a critical *t*-value $t_{crit}$ depending on our $\alpha$ criterion and the df. For example, for a df of 99, $t_{crit}$ for the upper bound is `=T.INV(.975,99)`, which gives us `r qt(.975, df = 99)` and $t_{crit}$ for the lower bound is `=T.INV(.025,99)`, which gives us `r qt(.025, df = 99)`
- Note that the *t*-distribution is symmetrical
In short, if $t \ge |t_{crit}|$, we can reject the null hypothesis.

 
# The two-sample *t*-test

- Remember, in our example we are comparing two samples. We'll call the sample means $\mu_1$ and $\mu_2$.
- Our null hypothesis is $H_0: \mu_1 = \mu_2$
- We can rephrase this as $H_0: \mu_1 - \mu_2 = \delta = 0$
- We already know the logic of this: we just want to find out if $d$ ($\delta$ = true population difference, $d$ = sample difference) is extreme enough so we can reject the $H_0$.
- Let's see how $\delta$ is distributed.
- We could do this analytically, using the things we've learned about expected values, but I'll leave that to those of you who are really really interested and just give you the end results.

# The two-sample *t*-test (2)

- Remember, our null hypothesis is that groups come from the same populations 
- We're looking for the distribution of sample differences $x_1 - x_2$:
- The mean of that distribution needs to be 0, since the $H_0$ says that they both are from the same distribution
- Since the sample sizes are the same (50 and 50), we can calculate the variance of the difference by just taking the average of the sample variances: $s^2 = \frac{s^2_1 +s^2_2}{2}$, where $s_1$ is the sample standard deviation of the first group and $s_2$ is the sample standard deviation of the second group.

# The two-sample *t*-test (3)

- We can then just plug this into the formula for the *t*-statistic:
$t = \frac{(\bar{x}_1 - \bar{x}_2)}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$

- Only problem: what is the *df* of that test? If the variances are equal, it's easy: $df = n_1+n_2-2$
  
- We won't cover special cases of t-tests where you have unequal sample sizes and/or variances -- you can look those up in a textbook.
- The important thing is the principle


# The dependent *t*-test for paired samples

- This is one particular, easier case, so we'll touch on it briefly
- Since we have two samples per person/group/analysis unit, we can simply compute the differences between measurements and then use the *t*-test to check if they are 0.

# The dependent *t*-test for paired samples (2)

- First, we calculate the mean and the standard deviation of our sample of $n$ difference values $d_i = x_{i1}-x_{i2}$:
$$ 
\begin{aligned}
\hat{\mu}_d = \bar{d} &= \frac{\sum\limits_{i=1}^{n}d_i}{n}\\
\hat{\sigma}_d = s_d &= \sqrt{\frac{\sum\limits_{i=1}^{n}(d_i - \bar{d})^2}{n-1}} \\
\hat{\sigma}_{\bar{d}} &= \frac{\hat{\sigma}_d}{\sqrt{n}} = \frac{\hat{s}_d} {\sqrt{n}}\\
                      &= \frac{\sqrt{\frac{\sum\limits_{i=1}^{n}(d_i - \bar{d})^2}{n-1}}}{\sqrt{n}}
\end{aligned}
$$
- Here, $n$ is the number of sample *pairs*


# The dependent *t*-test for paired samples (2)

- Then we can calculate the *t*-value:
$$t = \frac{\bar{d} - \mu_d}{\hat{\sigma}_d}$$, where $\mu_d$ is the population mean for the difference given that the $H_0$ is true. If the $H_0$ is that both samples are the same ($\mu_d = 0$), this simplifies to
$$t = \frac{\bar{d}}{\hat{\sigma}_{\bar{d}}}$$
-We can estimate the standard error of the difference mean $\hat{\sigma}_{\bar{d}}$ from the standard deviation of the difference: $$\hat{\sigma}_{\bar{d}} = \frac{s_d}{\sqrt{n}}$$

# The dependent *t*-test for paired samples (3)

- Plugging this into the equation for *t*, we get: 

$$t_{n-1} =  \frac{\bar{d}}{\frac{s_d}{\sqrt{n}}}$$

- The resulting *t*-value will have a df of $n-1$, where $n$ is the number of sample pairs.
- Since the sd of the differences will be a lot lower than the overall sd, the power of this test is quite a bit higher.

# What about one-tailed *t*-tests?

- If we are absolutely sure of the direction of the effect, then we could use a *t*-test that only rejects the null hypothesis when the *t*-value is greater than $t_{crit}$ or if it is smaller than $t_{crit}$ (depending on what direction we want to test for).
- In this case, our $t_{crit}$ can be a little closer to 0, since the entire 5% rejection area is in one tail only: `=T.INV(.95,99)`, which gives us `r qt(.95, df = 99)`.
- But be careful, if the effect is in the wrong direction (even if it's ridiculously strong in the wrong direction), we can't reject the null hypothesis with that test.
- This is one of the weird cases in null hypothesis significance testing (NHST) where our intentions can determine the results of the test. Bayesian statisticians are right to complain about this.

# Power simulations

- Power is the probability of rejecting the null hypothesis given that it is actually false.
- For this, you have to simulate what the world would be like if the true effect is different from 0
- So you have to assume an effect size that you are interested in
- For simple (and even more complex) designs, you can compute power analytically. I will show you how to do this using a program called GPower.
- But simulations are a lot more intuitive!
- Let's go back to the depression example and assume that the true difference is 5 (on average, people score 5 more on the BDI if they have the long gene variant), the sample size is 50 and 50, and the average sd of both groups was 25.

# Power simulations plot

- Remember, $t_{crit} = `r qt(.975, 99)`$

```{r, echo = FALSE}
t_test_sim <- function(n, mean = 1, sd = 1){
  t_results <- t.test(rnorm(n, mean, sd))
  t_results$statistic}
simulation_results <- replicate(1000, t_test_sim(50,5, 25))
hist(simulation_results, main = paste0("Proportion of |t| > ", round(qt(.975, 49),2), ": ", sum(abs(simulation_results) > qt(.975, 49),2)/length(simulation_results)), 50)
```

- Not so great! We can only detect the difference of 5 in 27.2% of all experiments set up in this way

# How to increase power

- Let's try a higher true difference ($\mu = 10$):

```{r, echo =F}
simulation_results <- replicate(1000, t_test_sim(50, 10, 25))
hist(simulation_results, main = paste0("Proportion of |t| > ", round(qt(.975, 49),2), ": ", sum(abs(simulation_results) > qt(.975, 49),2)/length(simulation_results)), 50)
```

# How to increase power (2)

- The standard deviation (i.e. the noise) in the population is lower (people don't vary as much in terms of their depression scores)
- Let's try a true value of $\mu = 5$ and $\sigma = 15$

```{r, echo = F, cache = FALSE}
simulation_results <- replicate(1000, t_test_sim(50, 5, 15))
hist(simulation_results, main = paste0("Proportion of |t| > ", round(qt(.975, 49),2), ": ", sum(abs(simulation_results) > qt(.975, 49),2)/length(simulation_results)), 50)

```

# How to increase power (realistically!)

- You don't really have any direct control over population mean (i.e. effect size) or sd (i.e. noise). Let's focus on the one variable that you do have control over.
- The sample size is larger: let's try $n=800$ instead of 100

```{r, echo = F, cache = FALSE}
simulation_results <- replicate(1000, t_test_sim(400, 5, 25))
hist(simulation_results, main = paste0("Proportion of |t| > ", round(qt(.975, 399),2), ": ", sum(abs(simulation_results) > qt(.975, 399),2)/length(simulation_results)), 400)
```

# Double-checking our results

- Let's just check analytically that we have this correctly: If we want to show in the one-sample *t*-test that a mean of 5 is different from 0 (when $sd = 25$), we need about 800 subjects. I will show you how to use GPower for that.

# Setting yourself up for success (or failure)

- You don't want to run an underpowered study. Most likely, you'll get a null result that tells you nothing about the true state of the world.
- How can you avoid this? 
  - Run a realistic number of participants so you reach acceptable power (the APA recomments .8).

# Converting differences into effect sizes
- Effect size is defined as
$$d = \frac{\mu_1 - \mu_2}{\hat{\sigma}}$$
- We're doing a *t*-test where we want to know how many participants we need to detect a group difference of 5 when the standard deviation is 25
- That means we are looking at an effect size of $d = \frac{5}{25} = 0.2$
- We want to find the necessary effect size given the sample size and the standard error of the mean. The test would be two-tailed, since we don't know the direction of the effect. The power we want is $(1-\beta) = .8$
- Both groups will have the same size

# In GPower

- In GPower, select `t tests` as `Test family` and `Means: Difference between two independent means` as `Statistical test`. As `Type of power analysis`, select `A priori: Compute required sample size`
- The `Allocation Ratio N1/N2` is 1, since both groups are the same size.
- In the `Input Parameters` area, select `Two` for `Tails`, leave the $\alpha$ at `.05`, set the `Power` to `0.8`, and set the `Effect size d` to `0.2`. You will get a sample size per group of $d = `r power.t.test(delta = 5, sd = 25, power = .8, type = "two.sample", alternative = "two.sided")$delta`$

- You would need almost 800 people to have a power of .8!
- Detecting weak effects takes a lot of effort!

# Don't cheat!

- How about the following strategy? 

> Just run the hypothesis test on the data after every new sample and stop as soon as you get a significant result.

- Let's see just what happens to $\alpha$ if you do that.
- Run a simulation where there is no effect (i.e. where we know the $H_0$ is true)

```{r, echo = FALSE}  
t_test_cheating_sim <- function(n_max = 30, n_increments = 2, sd = 1){
  samples <- NULL
  significant <- FALSE
  
  while(length(samples) <= n_max & significant == FALSE){
      samples <- c(samples, rnorm(n_increments, mean = 0, sd = sd))
      significant <- t.test(samples)$p.value <= .05
    }
  return(significant)
  }
```

# The consequences of cheating

Let's run this simulation 1000 times and make a plot with the results:

```{r, echo = F}
simulation_results <- replicate(1000, t_test_cheating_sim(n_max = 30, n_increments = 2, sd = 1))
barplot(table(simulation_results), names.arg = c("no","yes"),xlab= "Significant", ylab = "Number of simulations", main = paste0("Proportion of significant results: ", sum(simulation_results)/length(simulation_results)), 100)
```

# The consequences of cheating (2)

- Here, you get a significant result 25% of the time, even though we are actually sampling from the *SAME* distribution.
    - In other words, $\alpha$ is at 25%, instead of 5% where it should be.
- Unfortunately, this strategy of using stopping rules ("data peeking") is quite common.
    - Solution: do a power analysis, set your sample size beforehand, and stick to it!

# Testing more interesting hypotheses

- So far, we have been testing the null hypothesis that our sample mean is 0.
- This is not what we usually do in Psychology.
- Instead, we want to know if there is a significant difference between the means of two (or more) samples.
  - For example, you might give only one group an intervention against anxiety, with the other one serving as the control.
    - Does the intervention work? 
      - Do people in the treatment group report lower anxiety? 
      - Can we generalise this to the population?
      - Should we use this intervention in clinical practice?
  - A lot of effort and money may be wasted if you get these questions wrong.

# The real-life *5-HTTLPR*
- The sad truth about the *5-HTTLPR* research is that, in almost 15 years, not many of the initial exciting findings have been replicated.
- An incredible amount of money was spent

# Funding spent on *5-HTTPLR*
- Dorothy Bishop (a professor at Oxford University), calculated how much money was wasted on this topic:
<blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">Interesting. I did a little analysis on US $ spent on research on this topic for a talk I did last year, using data from NIH reporter. just a quick and dirty analysis, but wanted to show how jumping on bad stuff is an ethical requirement to prevent waste of scarce funds <a href="https://t.co/OPOEKYQxwQ">pic.twitter.com/OPOEKYQxwQ</a></p>&mdash; Dorothy Bishop (@deevybee) <a href="https://twitter.com/deevybee/status/1490374839785373698?ref_src=twsrc%5Etfw">February 6, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>