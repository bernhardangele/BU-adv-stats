Advanced Statistics
========================================================
author: Bernhard Angele
date: Lecture 5

Why do we get conflicting results?
=================================================================
- This is weird: We didn't have an effect for CatAge when we looked at the *t*-values, but we have when doing F-tests?

```{r, results='asis', echo = FALSE}
kable(anova(lm_catfood_interaction))
```

- The answer lies in **what each test compares**. This may sound nitpicky, but it's really important!

Types of sums of squares for the F-test
=================================================================
- Type I: Compare a model containing the predictor **and all other predictors** entered *so far* with a model only containing the predictors entered **so far** (order matters).
    
- Type II: Compare a model containing the predictor *along with* all the other predictors to a model containing *all the other predictors* **except** the predictor in question **and its interactions**.
    
- Type III: Compare a model containing the predictor *along with* all the other predictors to a model containing *all the other predictors* **except** the predictor in question, but **including its interactions**.
    - This is the equivalent of the *t*-tests.
    - This is the standard in SPSS.

Example: Type I sum of squares
=================================================================
```{r, results='asis', echo = FALSE}
kable(anova(lm_catfood_interaction))
```

Example: Type II sum of squares
=================================================================
```{r, results='asis', echo = FALSE}
library(car)
kable(Anova(lm_catfood_interaction,type = "II"))
```

Example: Type III sum of squares
=================================================================
```{r, results='asis', echo = FALSE}
library(car)
kable(Anova(lm_catfood_interaction,type = "III"))
```

Summary: Sums of square types
==================================================================
- ANOVA results (both classic ANOVA and regression model tests) can vary depending on which SS you use
    - Make sure that you know which ones you are using
    - If you are using SPSS, the answer is *probably* III.
- In standard ANOVA (with only discrete predictors), all SS types give the same result *as long as your design is balanced*
    - An unbalanced design will lead to differing sums of squares.
- In multiple regression, all SS types give the same result *as long as your predictor variables are not correlated*


Diagnosing influential cases
======================================================================
- Occasionally, you get **outliers** in your data. These are cases that differ quite a bit from the rest of the data. Often (but not always) outliers are caused by an error in measurement or coding. Outliers definitely merit closer attention.
- In SPSS, you can get a number of diagnostic values for each observation. You can save these to the Data Editor using the `Save...` button in the Linear Regression module.


Diagnosing influential cases (2)
======================================================================
- Diagnostic values you can save:
      - DFBetas (one for each predictor, including the intercept) 
          - Difference between the parameter when this case is excluded and when it is included
          - Should not be much larger than that of the other cases
          - Absolute value should not be > 1
      - DFFit
          - Difference between the predicted value for a case when this case is excluded and when it is included
          - Should not be much larger than that of the other cases
      - Covariance ratio
          - The farther it is from 1 and the smaller it is the more this case increases the error variance

Diagnosing influential cases (4)
========================================================================
- More diagnostic values you can save to the Data Editor:
  - Cook's distance
      - Measure of the overall influence of a case on the model
        - Values > 1 are possibly problematic
  - Hat = leverage
      - How strongly does this case influence the prediction?
      - Average value is $p/n$, where $p$ is the number of predictors (including the intercept, so 4 for our model) and $n$ is the number of observations(`r nrow(catfood)` for our model, so our average should be `r 4/nrow(catfood)`)
      - Values over 2 or 3 times the average should be cause for concern

General strategy for diagnosing influential cases
==========================================================================
- Check if any cases have a Cook's distance of > 1
  - Then check if they have an undue effect on the model using the other statistics
  - SPSS can also give you standardised residuals (i.e. residuals transformed into *z*-values) above a certain cutoff (e.g. 3 standard deviations).
    - You can get these from `Statistics...` under `Casewise diagnostics` in the Linear Regression module
  - Outliers outside 3 standard deviations are potentially problematic
  
Testing the regression assumptions
===========================================================================
- Normality of the residuals:
  - Make a histogram of the standardised residuals (`Plot...` button in the `Linear Regression` module)
    - Does it look roughly normal?
    
```{r, echo = FALSE}
hist(scale(resid(lm_catfood_interaction)), xlab = "Regression Standardized Residual", ylab = "Frequency", main = "Dependent Variable: Food Eaten")
```

More assumption tests (3)
===========================================================================
- Heteroskedasticity
  - Make a plot of *z*-transformed predicted values against *z*-transformed residuals
      - In the `Plot...` window, choose ZPRED as the y-value and ZRESID as the x-value
      - Does it look like there is more variance for certain predicted values?
      
```{r, echo = FALSE}
plot(x = scale(predict(lm_catfood_interaction)), y = scale(resid(lm_catfood_interaction)), xlab = "Regression Standardized Residual", ylab = "Regression Standardized Predicted Variable", main = "Dependent Variable: Food Eaten")
```

Reporting the regression results
===================================================================
```{r, results='asis', echo = FALSE}
lm_catfood_interaction_table <- coef(summary(lm_catfood_interaction))
kable(lm_catfood_interaction_table)
```

1. Make a table with the model coefficients (see above.)
2. Introductory paragraph: In order to test the hypothesis that cat weight and cat age can predict how much food a cat eats, we performed a multiple regression analysis  with food eaten (in g) as the dependent variable and cat weight and cat age (both mean-centered) as well aas their interactions as continuous independent variables. The model explained a very high amount of the variance in the dependent variable, with an adjusted $R^2$ of `r summary(lm_catfood_interaction)$adj.r.squared`.


Reporting the regression results: Coefficients
===================================================================
Our results show that both cat weight (b = `r lm_catfood_interaction_table["CatWeight_centered","Estimate"]`, SE = `r lm_catfood_interaction_table["CatWeight_centered","Std. Error"]`, t = `r lm_catfood_interaction_table["CatWeight_centered","t value"]`, `r print_p(lm_catfood_interaction_table["CatWeight_centered","Pr(>|t|)"])`) and cat age (b = `r lm_catfood_interaction_table["CatAge_centered","Estimate"]`, SE = `r lm_catfood_interaction_table["CatAge_centered","Std. Error"]`, t = `r lm_catfood_interaction_table["CatAge_centered","t value"]`, `r print_p(lm_catfood_interaction_table["CatAge_centered","Pr(>|t|)"])`)) had a significant effect on the food eaten. On average, an increase in weight by one kg went along with an increase in food eaten of `r lm_catfood_interaction_table["CatWeight_centered","Estimate"]` g. Similarly, an increase in age by one month went along with an increase in food eaten of `r lm_catfood_interaction_table["CatAge_centered","Estimate"]` g.

There was no significant interaction between cat weight and cat age (p > .05), suggesting that the effects of cat weight and cat age were additive. Assumption tests and visual inspection of residual plots showed that there was no evidence of violations of normality, independence, or homoscedasticity. [If you removed influential cases, say this here.]



Comparing multiple groups
========================================================
- *t*-tests are nice if you only have two groups that you want to compare.
- But maybe you have more groups
- Example:
> A researcher wants to find out if there is a systematic difference in intelligence between MSc students from different universities. She performs intelligence tests on 10 students each from BU, University of Southampton and Oxford University and records the results.

Making fake data for our example
========================================================
- Let's assume that the true state of affairs is that there is no difference in intelligence
  - $H_0: \mu_1 = \mu_2 = \mu_3$
  - The alternative hypothesis is that the population means differ (but we don't specify a direction, hence we'll do a two-tailed test)
- In that case, all intelligence scores would come from the same distribution: a normal distribution with mean = 100 and sd = 15
- Let's generate 3 data sets according to this criterion
```{r, echo=FALSE}
# The following line sets the random number generator to a specific state
# and ensures that you get the same numbers that I did.
set.seed("16102014")
bu <- rnorm(n = 10, mean = 100, sd = 15)
soton <- rnorm(n = 10, mean = 100, sd = 15)
oxford <- rnorm(n = 10, mean = 100, sd = 15)
```

Simulating data with Excel
========================================================
- `=RAND()` will give you a random number between 0 and 1 (continuous uniform distribution, if you must know)
- `=NORM.INV(probability, mean, sd)` will take a probability (= left tail proportion) and give you the corresponding x-value from the distribution
- combine the two, and you get random samples from a normal distribution: `=NORM.INV(RAND(), mean, sd)`

Let's make some random samples for our simulation
========================================================
- Make a spreadsheet, add three column headers (BU, Soton, Oxford)
    - Below the group headers, generate 10 samples each using the formula `=NORM.INV(RAND(), 100, 15)`
- Note that the null hypothesis is true here: all values come from the same normal distribution
- Let's do *t*-tests first
  - How many would we need?
    - 3: BU vs. Soton, BU vs. Oxford, Soton vs. Oxford
- Let's pretend that we (just like in real life) don't know the true properties of the population (so we use *t*-tests)
  
t-tests in Excel
========================================================
- Simple, just follow the formulas
- First, calculate the mean and standard deviation for each group (using `=AVERAGE` and `=STDEV.S`; the .S stands for the sample variance, essentially meaning that we're using the corrected standard deviation, dividing by $n-1$ instead of $n$)
- Then calculate the t-value based on the formula I gave you in the last lecture:
$$t = \frac{(\bar{x}_1 - \bar{x}_2)}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$$
- In this case, the Excel version of the formula above will be `=(MEAN1-MEAN2)/SQRT((SD1^2/10)+(SD2^2/10))`
      - Replace `MEAN1`, `MEAN2`, `SD1`, and `SD2` with the actual cells that these values are in (e.g. `A12`)

t-tests in Excel (2)
========================================================
- Now use `=T.DIST.2T(ABS(TVALUE), DF)` to get the p-value for a two-tailed test
  - Replace `TVALUE` and `DF` with the cells containing those values
    - What should the DF be? Let's first assume that the population variances are equal (as they should be if these really come from the same population). In that case, the DF should be `n_1+n_2-2`. So, in our example with two groups of 10, it should be 18.
  - Why `T.DIST.2T`? This will give us the two-tailed p-value (we could also use 1-T.DIST and multiply the result by 2)
  - Why `ABS(TVALUE)`? If $\bar{x_1} < \bar{x_2}$, the *t*-value will be negative. Since the t-distribution is symmetrical, it's easiest to just use the absolute t-value ($|t|$, i.e. the t-value without the sign) and get the area under the curve in the right tail

A quick plot to visualise this
=========================================================
- The 2.5% lower and upper tails are highlighted. They have exactly the same area, so we could also just use double the area of the upper tail. This is exaclty what `T.DIST.2T` does.
```{r, echo = F}
cord.x <- seq(-3,3,0.01)
cord.y <- dt(cord.x, df = 18)
cord.y[cord.x > qt(.025, df =18) & cord.x < qt(.975, df =18)] <- dt(-3, df = 18)
curve(dt(x, df =18),xlim=c(-3,3),main='t(df=18)')
polygon(cord.x,cord.y,col='skyblue')
```

t-tests in Excel (3)
========================================================
- Believe it or not, Excel also has its own *t*-test function: `=T.TEST(ARRAY1, ARRAY2, TAILS, TYPE)`
    - `ARRAY1` is the data from the first group (e.g. A2:A11, if your data are in those cells)
    - `ARRAY2` is the data from the second group (e.g. A2:A11, if your data are in those cells)
    - `TAILS`: `1` for a one-tailed test, `2` for a two-tailed test
    - `TYPE`: `1` for a paired *t*-test, `2` for a two-group *t*-test with equal variances, `3` for a two-group *t*-test with unequal variances
- The test we just did by hand is a two-tailed test with equal variances
- Confirm that you get the same result using `T.TEST` as you did by hand. If you don't, you did something wrong.

t-tests in Excel (4)
========================================================
- If you set `TYPE` to `3`, you get Welch's *t*-test. This corrects the degrees of freedom to account for unequal variances.
- Do a Welch's *t*-test on the same data and compare the results with the *t*-test assuming equal variances. What changes?

BU vs Soton
=========================================================
- In case you don't have Excel handy, here I'm doing the same tests in R:
```{r}
t.test(bu, soton)
```

BU vs Oxford
=========================================================
- In case you don't have Excel handy, here I'm doing the same tests in R:
```{r}
t.test(bu, oxford)
```

Soton vs Oxford
=========================================================
- In case you don't have Excel handy, here I'm doing the same tests in R:
```{r}
t.test(soton, oxford)
```

Anything wrong with that?
========================================================
- We are doing three independent *t*-tests
- Each *t*-test has a 5% chance of producing a spurious result ($\alpha$)
- What is the probability that we get at least one spuriously significant result?
  - It's 1 - the probability that we get no spurious results
  - $1 - .95\cdot.95\cdot.95 = .14$
  - We have a problem: our $\alpha$ is almost three times as high as it should be.
  - SPSS calls this "correction" (which really isn't one) LSD (least significant differences) - don't use it!
  
Just a quick reminder about power
========================================================
- Possible outcomes of a hypothesis test (given the true state of the world)

                                             |Null hypothesis is actually **TRUE**    |Null hypothesis is actually **FALSE**
---------------------------------------------|----------------------------------------|-----------------------------------------------------
Decision from sample is "reject $H_0$"       | **Type I error**. Probability: $\alpha$|Correct rejection. Probability: $1~-~\beta=$**power** 
Decision from sample is "do not reject $H_0$"| Correct failure to reject: Probability: $1 - \alpha$    |**Type II error**. Probability: $\beta$ 

- Of course, we want to minimise $\alpha$, the probability of a **Type I error**. But we also want to minimise $\beta$, the probability of a **Type II error* (i.e. we want to maximise power). 
- Lowering $\alpha$ (i.e. making the test more conservative) increases $\beta$. By how much? Dependent on the effect size, the population standard deviation, and the sample size.
- $\alpha = .05$ is a compromise! Thanks, R.A. Fisher!

Exercise: Give it a try
=========================================================
- Instead of running the simulation just my computer, I'll run it on **you**
- What I mean by that:
  - Generate three data sets in Excel like I've just shown you
  - Make sure that all three data sets are samples from the same normal distribution: Same mean and sd, of course same n as well
  - Run three two-sample *t*-tests (assuming equal variances) comparing the means
  - Re-generate the random variables 20 times (you can do that by simply double-clicking on any empty cell and changing it)
  - Afterwards, I will ask you and count how many significant *t*-tests you observed (p < .05).
  - If the $\alpha$ level isn't inflated, you should not observe much more than 1 (5% is 1 in 20).

Solutions
=========================================================
- We can adjust the $\alpha$ level of each *t*-test:
  - If we divide the alpha level by the number of tests, we get $.05/3 = .0167$
  - Our total $\alpha$ is then: $1-(1-.05/3)^3 = .049$
  - This is called a **Bonferroni correction**
  - Problem solved?
  - Yes, but this is essentially lowering the $\alpha$ level, leading to lower power.
- Better ways (but still lowering power):
  - Holm-Bonferroni (same principle as Bonferroni, but better power)
  - Tukey's HSD (honestly significant differences)
- Maybe we just want to know if there is a difference at all between any of these three means
  - One-way ANOVA

Analysis of Variance
========================================================
- The idea behind the analysis of variance is simple: We want to split the total variance in our data into variance explained by our grouping factor and random noise (error) variance.
- If the grouping factor explains more of the variance than we would expect based on random noise, then we can conclude that the grouping factor *significantly* improves our model (because yes, an ANOVA is a very simple statistical model)
- In other words, we can conclude that at least two of the factor levels are significantly different

Analysis of Variance (2)
========================================================
- How do we compare variances? We divide them!
- First, a bit of terminology. As you should remember, we can estimate the population variance from the sample variance:
$$ \hat{\sigma}^2 = s^2 = \frac{\sum\limits_{i = 1}^{n}(x_i - \bar{x})^2}{n-1}$$
- Let's change the names a little to better fit the ANOVA model: since the data will come in different groups (factor levels), let's use the index $i$ to denote the factor level and the index $m$ to denote the $m^{th}$ observation (e.g. the $m^{th}$ person) within each factor level
- Let's also call the number of factor levels $p$ and the number of observations per level $n$ (this is different from the way we have used $n$ so far).

Data matrix
========================================================
- Columns are factor levels, rows are observations within a level
- $i$ = factor level, $p$ = number of factor levels
- $m$ = observation, $n$ = number of observations per factor level

$$
\begin{matrix}
x_{11} &  x_{12}  & \ldots & x_{1i} & \ldots & x_{1p}\\
x_{21} &  x_{22}  & \ldots & x_{2i} & \ldots & x_{2p}\\
\vdots & \vdots   &       & \vdots  &        &\vdots \\
x_{m1} &  x_{m2}  & \ldots & x_{mi} & \ldots & x_{mp}\\
\vdots & \vdots   &       & \vdots  &        &\vdots \\
x_{n1} &  x_{n2}  & \ldots & x_{ni} & \ldots & x_{np}\\
\end{matrix}
$$

Analysis of Variance (2)
========================================================
- With this new terminology, our formula looks like this (remember the double sum operator!):
$$ \hat{\sigma}^2 = s^2 = \frac{\sum\limits_{m=1}^{n}\sum\limits_{i = 1}^{p}(x_{mi} - \bar{x})^2}{n\cdot p-1}$$
- In ANOVA terminology, we call the **numerator** of this equation the **sum of squares of the total variance** or ${SS}_{total}$ for short.
- So, ${SS}_{total} = \sum\limits_{m=1}^{n}\sum\limits_{i = 1}^{p}(x_{mi} - \bar{x})^2$
- We call the **denominator** of this equation the **degrees of freedom of the total variance** or ${df}_{total}$ for short.
- So, ${df}_{total} = n\cdot p-1$

Partitioning the variance
=======================================================
- Now we can start the interesting part of actually dividing up the variance into variance explained by the factor and unexplained error variance:
- First, we compute the estimate of the variance explained by our factor. We call the corresponding sum of squares ${SS}_{model}$ (remember, an ANOVA is a simple statistical model. We'll deal with more complicated models later).

The model variance
========================================================
- Some textbooks also call this the **treatment** variance.
- In ANOVA terminology, we call the means of the observations in each level of our factor $\bar{A}_i$. (Why $A$? Well, in more complex designs, there might be a Factor $B$). $${SS}_{model} = \sum\limits_{i = 1}^{p}n\cdot(\bar{A}_i - \bar{x})^2 = n\cdot\sum\limits_{i = 1}^{p}(\bar{A}_i - \bar{x})^2$$
- This gives us the variance that we would get if each observation were exactly equal to the group (factor level) mean
    - i.e. if all the variance were **between** groups, and none **within**

The error variance
========================================================
- Now get an estimate of the variance that is *not* explained by `group`, i.e. the error.
- ${SS}_{error}$ is the sum of the squares of the differences between each observation $x_{mi}$ and its group mean $\bar{A}_m$:
$${SS}_{error} = \sum\limits_{i=1}^{p}\sum\limits_{m = 1}^{n}(x_{mi} - \bar{A}_i)^2$$
- As you may have guessed (or not) from the sums: ${SS}_{total} = {SS}_{model} + {SS}_{error}$
- So, we can take a shortcut for calculating ${SS}_{error}$. Just subtract ${SS}_{model}$ from ${SS}_{total}$.
- ${SS}_{error} = {SS}_{total} - {SS}_{model}$

Degrees of freedom
========================================================
- With ${SS}_{model}$ and ${SS}_{error}$, we can compute the ratio of explained variance to error (or unexplained) variance.
- First, we need to take into account the number of measurements which went into each SS (i.e. actually compute the variance)
- Again, the denominator of each variance term is called the **degrees of freedom** of that variance
- ${df}_{total} = n \cdot p-1$
- ${df}_{model} = p-1$, where $p$ is the number of groups.
- ${df}_{error} = {df}_{total} - {df}_{model} = n \cdot p - p$

Mean squares and F-value
========================================================
- Now we compute our sample variances, called mean squares (MS) in the ANOVA terminology.
  - ${MS}_{model} = \frac{{SS}_{model}}{{df}_{model}}$
  - ${MS}_{error} = \frac{{SS}_{error}}{{df}_{error}}$
- Finally, we take the ratio of the two.
  - $F_({df}_{model}, {df}_{error}) = \frac{{MS}_{model}}{{MS}_{error}}$

What is an F-value?
========================================================
- If you have followed our discussion of how **variance estimates** of random variables that come from a normal distribution are always $\chi^2$ distributed, you may not be too surprised by this.
- An *F*-value is the *quotient* of two random variables following the $\chi^2$ distribution, each divided by their degrees of freedom:
$$ F_{(n_1, n_2)} = \frac{\chi_{n_1}^2/n_1}{\chi_{n_2}^2/n_2} = \frac{\chi_{n_1}^2}{\chi_{n_2}^2}\cdot\frac{n_2}{n_1}$$
  - *F*-values inherit both of the $\chi^2$'s degrees of freedom, so that they have both a numerator and a denominator degree of freedom.

(You remember that a quotient -- or a ratio--is the result of a division, right?)

The F distribution
========================================================
- It turns out that the ratio between model and error variance follows a specific distribution
  - If there is no actual effect (i.e. the groups are just assigned at random) and
  - As long as certain assumptions are valid (more on that later)
- This distribution is called the F-distribution
- Occasionally you will get a high ${MS}_{model}$ simply by chance, but such occurrences are quite rare
- The F-distribution is the probability density function for different values of the variance ratio, i.e. the F-value.
- We essentially want to test if the F-value we get is extreme enough that it could only have occurred by chance 5% of the time (our $\alpha$ level)

The F-distribution
=========================================================
- Like the *t*-distribution, the shape of the F-distribution varies depending on sample size (degrees of freedom).
- Remember that F is the *ratio* of two variances (both $chi^2$-distributed).
  - Because of this, the F distribtion has *two* degrees of freedom parameters
    - ${df}_1$, also called ${df}_{numerator}$
    - ${df}_2$, also called ${df}_{denominator}$
  
Plotting the F distribution (1)
==========================================================
- Let's take a look:

```{r, echo = FALSE}
curve(df(x, df1 = 1, df2 = 30), from = -3, to = 3)
```
- F can't be negative
  - this makes sense: it's the quotient of two $\chi^2$. You may remember that a square of a number can never be negative!

Plotting the F-distribution (2)
==========================================================
```{r, echo = F}
par(mfrow = c(4,4))
for(df1 in c(1,2,4,40)){
  for(df2 in c(1,2,4,40)){
    curve(df(x, df1 = df1, df2 = df2), from = 0, to = 5, ylab = "f(x)", main = paste0("df1 = ", df1, ", df2 = ", df2))    
  }
}
```


The F distribution and the t distribution
=========================================================
- For ${df}_1 = 1$, the *F* distribution is the same as the distribution of the square of the *t* distribution with the same ${df}_2$
- Remember, $t_{df} = \frac{z}{\sqrt{\chi_{df}^2}/df}$
    - So, $t^2_{df} = \frac{z^2}{\chi_{df}^2/df}$. A $z^2$ value is $\chi^2$ distributed with $df = 1$, so that $t^2_{df} = F_{(1, df)}=\frac{\chi_1^2}{\chi_{df}^2/df}$
- You can literally take the square root of the *F*-value of a two-level one-way ANOVA and get the corresponding *t*-value.
- Of course, if you have more than two factor levels, then this doesn't work anymore because ${df}_1 > 0$ (but you can't analyse a design like that with a *t*-test anyway.)

Let's do an ANOVA by hand
==========================================================
- Use the same spreadsheet with fake values that we used for the *t*-tests
- For this, let's assume that you have 3 groups ($p=3$) with 10 participants each ($n=10$) and that your data are in `A2:A11` for Group 1, `B2:B11` for Group 2, and `C2:C11` for Group 3. `A12`, `B12`, and `C12` contain your group means (`=AVERAGE(A2:A11)` etc.) and `A13`, `B13`, and `C13` contain your group standard deviations (`=STDEV.S(A2:A11)` etc.)

Calculate sums of squares in Excel
==========================================================
- ${SS}_{model} = n\cdot\sum\limits_{i = 1}^{p}(\bar{A}_i - \bar{x})^2$
    - In Excel: `=10*((A12-AVERAGE(A2:C11))^2+(B12-AVERAGE(A2:C11))^2+(C12-AVERAGE(A2:C11))^2)`
- ${SS}_{error} = \sum\limits_{i=1}^{p}\sum\limits_{m = 1}^{n}(x_{mi} - \bar{A}_i)^2$. 
    - Excel has a useful function called `DEVSQ` which gives you the sum of squares of deviations of data points from their sample mean
    - So: `=DEVSQ(A2:A11)+DEVSQ(B2:B11)+DEVSQ(C2:C11)`
- ${SS}_{total} = \sum\limits_{m=1}^{n}\sum\limits_{i = 1}^{p}(x_{mi} - \bar{x})^2$
  - Thanks to DEVSQ, this is really easy: `=DEVSQ(A2:C11)`

Calculate degrees of freedom and mean squares
============================================================
- ${df}_{model} = p - 1 = 3 - 1= 2$
- ${df}_{error} = n\cdot p - p = 10\cdot3-3 = 27$
- ${df}_{total} = n\cdot p-1 = 10\cdot3-1 = 29$
- Let's assume that your sums of squares are in `B20`, `B21`, `B22` for model, error, and total, respectively. Then your means squares are
    - ${MS}_{model} = \frac{{SS}_{model}}{{df}_{model}}$; `=B20/2` in Excel.
    - ${MS}_{error} = \frac{{SS}_{error}}{{df}_{error}}$; `=B21/27` in Excel.
    - ${MS}_{total}$: We don't actually need that!
    
Compute your F-value and the corresponding p-value
=============================================================
- Our *F*-value: $F_({df}_{model}, {df}_{error}) = \frac{{MS}_{model}}{{MS}_{error}}$
- Assuming that you've stored ${MS}_{model}$ in `D17` and ${MS}_{error}$ in `D18`: `=C=D17/D18`
  - Easy!
- Assuming that you put the *F*-value in `E17`, we can get the *p*-value using `=F.DIST.RT(E17,2,27)`, where 2 is ${df}_{model}$ and 27 is ${df}_{error}$.

Example data set
==============================================================
- If you don't have Excel handy to generate your own or if you want to compare your calculations to mine.

         BU|    Soton |    Oxford|
|---------:|---------:|---------:|
| 101.37080| 112.28569| 105.39426|
|  98.90038|  92.33469| 102.64322|
|  99.35124| 110.97803|  85.54996|
| 110.41595| 121.00897|  85.18428|
| 113.22132| 114.35717| 121.77122|
|  95.67537| 126.07178|  95.09605|
| 113.69695| 110.15121|  86.46963|
| 114.53939|  96.40311|  77.37942|
|  98.88912| 126.64088| 111.85448|
|  55.56055| 114.37858|  94.99734|

```{r echo = F}
my_data <- c(101.37080, 98.90038, 99.35124, 110.41595, 113.22132, 95.67537, 113.69695, 114.53939, 98.88912, 55.56055, 112.28569, 92.33469, 110.97803, 121.00897, 114.35717, 126.07178, 110.15121, 96.40311, 126.64088, 114.37858, 105.39426, 102.64322, 85.54996, 85.18428, 121.77122, 95.09605, 86.46963, 77.37942, 111.85448, 94.99734)
```

ANOVA table
==========================================================
```{r echo = F, results="as.is"}
my_df <- data.frame(Group = factor(rep(c("BU", "Soton", "Oxford"))), IQ = my_data)
my_aov <- aov(data = my_df, formula = IQ ~ Group)
my_aov_sum <- summary(my_aov)[[1]]
my_aov_sum <- rbind(my_aov_sum, c(sum(my_aov_sum$Df), sum(my_aov_sum$`Sum Sq`), NA, NA, NA))
row.names(my_aov_sum)[2:3] <- c("Error", "Total")
kable(my_aov_sum)
```

Effect size
===========================================================
- Just like for *t*-tests, we can get an estimate of effect sizes (called $\eta^2$, "eta-squared")
- $\eta^2 = \frac{{SS}_{model}}{{SS}_{total}}$
- $\eta^2$ is an estimate of the relationship between variance explained by the ANOVA model and total variance in the data
- You can use $\eta^2$ for power estimates with GPower.
- Compare this to Cohen's $d$, another estimate of effect size that we used for *t*-tests:
  - $d$ = $\frac{\bar{x_1} - \bar{x_2}}{s}$
  - Cohen's $d$ is an estimate of how large a difference in means is (in sample standard deviations)
  
ANOVA assumptions (1)
===========================================================
- **Normality**. The observed test statistic ($\frac{{MS}_{model}}{{MS}_{error}}$) can only be assumed to come from an *F*-distribution if:
  - The error variance (i.e. all the variance not explained by the group factors) is normally distributed
    - This is because the $\chi^2$-values are assumed to come from a standard normal distribution

ANOVA assumptions (2)
===========================================================
- **Homogeneity of variances** (also known as **homoscedasticity**). The variances within each group are similar.
      - This is because of the way we add up the variances in each group to get an estimate of the total error variance ${SS}_{error}$:
      $${SS}_{error} = \sum\limits_{m=1}^{p}\sum\limits_{i = 1}^{n}(x_{mi} - \bar{A}_m)^2$$
      - You can only do this if the variances in each group are roughly similar.
        - For example, the IQs *within* the BU group should not be more variable than the IQs *within* the Soton and the Oxford groups
  - If you have more than two groups, SPSS will automatically run Levene's test, which compares the group variances
    - What's the test statistic for a test that compares variances? Of course, it's *F* again!
    - The p-value tells you if the variances are significantly different between groups.

Levene's test for homogeneity of variances
===========================================================
- You could do Levene's test by hand, but if you actually need it, chances are that your design is too complicated to do by hand anyway.
- If Levene's test is not significant, all is well.
- If Levene's test is significant, you're violating the homogeneity of variance assumption.
  - Not a big issue if the sample sizes are equal for all groups (balanced design).
  - If sample sizes aren't equal (unbalanced design) and the larger groups have higher variance, your ANOVA loses power.
  - If sample sizes aren't equal and the larger groups have lower variance, your ANOVA becomes anti-conservative ($\alpha$ increases).

What to do if Levene's tests is significant?
============================================================
- If your group sizes are equal, nothing to worry about. The ANOVA is robust in this regard.
- If not:
  - Calculate the variance for each group and see if you're dealing with just a power issue or an $\alpha$ issue
  - If the largest group variance is less than 4 times the smallest group variance, you may have a power issue, but the test is not anticonservative.
  - If you have huge variance differences and there might be an $\alpha$ issue:
    - Easiest solution: Fix the sample size issue (e.g. run more participants)
    - Use linear mixed models (LMMs; more on that later)
    - Use specialised tests (this is the approach preferred in most SPSS textbooks):
      - Welch's *t*-test
      - Brown-Forsythe (uses median instead of mean)
      - Post-hoc tests:
          - Games-Howell for unequal variance
          - Hochberg's GT2 for non-equal sample sizes

The SPSS (or rather, SPSS textbook) approach to statistics
============================================================
- Throw as many obscure tests at the problem as you can
  - This is a sales strategy: "We need to buy SPSS since no other program has the Games-Howell test!"
- In reality, the standard ANOVA is remarkably robust to all but the most extreme violations of its assumptions
- Specialised tests often come at a huge cost in terms of power
- This doesn't mean that you shouldn't test the assumptions
  - But a simplistic strategy where you run one type of test if the assumption test is significant and another one if it isn't is not helpful
  - Take a good look at your data
    - Be aware of potential issues
    - Interpret the data accordingly.
    - Only use specialised and non-parametric tests as a last resort if your data massively violate the assumptions

Just so we're clear
============================================================
- Inflated $\alpha$ is not harmless
- But "researcher degrees of freedom" inflate $\alpha$ much more than all but the most extreme assumption violations
  - Stopping rules (test after every X participants, then stop as soon as you have a significant result)
  - Failing to report non-significant conditions
  - Failing to correct for multiple comparisons
- Don't let over-cautious textbooks discourage you from running plain, simple ANOVAs
- Be honest and transparent about your data and how you collected them and you'll be fine.

ANOVA assumptions (3)
===========================================================
- **Independence of variances** The variances within each group are independent.
      - There are no systematic relatioships between measurements in each group
      - Most commonly violated by within-participants (repeated measures) designs
        - Participants are tested in multiple conditions
        - This can be addressed by using a repeated-measures ANOVA or Linear Mixed Models.
      
Using SPSS to perform a one-way ANOVA
============================================================
- I took this data set from Andy Johnson, since he has made a great video explaining exactly how to analyse it. 
- We are investigating the effect of swearing on pain tolerance (see Stephens et al., 2009)
- Three groups: continuous use of swear word, neutral word, or no word whilst hand in cold water (DV = time until participant can't stand the pain and pulls hand from water)
- Get the SPSS data file `Swearing and Pain Data.sav` from myBU.
- Watch Andy Johnson's video and follow along.

Multiway ANOVA
=========================================================
- What if we have two independent variables, $A$ and $B$?
- We can still split the total variance into ${SS}_{total} = {SS}_{model} + {SS}_{error}$
  - But now ${SS}_{model}$ is composed of multiple terms: ${SS}_{model} = {SS}_{A} + {SS}_{B} + {SS}_{A \times B}$, so that ${SS}_{total} = {SS}_{A} + {SS}_{B} + {SS}_{A \times B} + {SS}_{Error}$
  - Each of these terms has degrees of freedom: ${df}_{Total} = {df}_A + {df}_B + {df}_{A \times B} + {df}_{Error}$
  - For each term, you can compute mean squares and F values, e.g. $F_A = \frac{{MS}_A}{{MS}_{Error}}$
  - What is ${SS}_{A \times B}$? It's the **interaction** between A and B
      - A *main effect* (A or B) is a difference between means
      - An *interaction* is a difference between differences

Multiway ANOVA (2)
==========================================================
- At this point, doing the analysis by hand is getting really tedious. Leave this to SPSS!
- As a little taster, I'll show you the formula for the total sums of squares: $${SS}_{total} = \sum\limits_{i = 1}^{p}\sum\limits_{j = 1}^{q}\sum\limits_{m = 1}^{n}(x_{ijm} - \bar{x})^2,$$ where $i$ is the level of factor A, $p$ is the total number of levels of factor A, $j$ is the level of factor B, $q$ is the total number of levels of factor B, $m$ denotes the current observation number within its cell (i.e. the $m^{th}$ observation within that combination of A and B), $n$ is the total number of observations within each cell, and the mean is $\bar{x} = \frac{\sum\limits_{i = 1}^{p}\sum\limits_{j = 1}^{q}\sum\limits_{m = 1}^{n}x_{ijm}}{p\cdot q \cdot n}$
- Nice and simple, right? This is why people started writing software to do this!

Multiway ANOVA (2)
==========================================================
- How to compute the dfs:
    - For main effects, just like in the oneway ANOVA: ${df}_A = k_{A} - 1$, where $k_A$ is the number of groups or *levels* of that variable
    - For interactions, it's the product of the dfs of the corresponding main effects:
      - ${df}_{A \times B} = {df}_A \cdot {df}_B$
    - Just as a reminder: ${df}_{Total}$ is still $N - 1$, where $N = p\cdot q \cdot n$ is the total number of subjects or observations in your study (across all variables)
    - And as before, if you subtract all the other dfs from ${df}_{Total}$, you get ${df}_{Error}$ 
    - ${df}_{Error} = {df}_{Total} - {df}_A - {df}_B - {df}_{A \times B}$ 

Interactions
==========================================================
- Main effects are additive
- For example, this table shows the (fictional) total calories that you might have for lunch given two different food choices and two different drink choices:

```{r, echo = FALSE, results='asis'}
library(knitr)
food_example <- data.frame(Food = c(rep("Pizza",2),rep("Salad",2)), Drink = rep(c("Water","Cola"), 2), Calories = c(800, 1100, 200, 500))
kable(food_example)
```

Additive effects
=========================================================
```{r, echo=FALSE}
options(digits = 3, scipen = 5)
library(ggplot2)
qplot(data = food_example, geom = c("point","line"), x = Food, fill = Drink, colour = Drink, group = Drink, y = Calories)
```

Non-additive effects
==========================================================
- Example: Animal and maximum movement speed (meters/s) on land and in water (mostly non-fictional, based on a very quick Wikipedia search)

```{r, echo = FALSE, results='asis'}
move_example <- data.frame(Where = c(rep("Land",2),rep("Water",2)), Animal = rep(c("Dog","Dolphin"), 2), Speed = c(15, 0, .4, 11))
kable(move_example)
```

Non-additive effects (2)
==========================================================
```{r, echo=FALSE}
library(ggplot2)
qplot(data = move_example, geom = c("point","line"), x = Where, fill = Animal, colour = Animal, group = Animal, y = Speed, ylab = "Maximum speed in m/s")
```
- Crossover interaction

Marginal effects
==========================================================
- In the presence of a significant interaction, main effects are much harder to interpret
    - Better to call them marginal effects (although few people do, even in publications!)
- What does it mean that the marginal speed of a dolphin is 7.5 m/s (when averaging over the water and land conditions)?
    - Not much! The mean is nearly meaningless here...
  
Marginal effects (2)
=========================================================
- In some cases, you will still be interested in the marginal effects
    - For example, your anxiety treatment might differ in its effectiveness for male and female participants, but the marginal effects show that overall, everyone benefits from it at least a little.
    - Of course, if males get a little more anxious and females get a lot less anxious (a crossover interaction), the positive marginal effect still doesn't mean you should give males this treatment!
  
How to do a multiway ANOVA
==========================================================
- Example data: Attractiveness, music, and alcohol
- Again, we have a video made by Andy Johnson on how this works in SPSS
- Download the SPSS data file (`Music, beer, and courting.sav`) and follow along with the video.

```{r, echo = FALSE, results='asis'}
attract <- read.csv("attract.csv")
# by default, R will sort factor levels alphabetically
# the following line will make "No alcohol" the baseline level for the "Alcohol" factor
attract$Alcohol <- relevel(attract$Alcohol, ref = "No alcohol")

# Now we make the "subject" column that ezANOVA needs
# Every row gets a different subject number
# The subject column is discrete, so we make it a factor
attract$subject <- factor(1:nrow(attract))
```

Make a means table
=========================================================
```{r, echo = FALSE, results='asis'}
library(ez)
library(knitr)
attract_stats <- ezStats(data = attract, 
                         dv = Attractiveness.Rating, 
                         wid = subject, 
                         between = .(Music, Alcohol))
kable(attract_stats[,-ncol(attract_stats)])
```

Do the ANOVA
===========================================================
```{r echo = FALSE}
attract_anova <- ezANOVA(data = attract, 
                          dv = Attractiveness.Rating, 
                          wid = subject, 
                          between = .(Music, Alcohol))
kable(attract_anova$ANOVA, col.names = c("Effect", "${df}_n$", "${df}_d$", "$F$", "$p$", "$p < 0.5$", "$\\eta_P^2$"))
```
- All three terms are significant.
    - Why $\eta_P^2$ instead of $\eta^2$? We want an estimate of the **partial** effect of each predictor. The standard $\eta^2$ is still the comparison between ${SS}_{model}$ and ${SS}_{error}$, which doesn't tell us much. 
- Partial $\eta^2$ (i.e. $\eta_P^2$) only takes into account the SS for our effect and ${SS}_{error}$, e.g. for Factor A: $\eta_P^2 = \frac{{SS}_A}{{SS}_A + {SS}_{error}}$

Assumption tests:
===========================================================
- Levene's test:

```{r echo=FALSE}
kable(attract_anova$"Levene's Test for Homogeneity of Variance", col.names = c("${df}_n$", "${df}_d$", "${SS}_n$","${SS}_d$","$F$", "$p$", "$p < 0.5$"))
```

- No problems with homogeneity of variances

Pairwise comparisons
============================================================
- Easiest way: Use Tukey's HSD (I'll explain that with SPSS, since the output is a bit complicated)

Writing it up
============================================================
A 2-factor (2x3) independent samples ANOVA was conducted where the first factor represents music exposure (quiet and music) and the second factor represents alcohol condition (no alcohol, 1-pint, and 4-pints). There was no evidence for a violation of the homogeneity of variance assumption. Overall, the ANOVA method should be robust to the slight deviation from normality that was observed. Attractiveness ratings were significantly higher with music exposure, *F*(1,54) = 5.01, *p* =.03, $\eta_G^2$ = .09. The main effect of alcohol was also significant, *F*(2,54) = 59.79, *p* < .001, $\eta_P^2$ = .69. A post hoc test (Tukey's HSD) indicated that participants who drank 4 pints of beer rated attractiveness as significantly higher than participants who had no alcohol (*p* < .001) and one pint (*p* < .001). There was no difference between the no alcohol and 1-pint groups (*p* = .11). 

Writing it up (2)
=============================================================
The music by alcohol interaction was also significant, *F*(2,54) = 3.24, *p* = .047, $\eta_G^2$ = .11. This indicates that alcohol had different effects under conditions of music exposure. Specifically, post-hoc comparisons showed that with no alcohol there was no difference in attractiveness ratings for music (M = 49.30, SD = 5.50) and no music (M = 47.90, SD = 4.91). Similarly, following 1-pint there was no difference in attractiveness ratings for music (M = 52.20, SD = 5.88) and no music (M = 52.30, SD = 5.77). However, following 4-pints attractiveness ratings were higher with music (M = 70.40, SD = 5.04) than without music (M = 62.30, SD = 5.36). This effect was significant (*p* = .018).

Repeated measures
=============================================================
- Remember the paired *t*-tests? We can have the same situation (more than one data point from one participant) in a more complex design.
- This is bad, because we violate the independence assumption in the standard ANOVA.
- This is good, because we can use a repeated-measures ANOVA to remove all between-participant variance
- ${SS}_{total} = {SS}_{betweenParticipants} + {SS}_{withinParticipants}$
- ${SS}_{withinParticipants} = {SS}_{model} + {SS}_{residual}$ (we call this the residual sum of squares rather than the error sum of squares, since technically the variance between participants is also error variance)
- Result: Less unexplained variance and higher power.
- In the between-subjects ANOVA the variance between participants is completely confounded with the error variance within participants.
- In the repeated measures ANOVA, we can separate them!

Repeated measures data matrix
=============================================================
- Almost the same as for the standard one-way ANOVA
  - Columns are factor levels, rows are **participants**
- $i$ = factor level, $p$ = number of factor levels
- $m$ = **participant**, $n$ = number of **participants**

$$
\begin{matrix}
x_{11} &  x_{12}  & \ldots & x_{1i} & \ldots & x_{1p}\\
x_{21} &  x_{22}  & \ldots & x_{2i} & \ldots & x_{2p}\\
\vdots & \vdots   &       & \vdots  &        &\vdots \\
x_{m1} &  x_{m2}  & \ldots & x_{mi} & \ldots & x_{mp}\\
\vdots & \vdots   &       & \vdots  &        &\vdots \\
x_{n1} &  x_{n2}  & \ldots & x_{ni} & \ldots & x_{np}\\
\end{matrix}
$$

Calculating the sums of squares
=============================================================
- The total sum of squares is still ${SS}_{total} = \sum\limits_{m=1}^{n}\sum\limits_{i = 1}^{p}(x_{mi} - \bar{x})^2$
- ${df}_{total} = n\cdot p-1$
- The between participants sum of squares is new. It is $${SS}_{betweenParticipants} = p \cdot \sum\limits_{m=1}^{n}(\bar{P}_{m} - \bar{x})^2,$$
where $\bar{P}_m$ is the mean for Participant $m$ and $p$ is the number of factor levels
- Same for the within participans sum of squares: $${SS}_{withinParticipants} = \sum\limits_{i=1}^{p}\sum\limits_{m=1}^{n}(x_{mi} - \bar{P}_{m})^2,$$
where $\bar{P}_m$ is the mean for Participant $m$ and $i$ is the factor level

Calculating the sums of squares (2)
=============================================================
- Finally, the model SS is just as before: $${SS}_{model} = n \cdot \sum\limits_{i=1}^{p}(\bar{A}_{i} - \bar{x})^2,$$ where $n$ is the number of participants, $\bar{A}_{i}$ is the mean of level $i$ of the group factor, and $p$ is the number of factor levels.
- The residual SS is a little bit more complicated (this is already a simplified version): $${SS}_{residual} = \sum\limits_{i=1}^{p}\sum\limits_{m=1}^{n}(x_{mi} - \bar{A}_i-\bar{P}_{m}+\bar{x})^2$$
  - Of course, you can just get it by subtracting the model SS from the within participant SS: $${SS}_{residual} = {SS}_{withinParticipants} - {SS}_{model}$$
  
Degrees of freedom
============================================================
- ${df}_{total} = p\cdot n - 1$
- ${df}_{betweenParticipants} = n - 1$
- ${df}_{withinParticipants} = n \cdot (p - 1)$
- ${df}_{model} = p - 1$
- ${df}_{residual} = (n-1) \cdot (p - 1)$
  - Where $p$ is the number of factor levels and $n$ is the number of participants

Test statistic
==============================================================
- Important: You get the *F*-value by dividing the model mean squares by the **residual** mean squares: $F_{A} = \frac{{MS}_{model}}{{MS}_{residual}}$
- The degrees of freedom of this *F*-value are ${df}_{numerator} = {df}_{model}$ and ${df}_{denominator} = {df}_{residual}$

Quick example by hand
=============================================================
- 10 cats were asked to try 3 different brands of cat food: Whiskers, Paws, and Industrial Waste. They received the same amount of each food after not having eaten for 8 hours. The dependent variable amount of food (in grammes) that they ate of each brand. Do cats prefer one or more brands over others or do they eat the same amount of each?

Copy this table into Excel (or SPSS)
==============================================================
```{r echo = F}
set.seed("3")
n_participants <- 10
overall_intercept <- 100

Subject <- rep(1:n_participants, each = 3)
Brand <- rep(1:3)

subject_intercept <- rnorm(length(Subject), mean = 0, sd = 30)

brand_means <- c(25, 25, -50)

random_error <- rnorm(length(Subject), mean = 0, sd = 10)

df <- data.frame(Subject, Brand)

df$eaten <- round(with(df, overall_intercept + subject_intercept[Subject] + brand_means[Brand] + random_error),0)

df$Subject <- factor(df$Subject, labels = c("Cali",
	"Callie",
	"Casper",
	"Charlie",
	"Chester",
	"Chloe",
	"Cleo",
	"Coco",
	"Cookie",
	"Cuddles"))

df$Brand <- factor(df$Brand, labels = c("Whiskers","Paws","Industrial Waste"))

library(reshape)
df_m <- melt(df, measure = "eaten")
df_c <- cast(df_m, Subject ~ Brand)
kable(df_c)
```

Doing the ANOVA in Excel
==============================================================
- Start by calculating subject and condition means using `=AVERAGE`. You should have one mean for each of the $n = 10$ cats (we'll assume that those are in `E2:E11`) and one mean for each of the $p = 3$ conditions (We'll assume that these are in `B12:D12`)
- Calculate your Sums of Squares
  - ${SS}_{model} = n \cdot \sum\limits_{i=1}^{p}(\bar{A}_{i} - \bar{x})^2$; in Excel: `=10*DEVSQ(B12:D12)`
    - Remember, `DEVSQ` is the squared deviation of the input values from their mean. Since we have a balanced design (all sample sizes are equal), the mean of the group means (and the mean of the subject means) is the overall mean.
  - ${SS}_{betweenParticipants} = p \cdot \sum\limits_{m=1}^{n}(\bar{P}_{m} - \bar{x})^2$; in Excel: `=3*DEVSQ(E2:E11)` (same principle as above)
  
Sums of squares (continued)
===============================================================
- ${SS}_{total} = \sum\limits_{m=1}^{n}\sum\limits_{i = 1}^{p}(x_{mi} - \bar{x})^2$; in Excel: `=DEVSQ(B2:D11)`, assuming that your data are in `B2:D11`
- ${SS}_{withinParticipants}$ requires a bit of extra work to calculate in Excel. The easiest way is to just subtract ${SS}_{betweenParticipants}$ from ${SS}_{total}$: ${SS}_{withinParticipants} = {SS}_{total} - {SS}_{betweenParticipants}$
- ${SS}_{residual}$ is also tricky. The easiest way is again to subtract: ${SS}_{residual} = {SS}_{withinParticipants} - {SS}_{model}$
- Then calculate the degrees of freedom and the MS as shown earlier
- Important: remember that the *F* value is calculated as $F_{A} = \frac{{MS}_{model}}{{MS}_{residual}}$

Final step
==============================================================
- Look up the *p*-value: `=F.DIST.RT(E15,2,18)`, where `E15` contains the *F*-value
- For comparison: ANOVA output form R
```{r, echo = FALSE}
summary(ezANOVA(data = df, wid = Subject, within = Brand, dv = eaten, return_aov = TRUE)$aov)
```

Post-hoc comparisons
==============================================================
- Compute pairwise *t*-tests
  - For each subject, calculate the differences between the conditions
  - For each comparison, calculate the mean and the sd of the difference values (the sample mean and SD)
  - Then compute the three observed t-values: $t_{d} = \frac{\mu_1 - \mu_2}{s_{d}/sqrt(n)} = \frac{\bar{d}}{s_d/{sqrt}(n)}$, where $d$ stands for the comparison that you're calculating and $n$ is the sample size within each comparison
  - Look up the *p*-value using `=T.DIST.2T(ABS(D16), D17)`, assuming that `D16` contains the *t*-value and `D17` contains the degrees of freedom (${df}_{d} = n - 1 = 9$)
  - Don't forget to correct for multiple comparisons: Multiply the p-values by 3 (because you are making three comparisons). Then you can compare them with a critical *p*-value of $\alpha = .05$.

One-way repeated measures ANOVA in SPSS
==============================================================
- Watch Andy Johnson's video on myBU and follow along with the data set (`Badger art identification.sav`).

Assumptions
==============================================================
- Essentially the same as for the independent ANOVA
    - Except: You no longer need to assume that the observations are independent (since observations from the same subjects are of course systematically related).
    - New assumption: Sphericity (this replaces the homogeneity of variances assumption)

What is sphericity?
==============================================================
- The variances of the differences between treatment levels should be roughly equal ("spherical")
- For example, it could be that all cats react similarly to the first two brands
    - But the "Industrial Waste" brand might might really be enjoyable for some cats, while others might eat nothing (not the case ion our example data)
- In that case, the difference between "Whiskers" and "Paws" would have a very low variance
    - But the difference between "Whiskers" or "Paws" and "Industrial Waste" would have a huge variance
- This could make the ANOVA anticonservative ($\alpha$ is inflated)

Testing for sphericity violations
=================================================================
- Mauchly's Test for Sphericity
- Performed automatically by SPSS
- If it's significant, sphericity is violated.
- In this case, we're OK
- You only need to test sphericity if you have more than two factor levels (i.e. conditions in your factor)
- If you only have two levels, there is only one difference, so differences can't be unequal

Dealing with sphericity violations
==================================================================
- Good news: It's easy. 
- Essentially, you can lower your degrees of freedom for the F-test to compensate for lack of sphericity
    - The F-value doesn't change, but lowering the df will make it harder to get a low *p*-value
- You do this by multiplying the ${df}_{Model}$ and ${df}_{Error}$ by a correction factor $\varepsilon$
- Two ways to calculate $\varepsilon$:
    - Greenhouse-Geisser
    - Huynh-Feldt
- Recommendation: If Greenhouse-Geisser $\varepsilon < .75$, use it. Otherwise, use Huynh-Feldt.
   - Of course, if Mauchly's test is not significant, use neither!
- SPSS computes the dfs for you and you just have to pick the corrected entry in the table

Testing for normality
===================================================================
- You can do the Shapiro-Wilk test in SPSS
- Normality isn't usually *much* of an issue, especially if your group sizes are equal
  - The **ANOVA is robust**

Post-hoc tests
===================================================================
- You can again use paired *t*-tests to compare factor levels
- Remember to do Bonferroni corrections if you do these tests by hand


Transformations
========================================================
- In some cases, our dependent variable will not be normally distributed
- Example: reaction times -- you get a long right tail of slow responses
    - Fixation times in eye movements are very similar

Example
========================================================
- For example, the probability density function for fixation durations might look like this:

```{r, echo = FALSE}
curve(dlnorm(x, 5.5, .3), from = 0.1, to = 1000)
```

Example data
========================================================
- Example experiment: how long do people look at swear words vs. non-swear words?
    - Let's assume that the true means are 250 ms for non swear words and 300 ms for swear words
- Let's generate data based on this assumption
```{r, echo = FALSE}
set.seed("11233")
# 60 subjects
word_condition <- factor(c(rep("swear word", 30), rep("non swear word", 30)))
# rlnorm: Generate random samples from the lognormal distribution
fixation_time <- c(rlnorm(n = 30, 
                          mean = log(250), 
                          sd = .3), 
                   rlnorm(n = 30,
                          mean = log(265), 
                          sd = .3))
swear_exp <- data.frame(word_condition, fixation_time)
```

Running a linear model
========================================================
```{r, echo = FALSE}
linear_model <- lm(data = swear_exp, fixation_time ~ word_condition)
summary(linear_model)
```

Plotting the fitted line
========================================================
```{r, echo = FALSE}
with(swear_exp, plot.default(y = fixation_time, x = word_condition))
abline(linear_model)
```

Histogram of the residuals
========================================================
```{r, echo = FALSE}
hist(resid(linear_model), 10)
```
- Clearly not normal - it's right-skewed!
- But notice how robust the analysis is. We still find the effect!

Logarithmic transformations
=========================================================
- Better to run a proper model where the values as the dependent variable
  - This is our first step into the world of generalised linear models (GLM)
- The most common transformation uses the logarithm
  - Remember the logarithm from school? 
    - I hope you remember exponentiation: $x \cdot x \cdot x \cdot x = x^4$
    - Every logarithm has a base
      - The logarithm of a number is the exponent to which the base needs to be raised to produce that number
      - For example, the base $x$ logarithm of $x^4$ is $4$, since $x$ needs to be raised to the 4th power to produce $x^4$: $log_x(x^4) = 4$

The natural logarithm
==========================================================
- It doesn't really matter which base you use for your logarithm
- In mathematics, the algorithm to the base $e = `r round(exp(1),3)`$ called the *natural logarithm*. It is used frequently, since it has some very convenient properties. The logarithm to the base $e$ is called $log_e$ or $ln$.
- Just remember that $ln(e^x) = x$ and $e^{ln(x)} = x$.
- Also, remember that $x^{a+b} = x^a \cdot x^b$
    - Because of this, $e^{ln(x) + ln(y)} = x\cdot y$
    - And, the other way around, $ln(e^x \cdot e^y) = x+y$

Running a log model
========================================================
- Use `Compute` in SPSS to transform the predicted variable

```{r, echo = FALSE}
log_model <- lm(data = swear_exp, log(fixation_time) ~ word_condition)
summary(log_model)
```

Histogram of the residuals of the transformed model
========================================================
```{r, echo = FALSE}
hist(resid(log_model), 10)
```

- Much better! No longer skewed.

How to interpret a log model
========================================================
- We are transforming the predicted values using the natural logarithm ($ln$) here. This is a logarithm to the base $e = `r exp(1)`$. You may remember from school that $e^{ln(x)} = x$ and $ln(e^x) = x$.
- Important: You can only log-transform positive and non-zero values. If you have zeroes or negative values in your dependent variable, you have to transform it.
  - For example, to eliminate zeroes, you might add a tiny amount to all values.
- Log models are *multiplicative* rather than *additive*:
  - $e^{x+y} = e^x \cdot e^y$, and $e^{ln(x) + ln(y)} = x\cdot y$
- Our model formula: $ln(y_{i}) = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \varepsilon_i$
  - Let's rewrite that: $y_{i} = e^{\alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \varepsilon_i} = e^{\alpha} \cdot e^{\beta_1x_{i1}} \cdot e^{\beta_2x_{i2}}$

How to interpret a log model (2)
========================================================
- Example: Our swear word fixation time study
    - Fitted model: $ln(y_{i}) = 5.683 - .195 \cdot x_i$
    - Remember: We're using treatment contrasts. $x_i$ is 0 for non swear words and 1 for swear words
    - Predicted value for non swear words: $e^{5.683} \cdot e^{-.195 \cdot 0} = e^{5.683} =  293.82$
    - Predicted value for swear words: $e^{5.683} \cdot e^{-.195 \cdot 1} = 293.82 \cdot e^{-.195} = 293.82 * .823 = 241.81$
- Conclusion: Fixation times on swear words were 17.7% lower than fixation times on non swear words (*b* = -.195, *SE* = .074, *t* = -2.63, *p* = .011).

Summary
========================================================
- If our data aren't normally distributed, we can sometimes transform them to get them closer to normality.
- A very common transformation uses the logarithm (usually the natural logarithm with base $e$, but others can be used to)
- You can't log-transform negative or zero values, so if your data contain any of those, you have to remove them or transform them (e.g. by adding a constant value) before doing the log-transformation
- The regression equation of a log-model is additive in its log format, but multiplicative when transformed back into raw values     - e.g. $log(y) = \alpha + \beta X_i \leftrightarrow y = e^{\alpha + \beta X_i} = e^{\alpha} * e^{\beta X_i}$

Logistic regression
========================================================
- What if we have a dichotomous dependent variable?
    - Yes vs. no, error vs. no error, alive vs. dead, pregnant vs. not pregnant
- Our example (from A. Johnson): Factors that make (or don't make) you fail your driving test
- 90 candidates
- Dependent variable: `Driving.Test`: Yes or No
- Predictor variables:
    - `Practice`: in hours
    - `Emergency.Stop`: Whether the candidate performed the emergency stop (yes or no)
    - `Examiner`: How difficult the examiner is (on a scale from 0 = easy to 100 = extremely difficult)
    - `Cold.Remedy`: How many doses of cold remedy the candidate had before the test

Examining the data
=======================================================
- These are our data (first 6 rows):

```{r, results='asis', echo =FALSE}
library(knitr)
driving_tests <- read.csv("driving_tests.csv")
kable(head(driving_tests))
```

Where to start?
=========================================================
- First, let's recode our dependent variable, replacing "No" with 0 and "Yes" with 1
```{r, results='asis', echo =FALSE}
driving_tests$Driving.Test <- as.numeric(driving_tests$Driving.Test) - 1
kable(head(driving_tests))
```

Let's plot it
=========================================================
- Now we can make a plot of the situation
- Let's just consider the number of practice hours right now and ignore the other predictors

```{r, echo = FALSE}
plot(x = driving_tests$Practice, y = driving_tests$Driving.Test, ylab = "Driving Test (+ jitter)",  xlab = "Practice (in hours)")
```

Adding some jitter
=========================================================
- Since the y-value is either 0 or 1, a lot of the points are plotted on top of each other (overplotting)
- To avoid this, we add or subtract a small amount to the y value ("jitter")
  - Now we can see the pattern a little better.

```{r, echo = FALSE}
plot(x = driving_tests$Practice, y = jitter(driving_tests$Driving.Test, factor = .25), ylab = "Driving Test (+ jitter)",  xlab = "Practice (in hours)")
```

What do we want to estimate?
=========================================================
- The regression line gives us the expected $\hat{y}$-value (the mean $y$-value) for each value $x_i$
- In this case, if we code our dependent variable as 0 and 1, the mean (expected value) at each x value will give us the conditional probability $E(Y|x_i) = \pi_i$ of our event. 
  - Note that we're talking about estimating the theoretical probability in the population, hence the Greek letter. This is not the mathematical constant $\pi = `r pi`$
- Then our model can give us predictions like this: What's the probability of passing the test given that I've had (at most) 20 hours of practice, did my emergency stop, had at most an average examiner (50) and had only one cup of cold remedy?
    
Predicting probabilities
==========================================================
- We can try a standard linear function
- This would be our model then: $P(Y_i = 1) = \alpha + \beta X_{i} + \varepsilon_i$

```{r, echo = FALSE}
plot(x = driving_tests$Practice, y = jitter(driving_tests$Driving.Test, factor = .25), ylab = "Driving Test (+ jitter)",  xlab = "Practice (in hours)")
lines(abline(lm(data = driving_tests, Driving.Test ~ Practice)))
```

Problems with a linear prediction
==========================================================
- But it's not very well suited!
  - It predicts probabilities $P(Y_i = 1) < 0$ and $P(Y_i = 1) > 1$ for some $x$-values!
  - Also, the error variance is clearly not equal for all $x$-values
  - And we really can't pretend that our data are normally distributed.

Using a different regression function
=========================================================
- We would like a function that makes sure our linear predictor $\alpha + \beta x_i$ stays between 0 and 1
 - This **link function** $P$ would apply to our linear predictor and transform it into a probability: $\pi_i = P(\alpha + \beta x_i)$
- If the function is monotone, meaning that it consistently increases as $\alpha + \beta x_i$ gets larger, that would be advantageous, since the inverse of the function $P^{-1}$ then links each value predicted by the linear model with the probability  $P^{-1}(\pi_i) =\alpha + \beta x_i$
- There are two link functions that are most commonly used: the cumulative probability function of the normal distribution and the logistic function

The cumulative probability function of the normal distribution
=========================================================
- Look up the function definition on Wikipedia, if you really care.

```{r, echo = FALSE}
curve(pnorm(x), -3,3)
```

The logistic distribution function
=========================================================
- Here, the definition is simpler: $\Lambda(z) = \frac{1}{1+e^{-z}}$
  - Where $z$ is a *z*-value, and $e$ is the constant $e = `r exp(1)`$. $\Lambda$ is the capital letter Lambda.

```{r, echo = FALSE}
curve(plogis(x), -3,3)
```

Applying the link functions
========================================================
- Applying the cumulative normal distribution function gives you the **linear probit model**.
- Applying the logistic distribution gives you the **linear logit model**, also called **linear logistic regression**:
$$\pi_i = \Lambda(\alpha + \beta x_i) = \frac{1}{1+e^{-(\alpha+\beta x_i)}}$$
- Both functions work equally well, but the logistic distribution function has the advantage that the transformed values are directly interpretable:
  - Rewrite the above equation and you get **odds**: $$\frac{\pi_i}{1-\pi_i} = e^{\alpha+\beta x_i}$$
    
Probability and odds
========================================================
- Very popular in betting, since they make it easy to estimate the payout
- $Odds = \frac{P}{1-P}$
- For example: 
      - $P = .5$ gives you even odds $\frac{.5}{.5} = 1/1$
      - $P = .25$ gives you $\frac{.25}{.75} = 1/3$
      - $P = .75$ gives you $\frac{.75}{.25} = 3/1$
- Odds are nice because they aren't bounded, but for high probabilities they get very large very quickly ($P = .99 \Leftrightarrow Odds = 99/1$) and for small probabilities, they get very small very quickly ($P = .01 \Leftrightarrow Odds = 1/99$)

Log odds (logits)
=========================================================
- Just transform our odds (just like we did with our continuous fixation time variable earlier) by taking the natural logarithm: $logit = ln(Odds) = ln(\frac{P}{1-P})$
- Now we have a dependent measure that is suitable for linear relationships
- Log odds is just what we happen to get if we apply the natural logarithm on both sides of our logistic regression equation from earlier:
$$
\begin{aligned}
\frac{\pi_i}{1-\pi_i} &= e^{\alpha+\beta x_i}\\
\leftrightarrow ln(\frac{\pi_i}{1-\pi_i}) &= ln(e^{\alpha+\beta x_i})\\
\leftrightarrow ln(\frac{\pi_i}{1-\pi_i}) &= \alpha+\beta x_i\\
\end{aligned}
$$

The logistic model
==========================================================
- So, our full model (including errors) is
$$logit_i = ln(\frac{\pi_i}{1-\pi}) = \alpha + \beta x_i + \varepsilon_i$$
- If we want to get back from logits to probabilities, we can do that by reversing the transformation:
$$\pi_i = \frac{1}{1+e^{-(logit_i)}}$$

How do you fit a logistic regression line?
=========================================================
- Sadly, you can't fit the logistic regression model using the least squares approach (the errors still aren't nromally distributed or homoscedastic)
- We can evaluate the **likelihood** of the parameters instead
- In statistics, probability and likelihood aren't the same:
  - Probability: observations given parameters
  - Likelihood: parameters given observations

Calculating likelihood
========================================================
- Very simple example: Let's assume we have the following data from flipping a coin: $Y = (H H H T H T H H)$
- Likelihood is the product of all the probabilities given a certain parameter value. In this case, we are trying different parameter values for the probability: 
    -We're interested in the population parameter, so again, our population probability of observing "heads" (ignoring order) will be called $\pi$:
- $\pi = .5$: $L(Y|\pi = .5) = .5 \cdot .5 \cdot .5 \cdot .5 \cdot .5 \cdot .5 \cdot .5 \cdot .5 = .5^8 = .00039$
- $\pi = .25$: $L(Y|\pi = .25) = .25 \cdot .25 \cdot .25 \cdot .75 \cdot .25 \cdot .75 \cdot .25 \cdot .25 = .25^6 \cdot .75^2 = .00014$
    - $\pi = .25$ has a lower likelihood than $\pi = .5$.
- Let's try $\pi = .75$: $L(Y|\pi = .75) = .75 \cdot .75 \cdot .75 \cdot .25 \cdot .75 \cdot .25 \cdot .75 \cdot .75 = .75^6 \cdot .25^2 = .00111$
    - This is the highest one yet.

The likelihood function for logistic regression
========================================================
- Do you see a pattern here? For each element $Y_i$ in $Y$, the likelihood of $\pi$ is either
    - $L(Y_i|\pi) = \pi$ if $Y_i = H$, (e.g. $.75$ for $\pi = .75$), or
    - $L(Y_i|\pi) = 1 - \pi$ if $Y_i = T$, (e.g. $.25$ for $\pi = .75$)
- Then you get the likelihood for the full data set $Y$ by multiplying all the individual likelihoods
    - $L(Y|\pi) = \prod_{i = 1}^N{L(Y_i|\pi)}$
- You can simplify this a bit if you replace H with 1 and T with 0:
    -  $L(Y_i|\pi) = \pi^{Y_i} \cdot (1-\pi)^{(1-Y_i)}$
    - And combining the two equations above:
        - $L(Y|\pi) = \prod_{i = 1}^N\pi^{Y_i} \cdot (1-\pi)^{(1-Y_i)}$

Maximum likelihood
========================================================
- Likelihood gets a little unwieldy -- lots of very small numbers
    - Solution: take the log (who would have thought?)
        - Added bonus: Now our multiplication becomes a sum (remember that from calculus?)
          $log\;likelihood = \sum_{i = 1}^N Y_i\cdot ln(\pi) + (1-Y_i)\cdot ln(1-\pi)$
- Now we can simply try different values of $\pi$ until we find the one with the maximum likelihood
    - Remember that in logistic regression, $\pi$ is defined by our regression equation: $\pi = \frac{1}{1 + e^{-(\alpha + \beta x_{i1} + \varepsilon_i)}}$ 
    - Instead of simply trying different values of $\pi$, we have to try different values for $\alpha$, $\beta_1$, etc. and compute $\pi$. This gets to be quite a lot of work.
    
Fitting the model through an iterative process
=======================================================
- Trying different values might not seem particularly elegant, but this is essentially what R or SPSS do -- no simple solution like with least-squares regression or ANOVA exists
- This is (relatively) processing-intensive, which is one reason why psychologists didn't use logistic regression in the statistics-by-hand era.
      
Log likelihood as an indicator of model fit
=======================================================
- The log likelihood ($LL$) of the final model is an indicator of how well the model fit the data, just like $R^2$.
    - In fact, there are several ways to estimate $R^2$ from the log likelihood
- Log likelihood also enables us to make model comparisons
    - The test statistic in that case is $\chi^2$ -- more about that later
- Another measure is *deviance*, which is simply $-2\cdot LL$
    - Conceptually, deviance is like the residual variance.
        - In the case of deviance, lower is better, of course.
- Closely related to this is Akaike's Information Criterion (AIC), which is $-2\cdot LL + 2\cdot \text{number of parameters}$ (lower is better, so adding parameters makes the AIC worse)

How to fit the model in SPSS
========================================================
- You can't do this in Excel anymore.

```{r, echo = FALSE}
driving_tests$Emergency.Stop <- factor(driving_tests$Emergency.Stop, levels = c(1,0), labels = c("Yes", "No"))

#driving_tests$Cold.Remedy <- factor(driving_tests$Cold.Remedy)

driving_glm <- glm(data = driving_tests, 
                   formula = Driving.Test ~ Practice + Emergency.Stop + Examiner + Cold.Remedy, 
                   family = binomial(link = "logit"))
```

- To see how it works, watch Andy Johnson's video on myBU.

Plotting the model fit
==========================================================
```{r, echo = FALSE}
plot(x = driving_tests$Practice, y = jitter(driving_tests$Driving.Test, factor = .25), ylab = "Driving Test (+ jitter)",  xlab = "Practice (in hours)")
curve(predict(driving_glm, newdata = data.frame(Practice = x, Emergency.Stop = driving_tests$Emergency.Stop[1], Examiner = mean(driving_tests$Examiner), Cold.Remedy = mean(driving_tests$Cold.Remedy)), type="resp"),add=TRUE)
```

Model summary
==========================================================
- The coefficients

```{r, echo=FALSE, results='asis'}
kable(coef(summary(driving_glm)))
```

Significance tests for coefficients
=============================================================
- Analogous to linear regression: *b* value, *SE*, significance test
- Using the **Wald** statistic instead of *t* tests
    - $z = \frac{b}{SE_b}$
    - We can use *z*-values because our dependent variable comes from the binomial distribution. In the binomial distribution, the variance is always a function of the mean, so we don't have to estimate it from the sample (and since we're not using the sample variance to estimate the population variance, we don't need to use a t-test).

Interpreting the b values
==========================================================
- If you take the exponential of the coefficients , you get an **odds ratio**
- Odds ratio = Odds after a unit change in the predictor divided by the original odds
- Example: According to our model, each hour of practice increases the odds of passing the test by a factor of $e^{0.12959} = `r exp(.12959)`$
    - e.g. if the odds were even (1/1) for X hours of practice, they would be slightly better than even (1.138/1) for X+1 hours of practice
- On the other hand, each "unit" of examiner difficulty decreases the odds of passing the test by a factor of $e^{-0.03485} = `r exp(-.03485)`$
    - e.g. if the odds were even (1/1) for an instructor with a difficulty of X, they would be slightly worse than even (0.9658/1) for an instructor with a difficulty of X+1

Model comparisons (LRT)
==========================================================
- Deviance has some neat properties
    - We can compare likelihoods just like we compared mean squares in the *F*-test: by dividing them
    - That is, we compute a likelihood ratio: $LR = \frac{L_{baseline}}{L_{new}}$, where *baseline* is the simpler model and *new* the more complex model.
    - Now we can convert this likelihood ratio into a deviance: $deviance_{LR} = -2\frac{ln(L_{baseline})}{ln(L_{new})} = deviance_{baseline} - deviance_{new}$
    - And now the most fun part: If the $H_0$ that the two models explain the data equally well is true, this likelihood-ratio deviance is distributed according to a $\chi^2$ distribution.
    - The $\chi^2$ distribution has one parameter, degrees of freedom
    - $df = k_{new}$ - $k_{baseline}$, where $k$ is the number of parameters (including the intercept)

Model comparisons
==========================================================
- Now we can get a *p*-value! This is called the **Likelihood ratio test (LRT)**
- So, if we want to test if the model is better than a model with just the intercept, we can do an LRT
- $\chi^2 = deviance_{null} - deviance_{model} = 124.366 - 82.572 = 41.794$
- $df = k_{null} - k_{model} = 89 - 85 = 4$ 
- $p(\chi^2(4) \geq 41.794) < .01$
- This is equivalent to the overall *F*-test for the model.
- SPSS calls this the "Omnibus test"

Model comparisons (2)
========================================================
- We can use model comparisons to test how specific predictors contribute to the whole model (analogous to the *F*-tests in linear regression)
```{r, echo = FALSE}
library(car)
Anova(driving_glm)
```

Model comparisons (3)
========================================================
- This analysis of deviance follows the same logic as the ANOVA in the linear regression case
- You can do Type I, Type II, and Type III LRT tests (they are not sums of squares in this case)
- The LRTs are better tests than the Wald tests, since the Wald tests might be prone to overinflating the SE, leading to Type II error.

Diagnostics and assumption tests
========================================================
- We do not assume normality (so nothing to test for that one)
- All the influence measures from linear regression work in logistic regression as well
- Multicollinearity:
  - You can get Variance Inflation Factors (VIFs) and Tolerance etc. in SPSS if you fit the standard linear model first
    - Of course, don't interpret the coefficients that you get!
- You can also take a look at the Hosmer-Lemeshow test (see Andy Johnson's video)


Looking at logistic regression residuals
========================================================
- Can't test if they are normally distributed (because they are not)
- But look out for very large residuals
- You can get still get standardised residuals.
    - Look out for cases that are far away from 0.
```{r, results='asis', echo = FALSE}
driving_residuals <- rstandard(driving_glm)
plot(driving_residuals)
```


Reporting it
=============================================================
A logistic regression was conducted where the dependent variable was passing a driving test and the predictor variables were hours of practice, whether an emergency stop was successfully executed, how much the examiner was difficult, and amount of 'cold remedy drinks' consumed.  90 cases were examined and the model was found to significantly predict whether the test was passed (omnibus chi-square = 41.79, df=4, p<.001). that practice and examiner were the only two variables that reliably predicted if the driving test was passed. Increases in practice was associated with increased rate of passing (odds of passing increased by 1.14 per hour of practice, *b* = .130, SE = .03, *z* = 4.28, *p* < .01). Increases in the examiner difficulty reduced the rate of passing (odds of passing decreased by 0.96 per unit of difficulty rating, *b* = -.00349, SE = .013, *z* = -2.679, *p* < .01). None of the other predictors reached significance (all *p*s > .05). There were no issues due to multicollinearity or influential cases.

Summary: Logistic regression
===============================================================
- If you have a dichotomous outcome variable (e.g. Yes vs. No, Heads vs. Tails, etc.), you cannot use standard linear regression
- Instead, you must use a $link function$ to convert your predicted values into probabilities and vice versa. The most commonly used are the cumulative distribution function (CDF) of the normal distribution and the CDF of the logistic distribution
    - if you use the CDF of the normal distribution, you are fitting a **probit** model
    - if you use the CDF of the logistic distribution, you are fitting a **logit** or logistic regression model
- The logit model has the advantage that you can interpret the coefficients directly as changes in log odds $= ln(frac{\pi_i}{1-\pi_1})$
- Just like when you are using a log transformation, the coefficients are additive as far as log odds are concerned, but multiplicative when you convert the log odds into odds.
- In both cases, you use the Wald test as a significance test for coefficients (analogous to *t*-tests in standard linear regression) and the likelihood ratio test (LRT, analogous to *F*-tests in standard linear regression) in order to test the significance of predictors.
    - Since we don't have to estimate the error variance from the data, the Wald statistic is $z$-distributed and the LRT statistic is $\chi^2$ distributed.
    - Just like the standard oneway/multiway ANOVA is a special case of linear regression with only categorical predictors, the $\chi^2$-test is a special case of logistic regression with only categorical predictors.

Linear mixed models (LMMs)
============================================================
- The final step to greatness!
    - Note that we can really only scratch the surface here.
- Main issue:
    - We know how to to regressions for continuous and discrete DVs now
    - We know what the regression equivalent of a between-subjects ANOVA is and we can take the regression analysis much further than an ANOVA or traditional ANCOVA would let us
    - However: 
        - What if we have a within-subject or repeated measures design?
        - What if there is some other underlying correlation in the data 
        - e.g. data collected from students in different classes in different schools
        - Surely the classes and schools share some variance -- how to account for that?
                    
Moving from linear regression to linear mixed models
=============================================================
- In repeated-measures ANOVA, we've dealt with within-subjects effects by removing the variance due to subject differences from the error
    - Essentially, we have added a "subject" factor to the model
    - Linear mixed models enable us to do the same thing for all kinds of regression analyses

Problem: how to add subject as a factor
=============================================================
- We could simply add a "subject" factor to the predictors
    - This would reflect the systematic differences between subjects
        - But that's not quite right: how do we deal with a factor with 40 levels?
        - Also, we want to generalise our model to more than those 40 subjects that are in the analysis
        - How do we do that?
    - Subject is really like a random variable: we get a different set each time we run the experiment
    - Instead of analysing the subject effect in a generalisable way, we really just want to get rid of the subject variance in the most efficient way possible

Problem: how to add subject as a factor (2)
=============================================================
- Fixed effects vs. random effects
    - Fixed effects: repeatable, generalisable (e.g. experiment condition)
    - Random effects: non-repeatable, sampled from a general population
    - Mixed effects models include both fixed and random effects
- Another issue with including subject as a fixed effect:
    - Each subject would take up a degree of freedom
    - That would majorly impact the power of our analysis
    - LMMs solve this issue by a procedure called **shrinkage**
    
Shrinkage
===============================================================
- Conceptually, LMMs allow subjects to have individual effects (e.g. in an eye-tracking experiment subject 1 might have an intercept of 200 ms, while subject 2 might have an intercept of 210 ms), but they pull each subject's effects
towards an underlying distribution of subject effects
- This reflects the idea that if 20 other subjects have intercepts between 180 and 220 ms, the current subject is unlikely to have an intercept of 400 ms, even though it looks like that from the data
- Shrinkage also helps majorly with missing data issues (although it won't fix them for you!)
- The downside of shrinkage is that it isn't clear what the $df_{Error}$ should be
    - This leads to some issues later on.
    
Example
==============================================================
A PhD student wants to investigate whether our mood affects how we react to visual scenes. In order to do this, she showed 40 subjects a total of 40 scenes. There are two version of each scene: one contains people, the other one doesn't -- everything else is identical. The PhD student spent a considerable amount of time taking photos to ensure this (until her supervisor got a bit impatient). Before the experiment, all subjects were asked to rate their current mood on a scale from 0 (very sad) to 100 (very happy). They then looked at each scene and rated how much they liked it on a scale from 0 (hate it) to 20 (love it). The student's hypothesis is that if you are happy, you should want to see scenes with people. If you are unhappy, you should prefer scenes without people. The data are given below. Will the student find what she is looking for? Or will she have to start from scratch and be in even more trouble with her supervisor?

Example Data
==============================================================
- Subject: Subject ID (1-40)
- Item: Item ID (1-40)
- Scene Type: within-item factor ("no people" vs. "people")
- Mood: between-subject factor (scale from 0--100)
- Rating: Dependent variable (scale from 0 to 20)
- For these kinds of data, R has a massive advantage over SPSS in terms of usability.
  - But you can still get the same results in SPSS!
- Don't be scared, I'll be walking you through this with R first and then show you how to do it in SPSS.
```{r}
# Start by loading the data
scene_liking <- read.csv("Class 6 exercise data.csv")
```

Looking at the data
=========================================================
```{r, results='asis'}
kable(head(scene_liking))
```

Looking at the data (2)
=========================================================
```{r}
str(scene_liking)
# We should set subject and item to be 0 (nominal scale)
scene_liking$subject <- factor(scene_liking$subject)
scene_liking$item <- factor(scene_liking$item)
```

Calculating means
=========================================================
- Let's get condition means for scene type
- In theory, we could report means by subject or means by item
- Either one would be fine, but usually people report subject means.
    - We use `melt` and `cast` from the `reshape` package to calculate the means
```{r}
library(reshape)
# set rating as the dependent (measure) variable
scene_liking.m <- melt(scene_liking, measure = "rating")
# collapse over item; calculate means
scene_liking.c <- cast(scene_liking.m, subject + mood + scene ~ variable, mean)
```

Calculating means (2)
=========================================================
```{r}
head(scene_liking.c)
```

Calculating means (3)
=========================================================
- Now we can use this to calculate our means for the scene condition
```{r}
(mean_people <- mean(subset(scene_liking.c, scene == "people")$rating))
(mean_no_people <- mean(subset(scene_liking.c, scene == "no people")$rating))
```

Calculating means (3)
=========================================================
- Let's also get sd, N, and SE
```{r}
(sd_people <- sd(subset(scene_liking.c, scene == "people")$rating))
(sd_no_people <- sd(subset(scene_liking.c, scene == "no people")$rating))
(N_people <- length(subset(scene_liking.c, scene == "people")$rating))
(N_no_people <- length(subset(scene_liking.c, scene == "no people")$rating))
(SE_people <- sd_people/sqrt(N_people))
(SE_no_people <- sd_no_people/sqrt(N_no_people))
```

Plotting the interaction
==========================================================
- We're really interested in the interaction between `scene` and `mood`.
    - Unfortunately, mood is a continuous variable
    - How to plot this?
- Use `qplot` from `ggplot2` with `geom = "smooth"`
    - This will give you a plot showing a smoothed conditional mean for each value of mood

Plotting the interaction (2)
=========================================================
```{r, message=FALSE, warning=FALSE}
library(ggplot2)
qplot(data = scene_liking, y = rating, x = mood, colour = scene, geom = "smooth")
```

Plotting the interaction (2)
=========================================================
- Look at how amazingly pretty that is. You just can't do stuff like that with SPSS.
- Looks like the student was right!
- Also looks like the effect is not really completely linear
    - Maybe this is due to the subject and item effects in the data?
    - Let's find out!

Start with linear regression
=========================================================
- Let's check our contrasts for `scene`
```{r}
contrasts(scene_liking$scene)
```
- Are we happy with this?
    - Sure -- we just have to be aware of the coding when we interpret the coefficients

Run the model
=========================================================
```{r, results='asis'}
scene_lm <- lm(data = scene_liking, rating ~ mood * scene)
kable(coef(summary(scene_lm)))
```

- Where did the interaction go?
- Let's do some quick diagnostics

Regression diagnostics
=========================================================
- Multicollinearity?
```{r}
vif(scene_lm)
```
- Aha! Those VIFs are quite  a bit larger than 1. That spells trouble.
- What is wrong?

Addressing multicollinearity
=========================================================
- What is wrong?
- We forgot to center the continuous predictor `mood`
- Let's fix this:
```{r}
scene_liking$mood <- scale(scene_liking$mood, scale = FALSE) # See Class 5
```

Run the model again
=========================================================
```{r, results='asis'}
scene_lm <- lm(data = scene_liking, rating ~ mood * scene)
kable(coef(summary(scene_lm)))
```

- Still not quite there...
- Let's do more diagnostics

Regression diagnostics -- again
=========================================================
- Multicollinearity?
```{r}
vif(scene_lm)
```
- The VIFs are fine now.

Influential cases
=========================================================
```{r, results='asis'}
kable(head(influence.measures(scene_lm)$infmat))
```

- Any Cook's d greater than 1?

```{r}
sum(cooks.distance(scene_lm) > 1 )
```

- Doesn't look like it, so we should be fine here.

Q-Q Plots
===========================================================
- Here's a visual way to assess normality
- Quantile-Quantile Plot: Split data into a number of quantiles and plot them against the quantiles of a hypothetical normal distribution

```{r}
qqnorm(resid(scene_lm))
# if the distribution is normal, the points should be on this line
qqline(resid(scene_lm))
```

How to fix this?
===========================================================
- As a first step, remember that there are subject and item effects in these data
- `lm` can't account for them, so we need something more powerful
- Linear Mixed Models!
- We use the function `lmer` ("Linear mixed effects regression") from the `lme4` package
- If you don't have `lme4` yet, install it by typing `install.packages("lme4")` in the Console

Adding random subject and item effects
===========================================================
- As a first step, we want our model to allow subjects and items to have different intercepts
    - For example, Subject 1 might just really dislike the whole experiment and rate all scenes lower
    - Or Item 33 might be particularly ugly and be disliked by all subjects
- Formally, our model will look like this: $y_{ij} = \alpha + \beta_1 x_{1} + \beta_2 x_{2} + \gamma_{0i} + \gamma_{0j} + \varepsilon_{ij}$, where $y_{ij}$ is the response of subject $i$ to item $j$, $\gamma_{0i}$ is the intercept for subject $i$ and $\gamma_{0j}$ is the intercept for item $j$

Running the model
============================================================
- In `lmer`, we specify the model in a formula just like in `lm`, but we add random effects terms, e.g. `(1|subject)`
    - The left side of the pipe stands for the random effect, the right side stands for the group for which we want to define the random effect
    - `1` stands for the intercept. It is implicitly added, *except* when there is no other predictor
```{r}
library(lme4)
scene_lmm <- lmer(data = scene_liking, rating ~ scene * mood + (1|subject) + (1|item))
```

Examining the model
============================================================
```{r}
summary(scene_lmm)
```

Understanding the model output
===========================================================
- Just like in logistic regression, LMMs are fitted in an iterative procedure using Maximum Likelihood (ML)
    - Actually, `lmer` by default uses a slightly modified criterion called Restricted Maximum Likelihood (REML)
- Residuals can be interpreted just like in a regular linear model
- Random effects: Here we get an estimate of the variance (and standard deviation) explained by the random intercepts
    - We also get an estimate of the residual variance
- Check the number of observations to see if there are any missing that shouldn't be missing

Coefficients
===========================================================
```{r, results='asis'}
kable(coef(summary(scene_lmm)))
```

- First thing you notice: There's no *p* value, even though the Wald statistic should follow a *t*-distribution
- That's because, due to the shrinkage procedure, it isn't clear what the df of that *t*-value should be
- In general, if the number of subjects is > 30, we should be able to interpret the *t* value like a *z* value, meaning that any *t* > 2 or < -2 should be significant
- SPSS uses a procedure called the Satterthwaite approximation for coming up with degrees of freedom for the *t*-values

Correlation of fixed effects
==========================================================
- These are the estimated correlations of the fixed effects
    - If any of these is > .8, you're in multicollinearity trouble!
    
Model comparisons
==========================================================
- Unfortunately, *F*-tests won't work, because we don't know what the $df_{Error}$ would be
- But we can do the likelihood ratio test (LRT)
- As always, we use `Anova` from `car`. This one gives us *p* values!
```{r}
library(car)
Anova(scene_lmm)
```

More model diagnostics
=========================================================
- Something still seems to be wrong with this model. How about testing the normality assumption again?
```{r}
shapiro.test(resid(scene_lmm))
```
- Still significant? Maybe there still is a random effect that we haven't accounted for.

Random slopes
=========================================================
- We can also allow the regression slopes to vary by subject or item.
- What are possible random slopes that we could consider?
    - Important: in theory, you could add random slopes for all fixed effects, but in practice, your data might not have enough information to fit these
    - In this case, there simply isn't enough data to fit random slopes for the interaction
      - How do you know this?
      - Well, your model will simply fail to converge if there is not enough data for a solution!
      - Even if there *is* enough data, multicollinearity can cause convergence failures, too.
    - In our case, some reasonable random slopes would be `scene` for subjects (do some people react more strongly to scenes with people than others) and `mood` for items (are some items really hated by people in a bad mood?)
    
Random slopes (2)
=========================================================
- If we include a random slope for subjects for $beta_1$, our model looks like this:
$y_{ij} = \alpha + \beta_1 x_{1} + \beta_2 x_{2} + \gamma_{0i} + \gamma_{1i} x_{1} + \gamma_{0j} + \varepsilon_{ij}$
- We can tell `lmer` to fit such models like this (note that the intercept is implicit again in `(mood|item)` and `(scene|subject)`.
    - Note that we don't have enough data to include both random effects in one model.
```{r}
scene_lmm_mood <- lmer(data = scene_liking, 
                       rating ~ scene * mood + (1|subject) + (mood|item))
scene_lmm_scene <- lmer(data = scene_liking, 
                        rating ~ scene * mood + (scene|subject) + (1|item))
```

Testing the effect of random slopes
===========================================================
- We can use the LRT to test whether the slopes actually improve the models.
    - We use the `anova` command (lower case `a`) to compare each random slope model with the random intercept model we fitted earlier
```{r}
anova(scene_lmm, scene_lmm_mood)
```
- Seems to improve the model! (Note that R automatically uses ML instead of REML for model comparison)

Testing the effect of random slopes
===========================================================
```{r}
anova(scene_lmm, scene_lmm_scene)
```
- No improvement here.

Diagnostics -- yet again!
==========================================================
```{r, echo = FALSE}
shapiro.test(resid(scene_lmm_mood))
```
- Looks like adding a random slope for `mood` also (mostly) fixed our normality problem

Interpreting the coefficients
==========================================================
```{r, results='asis'}
kable(coef(summary(scene_lmm_mood)))
```

- Now we have significant effects!
- Remember that scene was coded as 0 = no people, 1 = people
    - Looks like, on average, subjects gave the scene a rating that was -.26 lower when it contained people than when it didn't.
- The interaction is also significant. When the scene contained no people, there was a very weak, non-significant negative effect of mood. 
    - When the scene did contain people, there was a significant change in the effect of mood (with each point on the mood scale increasing the picture rating by -.007 + .0101 = .0031). Not a huge effect, but significant.

Writing it up
==========================================================
- See the exercise!

Summary: Linear mixed models
==========================================================
- We can do repeated-measures analyses as well as more complex hierarchical models (e.g. students within classrooms within schools) using linear mixed models (LMMs)
- Such models combine fixed effects (where we test all possible factor levels or predictor values, e.g. condition) and random effects (where we can only test a subset of a population of factor levels, e.g. participant, or school, or county, or sentence)
- Usually, you start with a fixed-effects model and then add random intercepts and slopes.
    - Random effects are always centered around the corresponding fixed effects. For example, we might have a fixed intercept of 50, with a random intercept for Subject 1 of 50-10 = 40 and a random intercept for Subject 2 of 50+10 = 60.
- Model fitting is iterative and can be tricky. Overspecified models often end up failing to converge.
- Which random effects to include? Figuring out which random effects actually explain variance can take awhile.

Thank you!
==========================================================
- I know this was (and still is) a massive effort
- Thank you for staying motivated and engaging with the material.
- As always, come see me if you have questions!