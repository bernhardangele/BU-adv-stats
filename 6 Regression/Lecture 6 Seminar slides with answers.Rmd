---
title: "Lecture 6 Seminar slides with answers"
author: "Regression"
date: "Press F for fullscreen"
output: 
  revealjs::revealjs_presentation:
    self_contained: true
    transition: fade
    css: "robot-lung.css"
---

# Some questions about regression

-   For this seminar, we will use various interactive visualisations to address some important details about regression that may still not be completely clear.

# How is the regression curve fitted?

-   How does Ordinary Least Squares (OLS) work?
    -   Minimise the sum (or the mean) of the squared errors (differences between predictions -- the line -- and the actual data points).

# Two ways to fit a regression curve

-   Solve some equations:

    -   [This interactive demonstration](https://antoinesoetewey.shinyapps.io/statistics-202/ "Link to webapp") by Antoine Soetewey from UC Louvain shows how you can calculate any simple regression equation by hand -- just enter the data in the spreadsheet.

-   Try different solutions until you find the best possible one:

    -   [This interactive visualisation](http://shinyapps.org/showapp.php?app=https://tellmi.psy.lmu.de/felix/lmfit "Find-a-fit!") by Felix Sch√∂nbrodt lets you try different intercepts and slopes to best fit the points in a data set. It plots the residual sum of squares (i.e., the sum of squared errors) for each attempt.

        -   You can also let an optimisation algorithm do the work for you. This is how parameters are estimated using maximum likelihood. Click the second button to see how this is done.

# Visualising the sums of squares

-   Try this [visualisation by Daniel Kunin from Brown University](https://seeing-theory.brown.edu/regression-analysis/index.html) which visualises the squared errors for each data point as actual squares. You can also move data points around and see how the regression equation changes
-   The data used here are four data sets known as **Anscombe's quartet**:
    -   Each of the data sets has the exact same regression equation, but if you plot them, they are very different indeed

    -   This is a reminder that a linear regression is not guaranteed to describe your data well

    -   You can fit a line to anything, but the line is not necessarily an appropriate fit to your data

# Multicollinearity

-   The interpretability of a model can be affected by multicollinearity, i.e. correlations between the predictors
-   [This visualisation by Saskia Otto](https://saskiaotto.de/shiny/collinearity/ "Collinearity visualisation") shows the effect of covariance (or correlation) between two predictor variables on the resulting regression model.
-   Note that the standard error of the estimates is larger the more covariance there is
-   Why is that the case?
    - *Answer:* Higher multicollinearity leads to higher standard errors of the coefficient estimates. You can think of it in terms of the ease of estimating the model: if there are two variables that are completely independent from each other, it is easy to say how much each contributes to the model. If the two variables are highly related to each other, it is very difficult to determine. In the extreme case of having two variables that are perfectly correlated (e.g. height in cm and height in inches), it is impossible to add them both to a regression model since the model would would be exactly as good with just one of the two or any combination of the two.

# Regression coefficients are random variables

-   [This visualisation, also by Saskia Otto](https://saskiaotto.de/shiny/reproducibility/), illustrates the point that, just like means, regression coefficients estimated from a sample are likely to be different from the true regression coefficients in the population (if indeed a true linear relationship exists in the population)
    -   The larger our sample size, the closer the coefficients in our estimated sample will (on average) be to the coefficients in the population
    -   Of course, the more noise there is in the population, the more variability there is in the samples.

# Challenge questions

-   What is the intercept and what does it represent?
    - **Answer:** In a plot, it represents the predicted value when all the predictors are zero. Depending what zero means for each predictor that can mean different things in practice. 
        - For example, in a simple regression where your predicted variable is a person's weight and your predictor is a person's height (in cm), 0 is essentially meaningless, and the value of the intercept is, too.
        - If you have centred your height predictor so that 0 now represents the average height of your participants, then the intercept reflects the predicted weight of a participant of average height.
-   If the coefficient for the intercept in a regression model is significant (or not significant), what does that mean?
    - It means that we can reject the null hypothesis that the intercept is exactly 0. Usually, that is not very surprising, so we do not care about this test in many situations.
-   If the coefficient for a slope in a regression model is significant, what does that mean?
    - It means that the null hypothesis that the slope is exactly 0 can be rejected. In other words, it means that there is evidence that the predictor helps explain the variance in the model **above and beyond** what the other predictors in the model explain.
-   Can you have a situation where the regression model as a whole is significant (on the F-test), but none of the coefficients are significant?
    - Yes, in case of multicollinearity, where the predictors all share a lot of variance. In this case, it is clear that the predictors taken together improve the model, but it is impossible to tell which one improves the model **above and beyond** the others, since they are so similar.
-   What is the difference between a t-test between two groups (assuming both have the same variance) and the t-tests we do for the coefficients in a regression model?
    - There really is very little difference -- we will talk about this in the next lecture.