Advanced Statistics
========================================================
author: Bernhard Angele
date: Hypothesis tests for simple and multiple regression


Statistical inference for regression
=========================================================
- This lecture corresponds to Chapter 6 in Fox (2013)
  - You can find the more involved details (such as a lot of the derivations) there
- Goals for this lecture:
  - Learn how to do hypothesis tests on least-squares regression results
      - For individual predictors (t-tests)
      - For entire models (F-tests)
      - Learn how to use SPSS to perform these tests for you
  - Learn about assumptions that need to be true so you can perform these tests
  - Introduction to the problem of **multicollinearity**
      
Hypothesis tests (for simple regression)
=========================================================
- When we do a regression analysis, we _assume_ that there is a true linear relationship in our population : $$Y_i = \alpha + \beta\cdot x_i + \varepsilon_i$$
  - $\alpha$ and $\beta$ are our population coefficients. $\varepsilon_i$ is the error for this particular observation.
  
```{r, echo=FALSE}
# From http://stackoverflow.com/questions/31794876/ggplot2-how-to-curve-small-gaussian-densities-on-a-regression-line
library(ggplot2)
set.seed(0)
dat <- data.frame(x=(x=runif(100, 0, 50)),
                  y=rnorm(100, 10*x, 100))

## breaks: where you want to compute densities
breaks <- seq(0, max(dat$x), len=5)
dat$section <- cut(dat$x, breaks)

## Get the residuals
dat$res <- residuals(lm(y ~ x, data=dat))

## Compute densities for each section, and flip the axes, and add means of sections
## Note: the densities need to be scaled in relation to the section size (2000 here)
dens <- do.call(rbind, lapply(split(dat, dat$section), function(x) {
    d <- density(x$res, n=50)
    res <- data.frame(x=max(x$x)- d$y*2000, y=d$x+mean(x$y))
    res <- res[order(res$y), ]
    ## Get some data for normal lines as well
    xs <- seq(min(x$res), max(x$res), len=50)
    res <- rbind(res, data.frame(y=xs + mean(x$y),
                                 x=max(x$x) - 2000*dnorm(xs, 0, 50)))
    res$type <- rep(c("empirical", "normal"), each=50)
    res
}))
dens$section <- rep(levels(dat$section), each=100)

## Plot both empirical and theoretical
ggplot(dat, aes(x, y)) +
  geom_point() +
  geom_smooth(method="lm", fill=NA, lwd=2) +
  geom_path(data=dens[dens$type=="normal",], aes(x, y, group=section), color="salmon", lwd=1.1) +
  theme_bw() +
  geom_vline(xintercept=breaks, lty=2)
```
- We will need to estimate our regression equation based on our sample, so that $a$ and $b$ are estimates of the population coefficients $\alpha$ and $\beta$.
- $\varepsilon$ is the **error**. It represents all the influences (random or not) that we are not considering in the linear relationship.
    - If the relationship is actually linear, then $E(\varepsilon) = 0$
- How can we interpret these estimated coefficients? Mostly, we care about whether $\beta = 0$ or not, i.e. whether there is or isn't a significant relationship between $x$ and $y$ in the population.

Assumptions
==========================================================
- First, we need to assume that $x$ and $y$ come from a **bivariate** normal distribution (i.e. that they are both normally distributed) with means $\mu_x$ and $\mu_y$, variances $\sigma^2_x$ and $\sigma^2_y$, and covariance $\sigma_{x,y}$.
- The bivariate (or, more generally, multivariate) normal distribution assumes that:
    - For each $x$-value $x_j$, the corresponding $y_{(i|x_j)}$-values are normally distributed: $\epsilon \sim N(0, \sigma_{\epsilon}^2$)
    - For each $x$-value $x_j$, the corresponding $y_{(i|x_j)}$-values have the same standard deviation (homoscedasticity assumption): $Var(\epsilon_i) = \sigma_{\epsilon}^2$
    - For each $x$-value $x_j$, the corresponding $y_{(i|x_j)}$-values are independent (i.e. all $\epsilon_i$ are independent)

Distribution of the coefficient estimates: Means
===========================================================
- If our assumptions are met, our coefficient estimates $A$ and $B$ will be themselves normally distributed
- What do their distributions look like?
    - Expected values (i.e. means): $E(A) = \alpha$; $E(B) = \beta$
        - The coefficients are unbiased estimators of the population coefficients

Distribution of the coefficient estimates: Variances
===========================================================
- What about the variance?
  - We don't really care about $Var(A)$ (see page 109 in Fox, 2015), but $Var(B)$ is important for our hypothesis tests: $$Var(B) = \frac{\sigma_{\varepsilon}^2}{\sum\limits_{i=1}^{n}(x_i-\bar{x})^2} = \frac{\sigma_{\varepsilon}^2}{(n-1)\cdot s^2_{x}}$$
- This means that the sampling variance of our estimate of $\beta$ will be smaller when:
    - Numerator: the overall error variance $\sigma_{\varepsilon}^2$ is small
    - Denominator: we have a large number of observations $(n-1)$ and $x$-values in our sample are spread out (the sample variance of $x$, $S_x^2$ is larger)

Doing a hypothesis test on B
==========================================================
- We now know the sampling distribution of $B$ (if our assumptions are met): $$ B \sim N\left(\beta, \frac{\sigma_{\varepsilon}^2}{\sum\limits_{i=1}^{n}(x_i-\bar{x})^2}\right)$$
- But we don't know $\sigma_{\varepsilon}^2$
    - Does this remind you of something?
        - Can we estimate this variance?
        
Doing a hypothesis test on B when the error variance is not known
=============================================================
- (i.e. pretty much in any situation)
- Remember what we did in Lecture 3: we took the sample variance to estimate the population variance
- Now we estimate the error in the population $\sigma_\varepsilon^2$ by the error in the sample $S^2_E$:
$$S^2_E = \frac{\sum\limits_{i = 1}^{n} E_i^2}{n-2} = \frac{\sum\limits_{i = 1}^{n} (Y_i - \hat{Y}_i)^2}{n-2}$$
    - Why divide by $n-2$? So we get an unbiased estimator of the error. Not showing you the derivation of why it has to be $n-2$.
- Remember this? We are *estimating* both the mean and the error variance from the sample, so we need to use the *t*-distribution instead of the standard normal distribution.

Testing coefficients
===========================================================
- Our null hypothesis is usually $H_0: \beta = 0$
    - i.e. there is no systematic relationship between $x$ and $y$; you can't predict $y$ from $x$
- We already have our best estimate of $\beta$ (the coefficient $B$ calculated from the sample). We still need an estimate for the standard error of $B$. We take the definition of $Var(B)$ from above and plug in $S_E^2$ for $\sigma_{\varepsilon}^2$
   $$SE(B) = \sqrt{Var(B)} = \sqrt{\frac{S^2_E}{\sum\limits_{i=1}^{n}(x_i-\bar{x})^2}}= \frac{S_E}{\sqrt{\sum\limits_{i=1}^{n}(x_i-\bar{x})^2}}$$


Calculating the t-value
===================================================================
- We calculate a *t*-value for our observed slope coefficient $B$ just like we do when we're comparing means $$t = \frac{B - \beta_0}{SE(B)}$$ where $\beta_0$ is the value of $\beta$ assumed by the null hypothesis. 
    - Usually, $\beta_0 = 0$, so we can rewrite this as $t = \frac{B}{SE(B)}$
- As always, this *t*-value has as many degrees of freedom as the denominator of the estimated variance. In the case of the error variance, $df_t = n-2$.
- We can also get confidence intervals, just like when comparing means: $CI: B \pm t_{\alpha/2}\cdot SE(B)$. We reject the $H_0$ if the CI doesn't include 0.

Back to our example from last week
=============================================================
- Remember, we're trying to predict cat food eaten from cat weight.

```{r, echo = FALSE}
library(knitr)
n <- 30
intercept <- 100
options(digits = 3, scipen = 4)

set.seed("126")
cat_weight <- rnorm(n, mean = 5, sd = 1) # cat weight in kg
cat_age <- rnorm(n, mean = 60, sd = 24) # cat age in months
cat_breed <- factor(c(rep("Shorthair",n/2), rep("Manx", n/2)))

food_eaten <- intercept + 20*(cat_weight - mean(cat_weight)) + 2 + .5*(cat_age - mean(cat_age)) + rnorm(n, mean = 0, sd = 10)
catfood <- data.frame(CatWeight = cat_weight, FoodEaten = food_eaten)
catfood_age <- data.frame(CatWeight = cat_weight, CatAge = cat_age, FoodEaten = food_eaten)


print_p <- function(p){
  if(p < .01) return("*p* < .01")
  if(p <= .05) return(paste("*p* =", p))
  if(p > .05) return("*p* > .05")
}
plot(x = catfood$CatWeight, y = catfood$FoodEaten, xlab = "Cat Weight", ylab = "Food Eaten")
abline(mod1 <- lm(catfood$FoodEaten ~ catfood$CatWeight))
pre <- predict(mod1) # plot distances between points and the regression line
segments(catfood$CatWeight, catfood$FoodEaten, catfood$CatWeight, pre, col = "red")
```

Calculating the coefficients
=======================================================
- We can calculate $B$ from the data by first getting the covariance:
$$
\begin{aligned}
&Cov(CatWeight,FoodEaten) \\ &= \frac{\sum\limits_{i = 1}^n {(CatWeight_i - \overline{CatWeight})\cdot(FoodEaten_i - \overline{FoodEaten})}}{n-1} \\ &= `r with(catfood, cov(CatWeight, FoodEaten))`
\end{aligned}$$

- Then 
$$B = \frac{Cov(CatWeight, CatFood)}{S_{CatWeight}^2} = \frac{`r with(catfood, cov(CatWeight, FoodEaten))`}{`r var(catfood$CatWeight)`} = `r (b <- with(catfood, cov(CatWeight, FoodEaten))/var(catfood$CatWeight))`$$

Calculating the t-value
===============================================================
- Now calculate the standard error of $B$ (replacing $CatWeight$ with $x$ and $CatFood$ with $Y$ for better readability): $$SE(B) = \frac{S_E^2}{(n-1)\cdot S^2_{x}} = \frac{\frac{\sum\limits_{i = 1}^{n} (Y_i - \hat{Y}_i)^2}{n-2}}{(n-1)\cdot S^2_{x}} = \frac{\frac{`r sum(resid(mod1)^2)`}{`r nrow(catfood)` - 2}}{(`r nrow(catfood)` - 1)\cdot `r var(catfood$CatWeight)`} = `r (var_b <- sum(resid(mod1)^2)/(nrow(catfood)-2)/((nrow(catfood) - 1)*var(catfood$CatWeight)))`$$
- And the $t$-value: $$t_{`r nrow(catfood)-2`} = \frac{B}{SE(B)} = \frac{`r b`}{\sqrt{`r var_b`}} = \frac{`r b`}{`r sqrt(var_b)`} = `r b/sqrt(var_b)`$$
- The degrees of freedom are $n-2=28$ (number of observations minus 1 for the intercept and 1 for the slope).

p-value and significance test
================================================================
- Finally, look up the *p*-value (e.g. in Excel): $p(|t_{`r nrow(catfood)-2`}| = `r b/sqrt(var_b)`) = `r format(pt(b/sqrt(var_b), nrow(catfood)-2, lower.tail = FALSE),digits = 2)`$
- Remember, we are doing a two-tailed test, so we have to multiply the p-value by 2 if we want to compare it to $\alpha = .05$: $p =`r format(2*pt(b/sqrt(var_b), nrow(catfood)-2, lower.tail = FALSE),digits = 2)`$ 
- Looks like we can reject the $H_0$ this time: Heavier cats tend to eat more.

Alternative: Do an F-Test to compare variances
================================================================
- Remember, an F-Value is is what you get when you divide a chi-square value by another chi-square value
    - That is, when you divide one variance estimate by another
- Basically, we compare the variance explained by the model with the error variance
    - If there is no effect, the variance attributed to the model will be solely due to random error, as will be the error variance (of course).
    - So, if there is no effect, the expected values of both variance estimates should be the same, and if you divide one by the other you should get an F-value close to 1.
    - You can occasionally get an F-value greater than one. The F-distribution tells you how likely that is given that the null hypothesis is true.

Calculating the F-value
================================================================
- Let's calculate the variance estimates. First the variance explained by the model.
    - We start with sums of squares (SS)
        - $SS_{model}$: The variance explained by the regression model. The sum of squared differences between the predicted values and the mean.
    $$SS_{model} = \sum\limits_{i=1}^{n}(\hat{Y}_i - \bar{Y})^2$$
        - $SS_{error}$: The variance that isn't explained by the regression model, i.e. the residual or error variance. The sum of squared differences between the actual values and the predicted values.
    $$SS_{error} = \sum\limits_{i=1}^{n}(Y_i - \hat{Y}_i)^2$$
    
Relationship between the sums of squares
=================================================================
- We partition the total variance between model and error variance  
- $SS_{total}$: The overall variance. The sum of squared differences between the actual values and the mean.
    $$SS_{total} = \sum\limits_{i=1}^{n}(y_i - \bar{y}_i)^2$$
- $SS_{model}$ and $SS_{error}$ sum up to $SS_{total}$:
$$SS_{total} = SS_{model} + SS_{error}$$

Degrees of freedom
===================================================================
- In order to actually get variance estimates, we have to divide the SS by their degrees of freedom.
- $df_{model} = p - 1$, where p is the number of regression parameters (intercept and slopes)
- $df_{error} = n - p$, where $n$ is the number of observations and $p$ is the number of regression parameters.
- $df_{total} = n - 1$, where $n$ is the number of observations
- $df_{model} + df_{error} = df_{total}$

Mean squares
==================================================
- In order to get variance estimates, we have to divide the sums of squares by their degrees of freedom
- For the cat example:
$MS_{model} = \frac{SS_{model}}{df_{model}} = \frac{\sum\limits_{i=1}^{n}(\hat{Y}_i - \bar{Y})^2}{1} = `r sum((predict(mod1)-mean(catfood$FoodEaten))^2)`$
- $MS_{error} = \frac{SS_{error}}{df_{error}} = \frac{\sum\limits_{i=1}^{n}E_i = \sum\limits_{i=1}^{n}(Y_i - \hat{Y}_i)^2}{n-2} = \frac{`r sum(resid(mod1)^2)`}{`r nrow(catfood)` - 2} = `r sum(resid(mod1)^2)/(nrow(catfood) - 2)`$

F-test
=====================================================================
- $F_{(1, `r nrow(catfood) - 2`)} = \frac{MS_{model}}{MS_{residual}} = \frac{`r sum((predict(mod1)-mean(catfood$FoodEaten))^2)`}{`r sum(resid(mod1)^2)/nrow(catfood) - 2`} = `r (my_f <- sum((predict(mod1)-mean(catfood$FoodEaten))^2)/(sum(resid(mod1)^2)/(nrow(catfood) - 2)))`$
- $p(F_{(1, `r nrow(catfood) - 2`)} = `r my_f`) = `r format(pf(my_f, 1, nrow(catfood) - 2, lower.tail = FALSE), digits = 2)`$ 
- Guess what? The square root of the *F*-value is our *t*-value. This only works if we have one single predictor: $\sqrt{F_{(1,28)}} = t_{28} = `r sqrt(my_f)`$


Summary: What did we find?
============================================================
- The best fitting line for the cat food data intersects the $y$-axis at the point (0, `r lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[1]`).
  - (We never bothered to estimate $a$ by hand, but that's what you would get.)
  - Not all x-values are sensible for all data. Saying that a cat with 0 kg weight would eat `r lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[1]` g of food makes no sense, since a cat with 0 kg weight is not a cat.
  - The linear function doesn't care, of course. It knows nothing about our data and just specifies a line.
- The slope might be more useful: It says that for each kg of extra weight, a cat will eat `r lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[2]` more grammes of food.
    - Using this information, we can predict that a giant 8 kg cat would eat $`r lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[1]` + `r lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[2]` \cdot 8 = `r lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[1] + lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[2] * 8`$ g of food.
    
Summary: Predictions and residual errors
===============================================================
- Of course, our prediction is likely to be at least a little off.
- If we had an 8 kg cat in our data and its actual amount of food consumed was 170 g, we'd have an error of $E_i = `r 170 - (lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[1] + lm(formula = FoodEaten ~ CatWeight, data = catfood)$coefficients[2] * 8)`$.
  - This is called the residual error.
- More formally, the **population** regression equation looks like this (where $x_i$ are the individual values for the $x$ variable, and $y_i$ are the corresponding values for the $Y$ variable):
    - $y_i = \alpha + \beta_1 x_i + \varepsilon_i$
    - Here, we've simply renamed the intercept to $\alpha$ and the slope to $\beta_1$.
    - $\varepsilon_i$ is the residual error for each data point.
    - Important: $\varepsilon_i$ is assumed to be normally distributed
      - This doesn't matter for the line fitting, but it does for the hypothesis tests!
      
Summary: Hypothesis testing
=================================================================
- Important: Note that the $\beta$ variables are greek letters, which means they are the *population parameters*
- For each $\beta$ coefficient in the regression formula, we can propose the $H_0$ that the true value of that $\beta$ coefficient is 0
- The $\beta$ that are estimated from our sample are simply called $B$
- We can once again test if our $B$ values are extreme enough so they would only occur 5% of the time or less given the $H_0$.
- We test this separately for each $B$ value. Guess what, it's a *t*-test (an F-test is also possible)!
- We can also test whether the intercept $A$ is 0
    - This is usually not particularly interesting unless you have a very specific hypothesis about the intercept.

Multiple regression
=================================================================
- Unlike simply running a hypothesis test on a correlation, we can easily add another predictor to a linear model, making it a multiple regression model, where $x_{1i}$ is observation *i* on the first predictor and $x_{2i}$ is observation *i* on the second predictor:
    - $Y_i = \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{1,i} \cdot x_{2,i} + \varepsilon_i$
    - Note that we have an interaction term in this equation: $\beta_3 x_{1,i} \cdot x_{2,i}$
      - We could also specify the model without the interaction if we think there might be a possibility that the effects are just additive:
          - $y_i = \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \varepsilon_i$
    - Which of the models is better?
      - That's exactly what we want to find out!

Hypothesis testing in multiple regression
==================================================================
- Getting the least-squares coefficients $A$ and $B_1$, $B_2$, etc. from the sample is computationally intensive in multiple regression. Let's leave this to Excel/SPSS.
- Once we have the coefficients, we would of course like to run hypothesis tests on them (mostly the slopes).
- If the assumptions (see above) are true, then the sample coefficients are unbiased estimators of the population coefficients. So $B_1$ will be distributed around $\beta_1$ and so on.
- But what is the sampling variance of the estimates?
    - We can estimate the error variance from the sample again: $S_E^2 = \frac{\sum E_i^2}{n-k-1}$, where $n$ is the number of observations and $k$ is the number of predictors.

The sampling variance of each B
===================================================================
- Each predictor $X_j$ has its own coefficient $B_j$ (i.e. $X_1$ has $B_1$ and so on as seen above)
- The sampling variance is different for each $B_j$ (i.e. $B_1$, $B_2$, $B_3$, and so on)
$$ Var(B_j) = \frac{1}{1-R_j^2} \cdot \frac{\sigma_\varepsilon^2}{\sum\limits_{i=1}^{n}(x_{ij}-\bar{x_j})^2}$$
- Let's take this apart:
  - $\frac{\sigma_\varepsilon^2}{\sum\limits_{i=1}^{n}(x_{ij}-\bar{x_j})^2}$ is what we already know from single regression: the error variance over the variability in $x_j$
- The first part, $\frac{1}{1-R_j^2}$ is new. It is called the Variance Inflation Factor (VIF).
    
Variance Inflation Factor (VIF)
===================================================================
- The greater the VIF, the greater the variance and standard error of $B_j$. In other words, the greater the VIF, the greater the uncertainty about the estimate of $\beta_j$.
    - Remember that we divide by $SE(B)$ to get the t-value. The greater the SE of $B_j$, the lower the t-value will be. The lower the t-value, the higher the p-value.
- So, what influences the VIF? $\frac{1}{1-R_j^2}$
  - $R_j^2$ is the squared multiple correlation coefficient between $X_j$ and all the other predictors (the other $X$ s).
  - The higher $R_j^2$, the higher the VIF.
      - The closer $X_j$ is related to the other predictors, the worse is our estimate $B_j$. This is called the effect of **multicollinearity**.

Finishing the hypothesis test
===================================================================
- Once we have $Var(B)$ and $SE(B) = \sqrt{Var(B)}$, we can calculate t-values for the coefficients as above. Each t-value tests the hypothesis $H_0^{(k)}: \beta_k = 0$
- The degrees of freedom of the t-values are, once again, the denominator of $S_E^2$ (see above): $n-k-1$, where $n$ is the number of observations and $k$ is the number of predictors
- You can also run an F-test. Partitioning the sums of squares works just like in the simple variance example. 
  - $df_{model}$ is the number of predictors $k$, and $df_{error}$ is $n-k-1$.
- This tests the *omnibus* hypothesis that all slopes are 0: $H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0$.
  - This is not the same as testing the individual hypotheses (as we will see!)
- You can also compare the $SS_{model}$ of two different models.

Example
==================================================================
- Let's assume that, apart from each cat's weight in kg, we also have its age in months:

```{r, results='asis', echo = FALSE}
kable(head(catfood_age))
write.csv(catfood_age, file = "catfood_age.csv")
```
- Does adding age to the model improve it?
- This will be our first adventure in SPSS. Feel free to follow along. The data file is in the lecture module on myBU.
- Let's open the file `catfood_age.csv` from myBU in SPSS

The regression output
==================================================================
- Calculating the coefficients in multiple regression gets computationally intense, so let's leave this to SPSS
- First, let's fit the model without the interaction.
- The output below is not from SPSS, but very similar:

```{r, echo = FALSE}
summary((lm_catfood <- lm(formula = FoodEaten ~ CatWeight + CatAge, data = catfood_age)))
```

Interpreting the coefficients
=================================================================
- Let's look at just the coefficients:

```{r, results='asis', echo = FALSE}
kable(summary(lm_catfood)$coefficients)
```

- Looks like both the coefficient for CatWeight and the coefficient for CatAge are significantly different from 0

Now let's add the interaction term
=================================================================
- Note that SPSS is a bit annoying about this, making you calculate the interaction term yourself

```{r, echo = FALSE}
kable(summary((lm_catfood_interaction <- lm(formula = FoodEaten ~ CatWeight * CatAge, data = catfood_age)))$coef)
```
- Now nothing is significant! What is going on?
- Important: these *t*-tests test the null hypothesis that each individual coefficient is 0 **given that all the other predictors are in the model as well**.

Our problem
=================================================================
- This model is really hard to interpret
    - CatWeight or CatAge would have a strong effect individually
    - But neither predictor adds anything if the interaction is already in the model
    - Why? Because the interaction is strongly correlated with both of them!
- Make sure to check for correlations between predictors before running a regression
    - `Part- and partial correlations` in the `Statistics...` menu of the Linear Regression module
- Important: Some multicollinearity is unavoidable, but the more strongly your predictors are correlated, the more problems you get


Diagnosing Multicollinearity
================================================================
- SPSS actually gives you the VIF for each predictor
  - Click on "Statistics..." and check "Collinearity diagnostics"
  - They are in the coefficients table
- For your convenience, the VIFs for this model are printed below

```{r, echo = FALSE}
library(car)
vif(lm(formula = FoodEaten ~ CatWeight * CatAge, data = catfood_age))
```

Interpreting the VIF
=================================================================
- You have a problem with multicollinearity if
    - The largest VIF is greater than 10 and/or
    - The average VIF is substantially greater than 1
- Multicollinearity seriously affects the interpretability of your model
  - In practice, it increases the estimate of the standard error of your coefficients $\sigma_{\beta}$
  - This reduces the power of the significance test

Where does the multicollinearity come from?
==================================================================
- Here's the problem: The interaction effect is equivalent to $CatWeight \cdot CatAge$
- What to do? Luckily, there is an easy solution to this particular issue:
  - If we **center** both variables (i.e. subtract the mean from each observation), the correlation will disappear
  - You can center variables using `Transform` --> `Compute Variable...`
  - Get the **mean** using `Analyze` --> `Descriptives...`, then subtract it from each observation of both variables:
    - $CatWeight - `r mean(catfood_age$CatWeight)`$; $CatAge - `r mean(catfood_age$CatAge)`$
  - Save the new variables as `CatWeight_centered` and `CatAge_centered`

Does this help?
===================================================================
```{r, echo = FALSE}
catfood_age2 <- catfood_age
catfood_age2$CatWeight_centered <- scale(catfood_age$CatWeight, scale = FALSE)
catfood_age2$CatAge_centered <- scale(catfood_age$CatAge, scale = FALSE)
# re-fit the model
lm_catfood_interaction <- lm(formula = FoodEaten ~ CatWeight_centered * CatAge_centered, data = catfood_age2)
lm_catfood_interaction_table <- coef(summary(lm_catfood_interaction))
rownames(lm_catfood_interaction_table) <- c("(Intercept)", "Cat Weight", "Cat Age", "Cat Weight by Cat Age")
kable(lm_catfood_interaction_table)
vif_table <- data.frame(vif(lm_catfood_interaction))
colnames(vif_table) <- "VIF"
rownames(vif_table) <-  c("Cat Weight", "Cat Age", "Cat Weight by Cat Age")
kable(vif_table)
```

- Yes, it does 

Let's look at the coefficients again
===================================================================

```{r, results='asis', echo = FALSE}
kable(summary(lm_catfood_interaction)$coefficients)
```

- Look at that: Now CatAge is significant, too!
- We would have made a Type II error if we hadn't centered the variables.
- Lesson of this story: When testing for interactions with continuous variables, **always center the continuous variables**.

Why you always need to look at your data
======================================================================
- These datasets all have the same regression line, but they look very different:

```{r, echo=FALSE}
library(ggplot2)
library(gridExtra)

data(anscombe)

p1 <- ggplot(anscombe) + geom_point(aes(x1, y1), color = "darkgrey", size = 3) + theme_bw() + scale_x_continuous(breaks = seq(0, 20, 2)) + scale_y_continuous(breaks = seq(0, 12, 2)) + geom_abline(intercept = 3, slope = 0.5) + expand_limits(x = 0, y = 0) + labs(title = "dataset 1")
p2 <- ggplot(anscombe) + geom_point(aes(x2, y2), color = "darkgrey", size = 3) + theme_bw() + scale_x_continuous(breaks = seq(0, 20, 2)) + scale_y_continuous(breaks = seq(0, 12, 2)) + geom_abline(intercept = 3, slope = 0.5) + expand_limits(x = 0, y = 0) + labs(title = "dataset 2")
p3 <- ggplot(anscombe) + geom_point(aes(x3, y3), color = "darkgrey", size = 3) + theme_bw() + scale_x_continuous(breaks = seq(0, 20, 2)) + scale_y_continuous(breaks = seq(0, 12, 2)) + geom_abline(intercept = 3, slope = 0.5) + expand_limits(x = 0, y = 0) + labs(title = "dataset 3")
p4 <- ggplot(anscombe) + geom_point(aes(x4, y4), color = "darkgrey", size = 3) + theme_bw() + scale_x_continuous(breaks = seq(0, 20, 2)) + scale_y_continuous(breaks = seq(0, 12, 2)) + geom_abline(intercept = 3, slope = 0.5) + expand_limits(x = 0, y = 0) + labs(title = "dataset 4")

grid.arrange(grobs = list(p1, p2, p3, p4), main = "Anscombe's Quartet")
```