Advanced Statistics
========================================================
author: Bernhard Angele 
date: Class 2, October 9, 2014

Recap
========================================================
- Last week, we talked a lot about sampling from different probability distribution.
- We also talked about the properties of the distribution of sample means (hint: it's always roughly normal).
- Now what can we do with this knowledge? Remember the convenience function we wrote last time:
```{r}
run_simulation <- function(sample_size = 100, number_of_simulations = 1000, population_mean = 0, population_sd = 1)
  {

sample_means <- replicate(number_of_simulations, mean(rnorm(n = sample_size, mean = population_mean, sd = population_sd)))
}
```

Recap (2)
========================================================
- Remember our function for plotting these distributions:
```{r, echo=TRUE, fig.height = 3}
make_hist_and_plot <- function(sample_means){
  par(mfrow=c(1,2)) # 
  hist(sample_means,freq=F, breaks = 30)
  plot(density(sample_means), main = paste("Mean = ", round(mean(sample_means),2) , "SD = ", round(sd(sample_means),2)))} 
make_hist_and_plot(run_simulation(100,1000,0,1))
```

What changes when we re-run the simulation?
========================================================
```{r}
make_hist_and_plot(run_simulation(100,1000,20,5))
```

What changes when we re-run the simulation?
========================================================
```{r}
make_hist_and_plot(run_simulation(100,1000,20,5))
```

What changes when we re-run the simulation?
========================================================
```{r}
make_hist_and_plot(run_simulation(100,1000,20,5))
```

What changes when we re-run the simulation?
========================================================
It turns out the mean of the distribution of sample means varies around the population mean. The sd also varies, but a lot less. It varies around
$$
\begin{equation} \label{sigmabar}
\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}
\end{equation}
$$
So, to sum up:
The distribution of sample means is (roughly) normal, with $\mu_{\bar{x}} = \mu$ and $\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$.
This means we can apply some of our knowledge about the normal distribution!

Confidence intervals
=========================================================
- If the distribution of sample means is normal, that means we can say something about the relationship between sample mean and population mean.
- Let's say the population mean $\mu$ is `r (pop.mean <- 3)` and the population sd $\sigma$ is `r (pop.sd <- 1)`.
- What is the sample mean going to be?
- Think: what is the answer to this going to look like?
  - $\mu_{\bar{x}}$ is a random variable, so it doesn't make sense to give a point estimate
  - Instead, we can give an interval.
  - Do you remember the function for that?
    - That's right, it's `qnorm`.
    
Confidence intervals (2)
=========================================================
- So, let's get the interval that $\mu_{\bar{x}}$ is going to be in 95% of the time.
- We want something like this:
```{r, echo = F}
cord.x <- c(-3,seq(-1.96,1.96,0.01),1.96)
cord.y <- c(-3,dnorm(seq(-1.96,1.96,0.01)),0)
curve(dnorm(x,0,1),xlim=c(-3,3),main='Standard Normal')
polygon(cord.x,cord.y,col='skyblue')
```

Confidence intervals (3)
=========================================================
- Let's start with the standard normal distribution
- We want to get an interval that includes 95% of the area under the curve
  - That means we need to take off 2.5% on every side
  - For the left interval boundary, we want the x value that is greater than or equal to 2.5% of x values
```{r}
qnorm(.025)
```

Confidence intervals (4)
=========================================================
- For the right interval boundary, we want the x value that is greater than or equal to 97.5% of x values
```{r}
qnorm(.975)
```
- If you've done statistics before, these numbers should be pretty familiar to you.
- Generalising this to other normal distributions is easy:
$\bar{x} = \mu \pm 1.96 \times \sigma_{\bar{x}}$
- Replacing $\sigma_{\bar{x}}$ with the expression based on the population SD:
$\bar{x} = \mu \pm 1.96 \times \frac{\sigma}{\sqrt{n}}$

Now reverse the idea
=========================================================
- Usually, we have no other information about a population but the sample we just collected.
- For example, let's say the sample mean is `r (sample.mean <- 3)` and the sample SD is `r (sample.sd <- 1)`. Apart from this, we know nothing about the population.
- Can we compute a CI for the sample mean?
- Sure enough we can, but it gets a little more complicated.
  - (who would have thought?)

Computing a CI from the sample mean
==========================================================
- We'll have to estimate both the population mean and the population variance.
  - We have already estabished that the sample mean is a good estimator for the population mean.
- What about the sample sd ($s$)? Is it a good estimator for the population sd ($\sigma$)?
- Or the equivalent question: is sample variance ($s^2$) a good estimator of population variance ($\sigma^2$)?
- This sounds like really tricky maths problem.
  - But we can take it easy and just simulate!
  
Set up a function to simulate sampling and calculate sample variances
===========================================================
```{r}
run_variance_simulation <- function(sample_size = 100, number_of_simulations = 1000, population_mean = 0, population_sd = 1)
  {

sample_variances <- replicate(number_of_simulations, var(rnorm(n = sample_size, mean = population_mean, sd = population_sd)))
}
```

Define a new plot function so that the plot titles are correct
===========================================================
```{r}
make_variance_hist_and_plot <- function(sample_variance){
  par(mfrow=c(1,2)) # 
  hist(sample_variance,freq=F, breaks = 30)
  plot(density(sample_variance), main = paste("Mean = ", round(mean(sample_variance),2) , "SD = ", round(sd(sample_variance),2)))} 
```

Population variance and sample variance: plots
===========================================================
```{r}
make_variance_hist_and_plot(run_variance_simulation(100,1000,20,5))
```

Population variance and sample variance: plots
===========================================================
```{r}
make_variance_hist_and_plot(run_variance_simulation(100,1000,20,5))
```

Population variance and sample variance: plots
===========================================================
```{r}
make_variance_hist_and_plot(run_variance_simulation(100,1000,20,5))
```

Sample variance as an estimator of population variance
===========================================================
- Looks like it's a pretty good estimator (unbiased actually)
- We can plug the sd of the sample into the equation for the SD of the sampling distribution (or rather, the standard error):
$$
\begin{equation}
SE_{\bar{x}} = \frac{s}{\sqrt{n}}
\end{equation}
$$
(Note that we are ignoring the question if the relationship between $s$ and $s^2$ is the same as the relationship between $\sigma$ and $\sigma^2$. Feel free to simulate that.)
- But this means that our SE is an estimate of an estimate (estimating $\sigma$ from $s$, then estimating $SE_{\bar{x}}$).
- This means that our estimate for $\sigma$ is going to vary. Its accuracy will depend on the sample size.

Dealing with the uncertainty in s
============================================================
- We need a way to account for $s$ being less accurate at low sample sizes.
- Solution: assume that the sample means aren't normally distributed, but rather *t*-distributed
- Why *t*?
  - The *t*-distribution is like the standard normal distribution, but it has an additional parameter that we call df (for degrees of freedom, but don't worry about that yet).
  - The higher df, the closer the *t*-distribution is to the standard normal distribution
  - For lower df, the *t*-distribution has "heavy tails", meaning that it's wider
    - This reflects greater uncertainty.

See for yourselves
============================================================
Solid = normal distribution, dashed = *t*-distribution
```{r, echo = F}
## plot multiple figures:
## replace ugly par... specification with 
## something easier to remember:
multiplot <- function(row,col){
     par(mfrow=c(row,col),pty="s")
   }

range <- seq(-4,4,.01)  
 
multiplot(2,2)

 for(i in c(2,5,15,20)){
   plot(range,dnorm(range),type="l",lty=1,
        xlab="",ylab="",
        cex.axis=1)
   lines(range,dt(range,df=i),lty=2,lwd=1)
   mtext(paste("df=",i),cex=1.2)
 }
```

Let's try this
==========================================================
- Using the *t*-distribution can compute CIs from samples as follows: get the lower and upper bounds (depending on sample size, e.g. 10):
```{r}
  n <- 10
sample_means <- rnorm(n, mean = 3, sd = 1)
  qt(.025, df = n - 1)
  qt(.975, df = n - 1)
```
Why $n-1$? Unless you really love statistics and want to find out more, don't worry about it.

Computing CIs
==========================================================
- Then take the upper and lower bounds and compute the CIs as follows:
$\bar{x} = \mu_{\bar{x}} \pm 2.262 \times \frac{s}{\sqrt{n}}$
- Remember we estimated $\mu_{\bar{x}}$ using the sample mean

Computing CIs (2)
===========================================================
```{r}
  (sample_mean <- mean(sample_means))
  (sample_sd <- sd(sample_means))
  (lower_bound <- sample_mean + qt(.025, df = n - 1)*(sample_sd/sqrt(n)))
  (upper_bound <- sample_mean + qt(.975, df = n - 1)*(sample_sd/sqrt(n)))
```

There's a function for that
===========================================================
```{r}
t.test(sample_means)
```
How convenient is that?

What does the CI of the sample mean mean? (sorry)
===========================================================
- Remember, we are reversing the idea that the sample mean has a 95% probability to be within the 95% confidence interval around the population mean.
- When we calculate a 95% CI from a *sample* this **DOES NOT MEAN** that there is a 95% probability that the population mean is within this 95% CI.
- Rather, it means that if you take a lot of samples and compute the CI around the sample mean, 95% of those CIs will contain the true population mean.
- In other words, the CI bounds are random variables, but the population mean isn't.
- (In Bayesian statistics, you can actually get something equivalent to the first definition -- a 95% credible interval.)

Let's test this
==========================================================
- Let's get 10 samples from a normal distribution, then get CIs from them and see how often they contain the true mean.
- Write a function for that:
```{r}
test_cis <- function(n, mean = 60, sd = 4){
  t_results <- t.test(rnorm(n, mean, sd))
  mean > t_results$conf.int[1] & mean < t_results$conf.int[2]
}
```

Let's test this (2)
==========================================================
- Now run the tests:
```{r}
mean_in_ci <- replicate(1000, test_cis(10, 60, 4))
table(mean_in_ci)
```
- It's true! Almost exactly 5%

What happens if we don't use the t distribution?
==========================================================
```{r}
test_cis_norm <- function(n, mean = 60, sd = 4){
  samples <- rnorm(n, mean, sd)
  upper <- mean(samples) + 1.96*(sd(samples)/sqrt(n))
  lower <- mean(samples) - 1.96*(sd(samples)/sqrt(n))
  mean > lower & mean < upper
}
```
What happens if we don't use the t distribution? (2)
==========================================================
```{r}
mean_in_ci <- replicate(1000, test_cis_norm(10, 60, 4))
table(mean_in_ci)
```
- Larger than 5%! This is because the normal distribution is narrower than the t-distribution at low dfs.

What if we use a larger sample size?
==========================================================
```{r}
mean_in_ci <- replicate(1000, test_cis_norm(100, 60, 4))
table(mean_in_ci)
```
- Back at 5%! For large sample sizes it's fine to use the normal distribution instead of the t-distribution (of course, the t-distribution works anyway).
- Could you have come up with this? You didn't have to thanks to the work William Sealy Gosset did back in 1908.

Random variables
==========================================================
Can we follow Shravan here? Warning: Some mathematical notation follows.

- A random variable $X$ is a function $X : S \rightarrow \mathbb{R}$ that associates to each outcome
$\omega \in S$ exactly one number $X(\omega) = x$.
  - Note: $\mathbb{R}$ = real numbers

- $S_X$ is all the $x$'s (all the possible values of X, the support of X). i.e., $x \in S_X$.

- Good example: number of coin tosses till H

  - $X: \omega \rightarrow x$
	- $\omega$: H, TH, TTH,$\dots$ (infinite)
	- $x=0,1,2,\dots; x \in S_X$
  
Random variables (2)
==========================================================
Every discrete random variable X has associated with it a  **probability mass/density function (PDF)**, also called *distribution function*.
$$
\begin{equation}
p_X : S_X \rightarrow [0, 1] 
\end{equation}
$$
defined by
$$
\begin{equation}
p_X(x) = P(X(\omega) = x), x \in S_X
 \end{equation}
$$
- Back to the example: number of coin tosses till H

  - $X: \omega \rightarrow x$
  - $\omega$: H, TH, TTH,$\dots$ (infinite)
	- $x=0,1,2,\dots; x \in S_X$
  - $p_X = .5, .25, .125,\dots$ 
  
Hypothesis tests
=========================================================
- In a way, by calculating the CI we already have a way to test hypotheses
- Let's say we got a 95% CI from our sample with a lower bound of 2 and an upper bound of 3.
- Let's use the simplest null hypothesis possible
  - Null hypothesis: the mean of the population that the sample came from is 0
  - $H_0: \mu = 0$
  - Given the 95% CI above, can we reject the null hypothesis?
    - And if so, what is the chance that we're wrong?
  - Answer: Yes, we can, since 0 is not part of the CI.
    - There is the possibility that we are wrong, though, since only 95% of the CIs will contain the true population mean.
    - This is called the $\alpha$-error, and its probability here is (at most) 5%.
    