Advanced Statistics
========================================================
author: Bernhard Angele 
date: Lecture 2


Maths basics: Probability
========================================================
- Basic rules:
    - All probabilities are between 0 and 1: $P(A)\in[0,1]$
    - The complementary probability of an event (i.e. the probability that an event will NOT happen) is 1-the probability of the event: $P(A^c)=1-P(A)$
    - A probability can be interpreted as the number of outcomes that form an event (e.g. the outcome "Heads" when flipping a coin) over the total number of outcomes (e.g. "Heads" and "Tails")
    $$ p(A) = \frac{n_A}{n}$$
    - But note that a probability of .5 (e.g. for getting "Heads" on a coin flip) doesn't mean that you will get "Heads" on exactly 50% of coin flips.
    
Maths basics: Probability (2)
========================================================
- Basic rules:
    - What is the probability that Event A and Event B will happen together?
$$\begin{aligned}
P(A\cap B) & = P(A|B)P(B) = P(B|A)P(A)\\
P(A\cap B) &  = P(A)P(B) \qquad\\&\mbox{if A and B are independent}\\
\end{aligned}$$

- What is the probability that either Event A OR Event B will happen?
$$\begin{aligned}
P(A\cup B) & = P(A)+P(B)-P(A\cap B) \\
P(A\cup B) & = P(A)+P(B) \\\qquad&\mbox{if A and B are mutually exclusive} \\
\end{aligned}$$

Maths basics: Conditional probability
==========================================================
- What is the probability of Event A *GIVEN THAT* Event B happened?
    - Divide the number of outcomes where A and B happen together by the number of all outcomes where B happens (regardless of whether A happened, too).
    - If we divide both nominator and denominator by $n$, we can convert this into probabilities:
$$P(A \mid B) = \frac{n_{AB}}{n_B} = \frac{n_{AB}/n}{n_B/n} = \frac{P(A \cap B)}{P(B)}$$
- Now plug in our definition of joint probability (see last slide):
$$P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B|A)P(A)}{P(B)}$$
- This is known as *Bayes' theorem*. Keep it in mind for later!

Back to our dice problem
==========================================================
- IN THEORY, each of our dice roll outcomes has the same probability:
$$ 
\begin{aligned}
p(1) = \frac{n_1}{n_{total}} &= \frac{1}{6},
p(2) = \frac{n_2}{n_{total}} &= \frac{1}{6}\\
p(3) = \frac{n_3}{n_{total}} &= \frac{1}{6},
p(4) = \frac{n_4}{n_{total}} &= \frac{1}{6}\\ 
p(5) = \frac{n_5}{n_{total}} &= \frac{1}{6},
p(6) = \frac{n_6}{n_{total}} &= \frac{1}{6}
\end{aligned}
$$
- But how can we test whether that is actually true?
- We need some way to compare the data to the theoretical probability distribution

Aggregating our dice results
==========================================================
- We obviously need to take more than one dice roll into account. But how can we aggregate all our results in a convenient number?
- As luck would have it, descriptive statistics provides us with a number of standard measures to characterise the properties of a *sample* (e.g. rolling the dice 10 times):
- Measures of *central tendency*:
  - The mean: $\bar{x}=\frac{\sum_{i=1}^{n} x_i}{n}$
  - The median: The number separating the higher half of a sample from the lower half
  - The mode: The most frequent observation
- Measures of dispersion:
  - The variance: $s_x^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2$

Statistics basics: Random variables
==========================================================
- We need to consider our sample as a random variable
- What is a random variable?
  - A random variable is a function that assigns a number to each possible outcome of our experiment (the dice roll)
- The outcome of a single dice roll can be described by a very obvious function: just assign the numbers from 1 to 6
- The outcome of *multiple* dice rolls is  little trickier
- Regardless of which one we choose, we can then come up with a *theoretical* probability distribution for the random variable.
- The opposite of a random variable is a *constant*, a value that's the same for every sample.

Statistics basics: Random variables (formal definition!)
==========================================================
- Warning: Some mathematical notation follows.
- A random variable $X$ is a function $X : O \rightarrow \mathbb{R}$ that associates to each outcome
$\omega \in O$ exactly one number $X(\omega) = x$.
  - Note: $\mathbb{R}$ = real numbers
- $O_X$ is all the $x$'s (all the possible values of X, the support of X). i.e., $x \in O_X$.
- Good example: number of coin tosses till you get Heads (H) for the first time

  - $X: \omega \rightarrow x$
	- $\omega$: H, TH, TTH,$\dots$ (infinite)
	- $x=0,1,2,\dots; x \in O_X$
  
Random variables (2)
==========================================================
Every discrete random variable X has associated with it a  **probability mass function**, also called *distribution function*.
$$
p_X : S_X \rightarrow [0, 1] 
$$
defined by
$$
p_X(x) = P(X(\omega) = x), x \in S_X
$$
- Back to the example: number of coin tosses till H

  - $X: \omega \rightarrow x$
  - $\omega$: H, TH, TTH,$\dots$ (infinite)
	- $x=0,1,2,\dots; x \in S_X$
  - $p_X = .5, .25, .125,\dots$ 
  

What is a probability distribution?
========================================================
From Wikipedia:
In probability and statistics, a probability distribution assigns a probability to each measurable subset of the possible outcomes of a random experiment, survey, or procedure of statistical inference.
Here's an example of a discrete probability distribution, the distribution of the *sum of two dice rolls*:

![Dice Distribution (from Wikipedia)](https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Dice_Distribution_%28bar%29.svg/512px-Dice_Distribution_%28bar%29.svg.png)

Discrete probability distribution
========================================================
![Dice Distribution (from Wikipedia)](https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Dice_Distribution_%28bar%29.svg/512px-Dice_Distribution_%28bar%29.svg.png)

Every possible outcome (sum of the numbers rolled on two dice) is assigned a corresponding probability. This is called a *probability mass function*.

Important: all values sum to 1.

A quick clarification: Sample and population
==========================================================
- With the introduction of *theoretical* probability distributions, we need to be very careful to not confuse properties of the theoretical distribution with properties of an individual sample.
- Standard practice is to use 
    - roman letters (e.g. $m$ or $\bar{x}$ for the mean, $s$ for the standard deviation) for properties of the sample
    - greek letters (e.g. $\mu$ "mu" for the mean and $\sigma$ "sigma" for the standard deviation) for properties of the distribution (or the population that is represented by the distribution).

From discrete to continuous
=======================================================
- Let's look at the probability distributions we get from rolling one, two, three etc. dice and summing up the results.
- We'll start with rolling one die (note that the bars may be a tiny bit uneven since I used a simulation to produce this graph).

```{r, echo=F}
plot_freq_hist <- function(vec, breaks = "Sturges", title){
  h <- hist(vec, plot=FALSE, breaks=breaks, right = FALSE)
  h$counts=h$counts/sum(h$counts)
  plot(h, main = title, xlab = "X", ylab = "P(X)")
}

plot_discrete_hist <- function(vec, title){
  counts <- table(vec)
  barplot(counts/sum(counts), main = title, xlab = "X", ylab = "P(X)")
}

plot_discrete_hist(sample(1:6, 1000000, replace = TRUE), title = "One die")

```

Two dice
=======================================================
- This is the same as the image from Wikipedia.

```{r, echo=F}

plot_discrete_hist(sample(1:6, 100000, replace = TRUE)+sample(1:6,100000, replace = TRUE), title = "Two dice")

```

Three dice
=======================================================
```{r, echo=F}

plot_discrete_hist(sample(1:6, 100000, replace = TRUE)+sample(1:6,100000, replace = TRUE)+sample(1:6, 100000, replace = TRUE), title = "Three dice")

```

Four dice
=======================================================
```{r, echo=F}

plot_discrete_hist(sample(1:6, 100000, replace = TRUE)+sample(1:6,100000, replace = TRUE)+sample(1:6, 100000, replace = TRUE)+sample(1:6, 100000, replace = TRUE),title = "Four dice")

```

Ten dice
=======================================================
```{r, echo=F}

roll_dice <- function(number_of_samples, number_of_dice){
  result <- sample(1:6, number_of_samples, replace = TRUE)
  for(i in 1:(number_of_dice-1)){
    result<-result + sample(1:6, number_of_samples, replace = TRUE)
  }
  result
}

plot_discrete_hist(roll_dice(100000, 10),title = "Ten dice")

```

100 dice
=======================================================
```{r, echo=F}

plot_freq_hist(roll_dice(100000, 100), 150, title = "100 dice")

```

Do you see a pattern?
========================================================
- Central limit theorem (CLT)

> When sampling from a population that has a mean, provided the sample size is large
> enough, the sampling distribution of the sample mean will be close to normal regardless
> of the shape of the population distribution

- (Technically, we were sampling the sum of X rather than the mean, but the mean of X is simply the sum divided by the number of observations. Do you care about this distintion? Didn't think so. It makes me feel better, though.)

What does this mean?
========================================================
- For our dice problem, it means that we can compute the means of our samples (e.g. the mean of the 5, or 10, or 100 samples) 
- Remember, the *sample mean* is a random variable as well, since it is different every time we take a sample
- We can then use a *continuous* probability distribution -- the **normal distribution** as the theoretical probability distribution for our random variable (i.e. the sample mean).
- This makes our life easy, because the normal distribution is very simple to handle mathematically (really!).


Continuous probability distributions
=======================================================
```{r, echo=F}
plot(function(x) dnorm(x), -3, 3,
main = "Normal density",ylim=c(0,.4),
ylab="density",xlab="X")
```

Here, the outcomes are continuous, so it doesn't make sense to ask about the probability of any point on the x-axis.
- What is the probability of x = 1? 
 - What do you mean by "1"? The function is continuous, so does 1.00001 still qualify as 1?
- It makes more sense to ask these questions about intervals. The probability is then the area under the curve for the interval.
- Important: the total area under the curve is 1.

Normal probability density function (PDF)
========================================================
$$
  f(x,\mu,\sigma) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-((x - \mu)^2/2 \sigma^2)}
$$

```{r, echo=F}
plot(function(x) dnorm(x), -3, 3,
main = "Normal density",ylim=c(0,.4),
ylab="density",xlab="X")
```

Normal probability density function (PDF)
========================================================
$$
  f(x,\mu,\sigma) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-((x - \mu)^2/2 \sigma^2)}
$$
- This looks scary, but it really isn't. This is simply a mathematical function that happens to describe the distribution of a lot of random variables in nature.
    - If you look closely, you can see that the function has three parameters, $x, \mu,$ and $\sigma$ ($\pi$ and $e$ are constants).
      - The first parameter, $x$ is the random variable. The function gives you the probability density at each value of $x$
      - The second parameter, $\mu$, is called the **expected value** or the **mean** of the distribution.
      - The third parameter, $\sigma$, is called the standard deviation.

Standard normal distribution
=======================================================
- There is an infinite number of normal distributions with different parameters $\mu$ and $\sigma$. The one with $\mu = 0$ and $\sigma = 1$ is particularly useful and is called the *standard* normal distribution.
- Look at how simple and nice the normal distribution appears when we plug in those values:
$$
  f(z,\mu,\sigma) = \frac{1}{\sqrt{2 \pi}} e^{-z^2/2}
$$
- You can *transform* values from any normal distribution to the normal distribution.
- This is known as a *z-transformation*: $z = \frac{x - \mu}{\sigma}$
- By transforming all our observations to z-values and then looking up their probability in the standard normal distribution, this is the only distribution we'll ever need (...mostly).

The probability of outcomes in the standard normal distribution
=======================================================
- Remember, we can't really get the probability of a *point* event in a continuous distribution, since there are no "points" in a continuous variable
- But we can ask questions about *intervals*:
- What's the probability of x being between 0 and 1?

```{r, echo = F}
cord.x <- c(0,seq(0,1,0.01),1)
cord.y <- c(0,dnorm(seq(0,1,0.01)),0)
curve(dnorm(x,0,1),xlim=c(-3,3),main='Standard Normal')
polygon(cord.x,cord.y,col='skyblue')
```

Getting the area under the curve
=======================================================
- Since we know exactly what the function is, we can get the area under the curve.
  - Remember how to do that from maths class? Your best friend, integration :
$$ p (0 < z < 1) = \int\limits_{0}^{1} \frac{1}{\sqrt{2 \pi}} e^{-z^2/2} dz$$
- Don't want to do integration? Well, you're in luck, because most statistical software (and Excel!) can do this for you.
- `=NORM.S.DIST(0,TRUE)` will give you the area under the curve to the left of 0 (i.e. the probability that $z < 0$)
- `=NORM.S.DIST(1,TRUE)` will give you the area under the curve to the left of 1 (i.e. the probability that $z < 1$)

Getting the area under the curve
=======================================================
- Remember, Excel gives us the *lower* tail (the area under the curve to the *left* of the $z$ value)
  - So we can calculate our interval: $p (0 < z < 1) = p(z < 1) - p(z < 0)$
  - In Excel: 
      - `=NORM.S.DIST(1,TRUE)-NORM.S.DIST(0, TRUE)`
      - Result: `r pnorm(1)-pnorm(0)`
- Success!

Things you can do with this knowledge
========================================================
- Say I'm looking at random numbers from a standard normal distribution, and I see that one of them is 4.
  - That seems very unusual
  - Just how unusual?
    - What's the probability of getting a value of 4 when sampling from a standard normal distribution (mean = 0, sd = 1)?  

Just how unusual is a value of 4?
========================================================
- Remember, when you have a continuous distribution, you can't think about point values (e.g. 5). Rather, what you want to know is:
    - What is the probability of getting a value of 4 *or greater* (or $p(z > 4) = 1-p(z < 4)$)?
- Let's ask Excel: `=1-NORM.S.DIST(4,TRUE)`
    - Result: `r format(dnorm(4), scipen = 8)`
- So it's very unusual.
- Can we come up with a similar test for our dice sample mean?
- We'll have to figure out how the dice sample means are distributed.
    - Then we can take our sample mean and see how likely (or unlikely) it is that it comes from the theoretical distribution.

The theoretical distribution of dice sample means
========================================================
- We've seen that we can approximate our theoretical distribution (which is actually discrete) using a continuous distribution function, namely the normal distribution, which makes our lives very easy (yes, really!).
- We have to figure out the $\mu$ and the $sigma$ parameters for our theoretical normal distribution of sample means, though.
- Note for those of you who care (probably no one): In the case that we actually know exactly what the probabilities for our discrete probability distribution should look like, we could also use a different distribution, the $\chi^2$ (chi square) distibution. More about that next week, otherwise our heads may explode.

Random variables: Expected value
========================================================
- Random variables have expected values
- For discrete random variables, the expected value is the outcome value multiplied by the probability of the outcome:
$$ E(X) = \mu = \sum\limits_{i=1}^k p(x_i)\cdot x_i$$
  - where $E(X)$ is the expected value of a discrete random variable $X$ with the outcomes $(x_1 \dots x_k)$ and the associated probabilities $(p(x_1)\dots p(x_k))$
- The equivalent for continuous random variables:
$$\mu = \int\limits_{-\infty}^{\infty}x f(x) dx$$

Random variables: Variance
========================================================
- Random variables also have variances
- For discrete random variables, the variance is the difference between the outcome value and the mean multiplied by the probability of the outcome:
$$ \sigma^2 = \sum\limits_{i=1}^k p(x_i)\cdot (x_i - \mu)^2$$
  - where $\sigma^2$ is the variance of a discrete random variable $X$ with the outcomes $(x_1 \dots x_k)$ and the associated probabilities $(p(x_1)\dots p(x_k))$
- The equivalent for continuous random variables:
$$\mu = \int\limits_{-\infty}^{\infty}(x-\mu)^2 f(x) dx$$


Maths basics: Expected values
========================================================
- For example, the expected value $\mu$ of rolling a six-sided die is:
$$
\begin{aligned}
E(X) &= \sum\limits_{i=1}^{6} p(x_i) \cdot x_i \\
    &= x_1 \cdot p(x_1) + x_2 \cdot p(x_2) + x_3 \cdot p(x_3) + x_4 \cdot p(x_4) \\ 
    &+ x_5 \cdot p(x_5) + x_6 \cdot p(x_6) \\
    &= 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} \\ 
    &+ 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6} \\
    &= \frac{21}{6} = 3.5
\end{aligned}
$$

Maths basics: Variance
========================================================
- For example, the variance $\sigma^2$ of rolling a six-sided die is:
$$
\begin{aligned}
\sigma^2 &= \sum\limits_{i=1}^6 p(x_i)\cdot (x_i - \mu)^2 \\
    &= (x_1-\mu)^2 \cdot p(x_1) + (x_2-\mu)^2 \cdot p(x_2) + (x_3-\mu)^2 \cdot p(x_3) \\
    &+ (x_4-\mu)^2 \cdot p(x_4) + (x_5-\mu)^2 \cdot p(x_5) + (x_6-\mu)^2 \cdot p(x_6) \\
    &= \frac{1}{6}\cdot \Big((1-3.5)^2 + (2-3.5)^2 + (3-3.5)^2 \\
    &+ (4-3.5)^2 + (5-3.5)^2+ (6-3.5)^2\Big) \\
    &= \frac{17.5}{6} = 2.9167
\end{aligned}
$$

Back to the dice example again
=======================================================
- So, we know that, if our dice are fair, our dice rolls come from a discrete theoretical distribution with $\mu = 3.5$ and $\sigma^2 = 2.9167$. 
- But remember, we don't want to evaluate single dice rolls, but rather the mean of a sample of dice rolls, since that will enable us to use the nice and easy normal distribution to calculate the probabilities.
- So, what is the mean $\mu_{\bar{x}}$ and what is the variance $\sigma_{\bar{x}^2}$ for the **distribution of sample means**?


Maths basics: Expected values (3)
========================================================
- What is the expected value of rolling two dice and adding the spots?
- What is the expected value of rolling two dice and multiplying the number of spots?
- What is the expected value of an IQ test result?
- Imagine you and your friend both take IQ tests. What is the expected value of the differences between your scores (assuming that you both come from the general population)?
    - Don't know? Well, stay tuned. This will require some maths, though.


Maths basics: Computing expected values
========================================================
- The expected value of a random variable is often also called $\mu$:
$$E(X) = \mu$$
  - $\mu$ is also called the distribution *mean*
- What if the expected value is constant across all the possible outcomes?
  - e.g what is the expected value of a die with 1 on all sides? 
      - 1, of course!
- More general:
  - if the value is the same across all outcomes, we can call it a constant
  - e.g. if $x_1 = x_2 = x_3 = \dots = x_i = 1$
      - then $E(X) = E(1) = 1$

Maths basics: Computing expected values (2)
=======================================================
Even more general: If a is a constant, then $E(a) = a$
- If $X$ is a random variable and $a$ is a constant, what is the expected value of $a \cdot X$?
$$E(a\cdot X) = a \cdot E(X)$$
- For example, if the expected value of rolling a 6-sided die is 3.5, what is the expected value of rolling a 6-sided die and then multiplying the number of spots by 3?
$$ E(3\cdot X) = 3\cdot E(X) = 3\cdot 3.5 = 10.5$$
- Try it if you don't believe me!

Maths basics: Computing expected values (3)
=======================================================
- If $X$ is a random variable and $a$ is a constant, what is the expected value of $a + X$?
$$E(a + X) = a + E(X)$$
- For example, if the expected value of rolling a 6-sided die is 3.5, what is the expected value of rolling a 6-sided die and then adding 3 to the number of spots?
$$ E(3 + X) = 3 + E(X) = 3 + 3.5 = 6.5$$
- Try it if you still don't believe me!

Maths basics: Computing expected values (4)
=======================================================
- If $X$ is a random variable and $Y$ is a random variable, what is the expected value of $X + Y$?
$$E(X + Y) = E(X) + E(Y)$$
- For example, if the expected value of rolling two 6-sided dice and adding the two results?
$$ E(X + Y) = E(X) + E(Y)= 3.5 + 3.5 = 7$$
- Try it if you still don't believe me!

Maths basics: Computing expected values (5)
=======================================================
- If $X$ is a random variable and $Y$ is a random variable, what is the expected value of $X \cdot Y$?
$$E(X \cdot Y) = E(X) \cdot E(Y)$$
- But *ONLY* if $X$ and $Y$ are *INDEPENDENT*
- For example, if the expected value of rolling two 6-sided dice and multiplying the two results?
$$ E(X + Y) = E(X) \cdot E(Y)= 3.5 \cdot 3.5 = 12.25$$
- Try it if you still don't believe me!

The expected value of the sample mean
========================================================
- If $X$ is a random variable, and we take a sample of size $n$ from $X$, what is the expected value of the mean of that sample?
- Remember, this is how you compute the sample mean: 
$$\bar{x} = \frac{\sum\limits_{i=1}^{n}}{n}$$
- The expected value of the sample mean is:
$$
\begin{aligned}
E(\bar{X}) &= E\Bigg(\frac{\sum\limits_{i=1}^{n}}{n}\Bigg)\\
           &= \frac{1}{n}\cdot\big(E\sum\limits_{i=1}^n{X_i}\big)
\end{aligned}$$

The expected value of the sample mean (2)
========================================================
$$
\begin{aligned}
           &= \frac{1}{n}\cdot\big(E\sum\limits_{i=1}^n{X_i}\big) \text{ (since } E(a\cdot X) = a \cdot E(X)\text{)} \\
           &= \frac{1}{n}\cdot\sum\limits_{i=1}^n{E(X_i)} \text{ (since } E(X+Y) = E(X) + E(Y)\text{)} \\
           &= \frac{1}{n}\cdot\sum\limits_{i=1}^n{\mu_x} \text{ (since } E(X) = \mu \text{)} \\
           &= \frac{1}{n}\cdot n \cdot\mu_x = \mu_x
\end{aligned}$$


The expected value of the sample mean (3)
========================================================
- We just found that the expected value of the sample mean $E(\bar{X})$ is identical to the expected value (the mean) of the population $\mu_x$, *regardless of the sample size*.
- We can say that the sample mean $\overline{X}$ is an *unbiased estimator* of the population mean $\mu$
- No matter what we do and what crazy population we're taking samples of, the sample means will always be distributed around the true population mean.
    - *Isn't that cool?*
    - I know what you're thinking right now, but this is *actually* cool. Just think about it: If you want to know the true population mean of any population, all you have to do is take enough samples.
    
Our dice example
========================================================
- Remember, we want to know how the means of our dice rolls should be distributed if the dice are fair. So, now we know that they are normally distributed (because of the Central Limit Theorem) with an expected value of $\mu_{\bar{x}} = 3.5$.
- What about the *variance* of the *distribution of sample means*?
    - Well, this will take some work. Sorry, people. It's maths time.
    - First, we need to know how the sample variance is related to the population variance.
    - In other words, we need to know the expected value of the sample variance.

The expected value of the sample variance
========================================================
- We could now go through the same process for the sample variance $s^2$, but this would take a lot of time and most of you may not care about the details.
- Instead, I will just tell you what we would find if we did this:
- It turns out that the sample variance $$s^2 = \frac{\sum\limits_{i=1}^{n}(x_i - \mu)^2}{n}$$ is **not** an unbiased estimator of the population variance. In fact, it *underestimates* the population variance.
- I have posted a link to [a video](https://www.youtube.com/watch?v=D1hgiAla3KI) that very nicely and intuitively demonstrates this point on myBU in case you don't believe me.

The expected value of the sample variance (2)
========================================================
- We can correct our sample variance if we divide by $n-1$ instead of $n$:
$$s^2 = \frac{\sum\limits_{i=1}^{n}(x_i - \mu)^2}{n-1}$$
- This definition of the sample variance *does* give us an unbiased estimator of the population variance $\sigma^2$.
- I have posted a link to [another video](https://www.youtube.com/watch?v=9ONRMymR2Eg) that gives you the proof of this in case you don't believe me. 

The expected value of the variance of sample means
========================================================
- Remember, now we are no longer talking about the population variance $\sigma^2$ now, but the **variance of the sample means** $\sigma_{\bar{x}}^2$. 
- Also remember that the population does not have to be distributed normally, but the means of samples from this distribution will be if the sample size is large enough (that's the whole reason we're concerning ourselves with the means rather than individual samples here).
- We'll start by rewriting $E(\sigma_{\bar{x}}^2)$ as $Var(\bar{X})$, that is, the expected variance of the sample mean. Same thing, but makes the following steps more obvious.

The expected value of the variance of sample means (2)
========================================================
- Now we replace $\bar{X}$ with the definition of the sample mean:
$$
\begin{aligned}
Var(\bar{X}) &= Var(\frac{X_1 + X_2 + \dots + X_n}{n})\\
&= \frac{1}{n^2}\cdot Var(X_1 + X_2 + \dots + X_n)
\end{aligned}
$$
- What have we done now? We just moved the constant out of the expected variance
    - Tiny not-that-intuitive detail: The variance is a square ($s^2$), so as we move $1/n$ out of the term, we need to square it too.
    
The expected value of the variance of sample means (3)
========================================================
- We can treat expected variances just like expected values. 
    - If (and only if!) $X_1, X_2,\dots, X_n$ are independent (i.e. the value of $X_1$ doesn't depend on the value of $X_2$, or $X_3$, etc.), we can rewrite the expected variance of a sum as the sum of its expected variances 
$$
\begin{aligned}
Var(\bar{X}) &= \frac{1}{n^2}\cdot Var(X_1 + X_2 + \dots + X_n)\\
             &=\frac{1}{n^2}\cdot (Var(X_1) + Var(X_2) + \dots + Var(X_n))
\end{aligned}
$$  

The expected value of the variance of sample means (4)
========================================================
- We just established that the expected value of the variance of each individual sample (if we apply the correction and divide by $n-1$) is the population variance $\sigma^2$. So let's replace each term $Var(X_i)$ with $\sigma^2$:
$$
\begin{aligned}
Var(\bar{X}) &= \frac{1}{n^2}\cdot(Var(X_1) + Var(X_2) + \dots + Var(X_n)) \\
             &= \frac{1}{n^2}\cdot(\sigma^2 + \sigma^2 + \dots + \sigma^2) \\
             &= \frac{1}{n^2}\cdot n \cdot \sigma^2 \\
             &= \frac{\sigma^2}{n}
\end{aligned}
$$
- So, the expected value of the variance of sample means $\sigma_{\bar{x}}^2$ is $\frac{\sigma^2}{n}$ and the expected value of the standard deviation of sample means (also called **standard error**) is $\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$.

Please, let's FINALLY finish the dice example
================================================================
- OK, OK. We now have everything we need to determine whether our dice roll sample mean is unusual assuming fair dice.
- More formally, we call this a **Hypothesis Test**
- We establish a *Null Hypothesis* $H_0$ (e.g. the dice are fair), 
    - determine a theoretical probability distribution of the random variable (our dice roll means) given that the $H_0$ is true:
        - a normal distribution with $\mu_{\bar{x}} = 3.5$ and $\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}} = \frac{\sqrt{2.9167}}{\sqrt{n}}$, where n is the number of dice rolls in our sample,
    - and finally we can calculate the probability that you would observe the sample mean you observed given the $H_0$.
    
Final steps
=================================================================
- So, let's assume you did 10 dice rolls for this example, and that your mean was 4.
- Since we know that the sample means should be normally distributed, we can transform your mean into a *z*-value: 
- Since $\mu_{\bar{x}} = 3.5$ and $\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}} = \frac{\sqrt{2.9167}}{\sqrt{10}} = `r sqrt(2.9167)/sqrt(10)`$:
$$ z(4) = \frac{4-3.5}{`r sqrt(2.9167)/sqrt(10)`}=`r (4-3.5)/(sqrt(2.9167)/sqrt(10))`$$
- Let's ask Excel what the probability of observing a sample mean this far away (or farther) from the population mean is for the standard normal distribution: `1-NORM.S.DIST(0.9258,TRUE)`
    - Result: 0.177274964
    
Final steps (2)
=================================================================
- Fisher (who popularised this sort of hypothesis testing) suggested that we should consider data with a probability of less than 5% (or .05) given the null hypothesis as **significant** evidence for rejecting the null hypothesis.
- In our case, we are far away from a probability (or short, *p*-value) of .05. So, we can't reject the null hypothesis. Try it for yourselves, though.
- More on this next week.

Technical note for those who really care
=================================================================
- We really don't care about the direction of the effect here, just the absolute distance from the mean (i.e. this is a *two-tailed* test).
- So, to be absolutely correct, we should ask Excel to give us the probability of z being at least this far away from the mean on either side: $p(z < -.9258 \cup z > .9258) = p(z < -.9258) + (1-p(z < .9258))$. 
- When we ask Excel for the p-value `=NORM.S.DIST(-0.9258,TRUE)+(1-NORM.S.DIST(0.9258,TRUE))` we get the actual, correct result of $`r pnorm(-.9258)+(1-pnorm(.9258))`$.
- Good times. Until next week!


What have we learned last time?
========================================================
- We figured out that, when we're taking random samples from *any* distribution (with a mean and a variance), the means (and sums) of these samples will approximately follow a **normal distribution** if the sample size is large enough (in general, $n \geq 30$ is a good rule of thumb).
- We even determined what the mean of this sampling distribution of the mean will be (namely, it will be the same as the mean of the population we're sampling from):
$$\mu_{\bar{x}} = \mu$$ 
- We also determined what the variance of the sampling distribution will be. It will be the variance of the population we're sampling from divided by the sample size:
$$\sigma_{\bar{x}}^2 = \frac{\sigma^2}{n}$$


How can we use this?
=======================================================
- We can do hypothesis testing with normal distributions:
  - Let's say we're forensic psychologists trying to screen prisoners for signs of psychopathy.
  - Let's say that we have a test that's normed for a standard (prison) population. Thanks to the norming, we know that this standard population has a mean psychopathy score of 50 and a standard deviation of 10. The psychopathy scores (the scores themselves, not just their means) are approximately normally distributed.
  - You see a prisoner with a score of 72. Is this an unusually high score? Should you be concerned?
  
What do you do now?
=======================================================
- Establish null and alternative hypotheses:
  - Null hypothesis ($H_0$): the prisoner comes from the standard prison population ($E(x) = \mu_{\bar{x}} = \mu$).
  - Alternative hypothesis ($H_A$): the prisoner's score is higher than that of the standard prison population ($E(x) > \mu$)
- Convert the score into a *z*-value:
$$ z(72) = \frac{72-50}{10} = 2.2$$
- What is the probability of getting a *z*-value of 2.2 given that the null hypothesis is true?
    - Check the standard normal distribution.

Make a plot
========================================================
- Always a good idea! A quick sketch is all it takes.

```{r, echo = F}
cord.x <- c(2.2,seq(2.2,3,0.01),3)
cord.y <- c(0,dnorm(seq(2.2,3,0.01)),0)
curve(dnorm(x,0,1),xlim=c(-3,3),main='Standard Normal')
polygon(cord.x,cord.y,col='skyblue')
```

Get the p-value
==========================================================
- Get Excel (or another software) to give you the area under the curve.
- $p(z > 2.2) = 1 - p(z < 2.2)$, so `=1-NORM.S.DIST(2.2,TRUE)`
- Result: `r 1-pnorm(2.2)`
- The prisoner is in the extreme 5% of the distribution.
- Maybe you should be concerned?


Use the EasyStats excel spreadsheet to run many simulations
========================================================
- This is a more intuitive way to do the same thing we did analytically last time.
- Observe:
    - What changes on each run?
    - What stays the same?

What changes when we re-run the simulation?
========================================================
```{r, echo = FALSE}
run_one_sample <- function(sample_size = 100, population_mean = 0, population_sd = 1)
{
  sample_means <- rnorm(n = sample_size, mean = population_mean, sd = population_sd) 
  data.frame(mean = mean(sample_means), variance = var(sample_means))
}

run_simulation <- function(sample_size = 100, 
                           number_of_simulations = 1000, 
                           population_mean = 0, 
                           population_sd = 1)
  {
require(data.table)
rbindlist(replicate(number_of_simulations, 
  list(run_one_sample(sample_size = sample_size, 
  population_mean = population_mean, 
  population_sd = population_sd))))

}
make_hist_and_plot <- function(sample_means){
  par(mfrow=c(2,2)) # 
  hist(sample_means$mean,freq=F, breaks = 30, main = "Sample Mean")
  plot(density(sample_means$mean), 
       main = paste("Mean = ", round(mean(sample_means$mean),2) , 
                    "SD = ", round(sd(sample_means$mean),2)))
  hist(sample_means$variance,freq=F, breaks = 30, main = 'Sample variance')
  plot(density(sample_means$variance), 
       main = paste("Mean = ", round(mean(sample_means$variance),2) , 
                    "SD = ", round(sd(sample_means$variance),2)))} 
make_hist_and_plot(
  run_simulation(10,1000,20,5))
```

What changes when we re-run the simulation?
========================================================
```{r, echo = FALSE}
make_hist_and_plot(
  run_simulation(10,1000,20,5))
```

What changes when we re-run the simulation?
========================================================
It turns out the mean of the distribution of sample means varies around the population mean. The sd also varies, but a lot less. It varies around
$$
\begin{aligned} \label{sigmabar}
\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}
\end{aligned}
$$
So, to sum up:
The distribution of sample means is (roughly) normal, with $\mu_{\bar{x}} = \mu$ and $\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$.
This means we can apply our knowledge about the normal distribution to find out the theoretical probability of our result given the $H_0$ (*p*-values).

Confidence intervals
=========================================================
- A different way of using the normal distribution to express our null hypotheses
- If the distribution of sample means is normal, that means we can say something about the relationship between sample mean and population mean.
- Let's say the population mean $\mu$ is `r (pop.mean <- 0)` and the population sd $\sigma$ is `r (pop.sd <- 1)`.
- What is the sample mean going to be?
- Think: what is the answer to this going to look like?
  - $\mu_{\bar{x}}$ is a random variable, so it doesn't make sense to give a point estimate
  - Instead, we can give an interval.

    
Confidence intervals (2)
=========================================================
- So, let's get the interval that $\mu_{\bar{x}}$ is going to be in 95% of the time.
- We want something like this:

```{r, echo = F}
cord.x <- c(-3,seq(-1.96,1.96,0.01),1.96)
cord.y <- c(-3,dnorm(seq(-1.96,1.96,0.01)),0)
curve(dnorm(x,0,1),xlim=c(-3,3),main='Standard Normal')
polygon(cord.x,cord.y,col='skyblue')
```

Confidence intervals (3)
=========================================================
- Let's start with the standard normal distribution (**z-scores**)
- We want to get an interval that includes 95% of the area under the curve
  - That means we need to take off 2.5% on every side
  - For the left interval boundary, we want the x value that is greater than or equal to 2.5% of x values
  - ask Excel for the *z*-value: For this, we use the *inverse* of the standard normal distribution: `=NORM.S.INV(0.025)`
  - Result: `r qnorm(.025)`

Confidence intervals (4)
=========================================================
- For the right interval boundary, we want the x value that is greater than or equal to 97.5% of x values.
- ask Excel for the *z*-value: For this, we use the *inverse* of the standard normal distribution: `=NORM.S.INV(0.975)`
  - Result: `r qnorm(.975)`
- If you've done statistics before, these numbers should be pretty familiar to you.
- Generalising this to other normal distributions is easy:
$\bar{x} = \mu \pm 1.96 \times \sigma_{\bar{x}}$
- Replacing $\sigma_{\bar{x}}$ with the expression based on the population SD:
$\bar{x} = \mu \pm 1.96 \times \frac{\sigma}{\sqrt{n}}$

Exercise
=========================================================
- It is (for some strange reason) well-known that the amount of cat food a cat needs per day is normally distributed with a mean of 2 cans per day and an sd of .5. I'm planning to adopt two (completely random) cats and need to plan this move financially. 
- What's the maximum and the minimum amount of cat food cans I must expect to buy every day?
- This estimate should be fairly accurate and should only have a 10% chance of being wrong.
- Suppose I don't care about the minimum amount, I just want to know the maximum -- does that change anything?
- Suppose I'm adopting 3 cats instead of 2 -- does that change anything about my estimate?

Solution
=========================================================
- I'm drawing a random sample of hungry cats (sample size 2) from the population of hungry cats
  - How hungry? Mean = 2 cans/day, sd = .5 cans/day
  - I want a 90% CI for the mean of that sample
- Get the *z*-scores for the lower and the upper bound:
    - lower: `=NORM.S.INV(.05)` = `r qnorm(.05)`
    - upper: `=NORM.S.INV(.95)` = `r qnorm(.95)`


Solution (2)
=========================================================
- Calculate the CI:
  - lower limit: `=2 + NORM.S.INV(.05) * .5/sqrt(2)` = `r 2 + qnorm(.05) * .5/sqrt(2)`
  - upper limit: `=2 + NORM.S.INV(.95) * .5/sqrt(2)` = `r 2 + qnorm(.95) * .5/sqrt(2)`
- Those are some hungry cats!
- I need to plan on buying between `r 2 + qnorm(.05) * .5/sqrt(2)` and `r 2 + qnorm(.95) * .5/sqrt(2)` cans of cat food per day (per cat).

Plot it!
=========================================================
```{r, echo = F}
plot_normal_shaded_interval <- function(mean = 0, sd = 1, shade_from = -2, shade_to = 2, x_lower = mean - 2*sd, x_upper = mean + 2*sd, title = "Distribution", xlab = "x", ylab = "Density"){
  cord.x <- c(seq(shade_from, shade_to,0.01))
  cord.y <- c(dnorm(cord.x, mean = mean, sd = sd))
  cord.x[1] <- cord.x[2]
  cord.x[length(cord.x)] <- cord.x[length(cord.x) - 1]
  cord.y[1] <- 0
  cord.y[length(cord.y)] <- 0
  curve(dnorm(x, mean, sd),xlim=c(x_lower,x_upper),main= title,xlab = xlab, ylab = ylab)
  polygon(cord.x,cord.y,col='skyblue')
  }
plot_normal_shaded_interval( mean = 2, sd = .5, shade_from = 2 + qnorm(.05) * .5/sqrt(2), shade_to = 2 + qnorm(.95) * .5/sqrt(2), title = "Distribution of cat hunger", xlab = "Cans per day", ylab = "Probability density") 
```

Solution (4)
=========================================================
- If I only care about the maximum, I don't need the lower limit.
- I can use a different upper limit to get an interval that delimits 90% of the area under the curve.
    - upper limit: `=2 + NORM.S.INV(.90) * .5/sqrt(2)`
- Those are still some hungry cats!
- I need to plan on buying at most `r 2 + qnorm(.90) * .5/sqrt(2)` cans of cat food per day (per cat).

Plot it again!
=========================================================
```{r, echo = FALSE}
plot_normal_shaded_interval(2, .5, 0, 2 + qnorm(.90) * .5/sqrt(2)) 
```

Solution (5)
=========================================================
- What if I'm getting 3 cats?
- upper limit: `2 + NORM.S.INV(.90) * .5/sqrt(3)`
- Result: `r 2 + qnorm(.90) * .5/sqrt(3)`
- Why is it less?
- The chances of getting 3 out of 3 very hungry cats are lower than the chances of getting 2 out of 2 very hungry cats (of course, these figures are per cat, so I'll still have to buy a ridiculous amount of food).


Now reverse the idea
=========================================================
- Usually, we have no other information about a population but the sample we just collected.
- For example, let's say the sample mean is `r (sample.mean <- 0)` and the sample SD is `r (sample.sd <- 1)`. Apart from this, we know nothing about the population.
- Can we compute a CI for the sample mean?
- Sure enough we can, but it gets a little more complicated.
  - (who would have thought?)


What do these numbers mean?
===========================================================
- Anything, really.
- But let's imagine that these numbers are from a survey of student's attitudes towards their Advanced Statistics class.
  - Imagine that they could give a rating from -3 ("This is the worst class ever and I want the instructor fired!") to 3 ("This is the best class I've ever taken! I'm going to make so much money with my new R skills!"), with 0 representing a neutral feeling ("It's alright. At least it will be over soon").
  - In this case, most students are pretty neutral about the class, but some really love it and some really hate it. 

Computing a CI from the sample mean (story time)
==========================================================
- Consider the following scenario:

> I have collected 10 responses to my class evaluation (the other students never turned their forms back in). The mean of the responses is 0 (apathy) and the sd is 1. Given that these 10 responses are just a small sample of the population, and that the population I'm really interested in is the population of all current and future Adv Stats students, is there anything I can say about the true population mean? Can I at least conclude that students didn't absolutely hate this class?


Computing a CI from the sample mean
==========================================================
- We'll have to estimate both the population mean and the population variance.
  - We have already estabished that the sample mean is a good estimator for the population mean.
- What about the sample sd ($s$)? Is it a good estimator for the population sd ($\sigma$)?
- Or the equivalent question: is sample variance ($s^2$) a good estimator of population variance ($\sigma^2$)?
- We just tackled this in the last lecture analytically
  - Today, we can take it easy and just simulate!
- If you haven't done it before, now is a really good time to watch (at the very least) the first 7 minutes of [Julian's video](https://www.youtube.com/watch?v=Juo5NJSHlMM) (see myBU).


Population variance and sample variance: plots
===========================================================
- Taking samples of size 2 from the standard normal distribution: $X \sim N(0,1)$ and calculating the variance:

```{r, echo = FALSE}
nsim = 1000
run_variance_simulation <- function(sample_size = 2, number_of_simulations = 1000, population_mean = 0, population_sd = 1)
  {

sample_variances <- replicate(number_of_simulations, var(rnorm(n = sample_size, mean = population_mean, sd = population_sd)))
}

#Define a new plot function so that the plot titles are correct
make_variance_hist_and_plot <- function(sample_variance){
  par(mfrow=c(1,2)) # 
  hist(sample_variance,freq=F, breaks = 30, main = "Sample variance")
  plot(density(sample_variance), main = paste("Mean = ", round(mean(sample_variance),2) , "SD = ", round(sd(sample_variance),2)))} 

make_variance_hist_and_plot(run_variance_simulation(sample_size = 2, number_of_simulations = nsim, population_mean = 0, population_sd = 1))
```


Population variance and sample variance: plots
===========================================================
- Taking samples of size 4 from the standard normal distribution: $X \sim N(0,1)$ and calculating the variance:

```{r, echo = FALSE}
make_variance_hist_and_plot(run_variance_simulation(sample_size = 4, number_of_simulations = nsim, population_mean = 0, population_sd = 1))
```


Population variance and sample variance: plots
===========================================================
- Taking samples of size 10 from the standard normal distribution: $X \sim N(0,1)$ and calculating the variance:

```{r, echo = FALSE}
make_variance_hist_and_plot(run_variance_simulation(sample_size = 10, number_of_simulations = nsim, population_mean = 0, population_sd = 1))
```

Population variance and sample variance: plots
===========================================================
- Taking samples of size 100 from the standard normal distribution: $X \sim N(0,1)$ and calculating the variance:

```{r, echo = FALSE}
make_variance_hist_and_plot(run_variance_simulation(sample_size = 100, number_of_simulations = nsim, population_mean = 0, population_sd = 1))
```

Sample variance as an estimator of population variance
===========================================================
- Looks like sample variance (at least if we calculate it dividing by $n-1$ instead of $n$ is a pretty good estimator (unbiased actually)
- We can plug the sd of the sample into the equation for the SD of the sampling distribution (or rather, the standard error):
$$
\begin{aligned}
SE_{\bar{x}} = \frac{s}{\sqrt{n}}
\end{aligned}
$$
(Note that we are ignoring the question if the relationship between $s$ and $s^2$ is really the same as the relationship between $\sigma$ and $\sigma^2$. Feel free to simulate that, if you are really curious.)
- But as you saw in Julian's video, the estimate of $\sigma$ from $s$ is sometimes quite far away from the correct $\sigma$, especially for small sample sizes.
- This means that our estimate for $\sigma$ is going to vary. Its accuracy will depend on the sample size.

The chi-square distribution
============================================================
- But there's another striking thing going on here. Look again at the distribution of variances for sample size 2:

```{r, echo = FALSE}
make_variance_hist_and_plot(run_variance_simulation(sample_size = 2, number_of_simulations = nsim, population_mean = 0, population_sd = 1))
```
- This is definitely not a normal distribution!

The chi-square distribution
============================================================
- What is a variance again?
    - Definition: $s^2 = \frac{\sum\limits_{i=1}^{n}(x_i - \mu)^2}{n-1}$
    - If we have a standard normal distribution ($\mu = 0$ and $\sigma = 1$): $s^2 = \frac{\sum\limits_{i=1}^{n}z_i^2}{n-1}$
    - If $n = 2$: $s^2 = \sum\limits_{i=1}^{n}z_i^2$
    - The $\chi_1^2$ distribution is the distribution of the square of a random variable following the standard normal distribution
        - i.e. squares of *z*-values are $\chi_1^2$ distributed
        
Chi-square distributions
============================================================
- There is more than one $\chi^2$ distribution:
    - The sum of the squares of two **independent**, squared random variables following the standard normal distribution (i.e. *z*-values) follows the $\chi_2^2$ distribution: $\chi_2^2 = z_1^2 + z_2^2$
    - In general, $\chi_n^2 = \sum\limits_{i=1}^{n}z_i^2$
      - Here, $n$, the number of independent $z^2$ variables is also known as the **degrees of freedom** of the $\chi^2$ distribution.
      
Chi-square distributions plotted
===========================================================
```{r, echo = F}
library(ggplot2)
x <- seq(from = 0.1,by = .01,to = 20)
ggplot(data.frame(x = x), aes(x = x)) + 
  stat_function(fun = dchisq, args=list(df = 1), aes(linetype = "df = 1")) +
  stat_function(fun = dchisq, args=list(df = 2), aes(linetype = "df = 2")) +
  stat_function(fun = dchisq, args=list(df = 4), aes(linetype = "df = 4")) +
  stat_function(fun = dchisq, args=list(df = 10), aes(linetype = "df = 10")) +
  scale_linetype_discrete(limits = paste("df", "=", c(1,2,4,10)))+ labs(linetype = NULL, y = "f(x)")
```

What can we do with chi-square?
============================================================
- We can approach our dice problem in a different way
- Instead of looking at the sample means, we can look at the dice roll results directly
- These come from a distribution called the **multinomial** distribution
- Let's start with coin flips though, because that way we can use the **binomial** distribution

The binomial distribution
============================================================
- This is the distribution of number of successes in a sequence of n independent yes/no experiments
- Definition: $$f(X = k|n, p) = \binom{n}{k}\cdot p^k\cdot (1-p)^{n-k}$$
    - Where $k$ is the number of successes (e.g. number of heads), $n$ is the total number of experiments (coin flips), and $p$ is the probability of the success (e.g. 0.5 for a fair coin).
- You almost definitely did this in school, but we won't go into the details of this distribution much. Instead, we'll just look at what happens when we increase the sample size 

Plotting the binomial distribution
============================================================
```{r echo = F}
par(mfcol=c(3, 1))
p = .5
for(n in c(5,10,60))
{
    x <- dbinom(0:(n), size=n, p=p)
    barplot(x, names.arg=0:(n), space=0, main=paste('n = ',n,", p = ",p,sep=''), xlab = "Number of successes (X)", ylab = ("p(X)"))
}
```

Binomial and normal distribution
===========================================================
- For large sample sizes, the binomial distribution approximates the normal distribution
- Because of this, there is an easy way of calculating a *z*-value (or rather, the square of a *z*-value -- I'll spare you the proof, but ask me if you're interested): 
    - First, get the frequencies of successes $f_{o(1)}$ and non-successes $f_{o(2)}$.
        - For example, if you had $f_{o(1)} = 40$ times Heads and $f_{o(2)} = 60$ times Tails, can we conclude that the coin is not fair?
    - Then get the expected frequencies given the null hypothesis. If we have a fair coin, our p(Heads) should be .5, so we're expecting $f_{e(1)}=50$ times Heads and $f_{e(2)}=50$ times tails.

The chi-square test
===========================================================
- If the sample size is large enough (more than 10 per category), the binomial distribution approximates the normal distributions and the squared differences between the observed ($f_{o(j)}$) and the expected ($f_{e(j)}$) are $z^2$-values (again, if you want to know why, I can tell you).
$$z^2 = \chi_1^2 = \frac{\sum\limits_{j = 1}^{n}(f_{o(j)}-f_{e(j)})^2}{f_{e(j)}}$$

The chi-square test (2)
===========================================================
- In this case, we have two groups (Heads and Tails), so $n = 2$. We can rewrite the sum as:
$$z^2 = \chi_1^2 = \frac{(f_{o(1)}-f_{e(1)})^2}{f_{e(1)}} + \frac{(f_{o(2)}-f_{e(2)})^2}{f_{e(2)}}$$
- Plug in our values ($f_{o(1)} = 40$, $f_{o(2)} = 60$, $f_{e(1)} = f_{e(2)} = 50$):
$$z^2 = \chi_1^2 = \frac{(40-50)^2}{50} + \frac{(60-50)^2}{50} = \frac{100}{50} + \frac{100}{50} = 4$$
- We can look up the probability of getting a value this extreme based on the $\chi^2$-value: `=1-CHISQ.DIST(4,1,TRUE)`, which is `r 1-pchisq(4,1)`
- Conclusion: if the null hypothesis (fair coin, p(H) = .5) is true, we would expect to find an outcome like H: 40, T:60 in less than 5% of samples.

Degrees of freedom
============================================================
- Wait, what is the `1` in `=1-CHISQ.DIST(4,1,TRUE)`?
    - That's the degrees of freedom. Remember, the degrees of freedom are the number of independent $z^2$ variables we are summing up.
    - Why only one, when we are summing two terms?
$$z^2 = \chi_1^2 = \frac{(f_{o(1)}-f_{e(1)})^2}{f_{e(1)}} + \frac{(f_{o(2)}-f_{e(2)})^2}{f_{e(2)}}$$
- In this expression, the second term is determined by the first, since $f_{o(2)} = n - f_{o(1)}$.
    - There is only one term that can vary freely, hence $df(\chi^2) = 1$.
    
Generalising the chi-square test
============================================================
- Why use $\chi^2$ here at all, when we could just take the square root and do a *z*-test?
- The answer is that this whole principle generalises to the **multinomial** distribution, i.e. cases where we have more than two groups.
- In the multinomial distribution, we have more than $n=2$ groups, but the general equation stays the same:
$$\chi_{n-1}^2 = \frac{\sum\limits_{j = 1}^{n}(f_{o(j)}-f_{e(j)})^2}{f_{e(j)}}$$    
- Our $\chi^2$ is distributed with $n-1$ degrees of freedom, where $n$ is the group size.

Try it
=============================================================
- The following table is from a dice roll experiment. Use the $\chi^2$ test to decide whether the die was fair or not.
```{r echo = F,results='as.is'}
library(knitr)
set.seed(238239)
dice_table <- data.frame(table(sample(1:6,100,replace = TRUE)))
names(dice_table) <- c("$x_i$", "$f_{o(i)}$")
kable(dice_table)
```

More fun things to do with chi-square
============================================================
- Remember, we still have the issue of usually not knowing anything at all about the true population mean $\mu$ and the true population standard deviation $\sigma$
- Instead, we have to estimate them using the sample mean $\bar{x}$ and the sample standard deviation $s$.
    - Both of this is not a problem at high sample sizes (as you can see very clearly in Julian's video and in our simulations here)
    - But we need a way to account for $s$ being less accurate at low sample sizes.
    
Solution: The t-distribution
=============================================================
- If we divide a *z*-value by the square root of an *independent* $\chi^2$ value divided by n, we get a *t*-value: $$t_n = \frac{z}{\sqrt{\chi_n^2/n}}$$
- The *t*-value has **degrees of freedom** as well -- it inherits them from the $\chi^2$ value in its denominator.
- Practically, the denominator makes the distribution have "heavier" tails -- exactly what we need for our problem.

Let's plot some t-distributions
==============================================================

```{r, echo = F}
library(ggplot2)
x <- seq(from = -5,by = .01,to = 5)
ggplot(data.frame(x = x), aes(x = x)) + 
  stat_function(fun = dt, args=list(df = 1), aes(linetype = "df = 1")) +
  stat_function(fun = dt, args=list(df = 2), aes(linetype = "df = 2")) +
  stat_function(fun = dt, args=list(df = 4), aes(linetype = "df = 4")) +
  stat_function(fun = dt, args=list(df = 10), aes(linetype = "df = 10")) +
  scale_linetype_discrete(limits = paste("df", "=", c(1,2,4,10)))+ labs(linetype = NULL, y = "f(x)")
```

The t-distribution vs. the normal distribution
============================================================
- Solid = normal distribution, dashed = *t*-distribution

```{r, echo = F}
## plot multiple figures:
## replace ugly par... specification with 
## something easier to remember:
multiplot <- function(row,col){
     par(mfrow= c(row,col), pty = "s")
   }

range <- seq(-4,4,.01)  
 
multiplot(2,2)

 for(i in c(2,5,15,20)){
   plot(range,dnorm(range),type="l",lty=1,
        xlab="",ylab="",
        cex.axis=1)
   lines(range,dt(range,df=i),lty=2,lwd=1)
   mtext(paste("df=",i),cex=1.2)
 }
```

The t-test
===============================================================
- Solution: assume that the sample means aren't normally distributed, but rather *t*-distributed
- Why *t*?
  - The *t*-distribution is like the standard normal distribution, but it has an additional parameter that we call df (for degrees of freedom, but don't worry about the name yet).
  - The higher df, the closer the *t*-distribution is to the standard normal distribution
  - For lower df, the *t*-distribution has "heavy tails", meaning that it's wider
    - This reflects greater uncertainty.

t as a test statistic
==========================================================
- Once again, the mathematical proof of this would take too long, but you can show that $$t_{n-1} = \frac{\bar{x} - \mu_0}{\hat{\sigma}_{\bar{x}}}$$, where $\mu_0$ is the mean according to the null hypothesis and $\hat{\sigma}_{\bar{x}}$ is the estimate of the standard error of the mean (based on the sample standard deviation) is *t*-distributed with $df = n-1$ degrees of freedom.

Back to my little example
==========================================================
- Consider the following scenario:

> I have collected 10 responses to my class evaluation (the other students never turned their forms back in). The mean of the responses is 0 (apathy) and the sd is 1. Given that these 10 responses are just a small sample of the population, and that the population I'm really interested in is the population of all current and future Adv Stats students, is there anything I can say about the true population mean? Can I at least conclude that students didn't absolutely hate this class?

Computing the 95% CI
===========================================================
- Using the *t*-distribution, we can compute CIs from samples as follows: get the lower and upper bounds from the *t*-distribution (which one depends on the sample size, e.g. in this we have $n = 10$, so we will use a *t*-distribution with $df = n-1 = 9$):
- Lower bound (remember, we want to exclude the extreme low 2.5%): `=T.INV(0.025, 9)` (where 9 is the df)
    - Result: `r qt(.025, 9)`
- Upper bound (remember, we want to exclude the extreme high 2.5%): `=T.INV(0.975, 9)` (where 9 is the df)
    - Result: `r qt(.975, 9)`
- No surprise: the *t*-distribution is symmetrical
    
Computing CIs
==========================================================
- Then take the upper and lower bounds and compute the CIs as follows:
$\bar{x} = \mu_{\bar{x}} \pm 2.262 \cdot \frac{s}{\sqrt{n}}$
- Remember, we estimated $\mu_{\bar{x}}$ using the sample mean (in our example, $\bar{x} = 0$) and the population variance $\sigma$ using the sample standard deviation (in our example, $s = 1$).
- CI: $0 \pm 2.262 \cdot \frac{1}{\sqrt{10}} = 0 \pm .7153$
- Lower bound: -.7153
- Upper bound: .7153

Back to our example
===========================================================
- Hey, there is a good chance that my current and future students don't absolutely hate me (yet)! 
  - The lowest mean in the CI is -.7153, which maybe translates to "apathetic but slightly worried."
- But they don't love me either:
  - The highest mean in the CI is .7153, which maybe translates to "apathetic but slightly hopeful."
- Of course, the true mean is actually outside 5% of the intervals calculated like this.

What does the CI of the sample mean mean? (sorry)
===========================================================
- Remember, we are reversing the idea that the sample mean has a 95% probability to be within the 95% confidence interval around the population mean.
- When we calculate a 95% CI from a *sample* this **DOES NOT MEAN** that there is a 95% probability that the population mean is within this 95% CI.
- The true mean either is or is not in this particular CI.
- Rather, it means that if you take a lot of samples and compute the CI around the sample mean, 95% of those CIs will contain the true population mean.
- In other words, the CI bounds are random variables, but the population mean isn't.
- (In Bayesian statistics, you can actually get something equivalent to the first definition -- a 95% credible interval.)

Let's test this
==========================================================
- Let's get 10 samples from a normal distribution, then get CIs from them and see how often they contain the true mean.
- We'll do this in class.
- Spoiler:
  - The proportion of CIs that does not contain the true mean is larger than 5%! This is because the normal distribution is narrower than the *t*-distribution at low dfs.
  - Be **very** careful! If you *think* you have a 95% CI, but you actually have a 90% CI or worse, you are prone to making errors in interpreting the results.
    - Horrible, money-wasting, science-distorting, extermely expensive errors!

Hypothesis tests
=========================================================
- In a way, by calculating the CI we already have a way to test hypotheses
- Let's say we got a 95% CI from our sample with a lower bound of 2 and an upper bound of 3.
- Let's use the simplest null hypothesis possible
  - Null hypothesis: the mean of the population that the sample came from is 0
  - $H_0: \mu = 0$
  - Given the 95% CI above, can we reject the null hypothesis?
    - And if so, what is the chance that we're wrong?
  - Answer: Yes, we can, since 0 is not part of the CI.
    - There is the possibility that we are wrong, though, since only 95% of the CIs will contain the true population mean.
    - This is called the type I error, and its probability here (called $\alpha$)is 5%.

Example
===========================================================
- Remember my survey? The CI did not contain -3, so I can conclude (with an $\alpha$ of 5%), that the average member of the population of current and future Adv Stats students attitude towards me is not intense hatred. Relief!

Two-tailed t-tests
============================================================
- Instead of computing the CI from the *t*-value, we can also just take the *t*-value itself as a measure of how far the sample mean is away from the mean specified in the null hypothesis.
- We can determine a critical *t*-value $t_{crit}$ depending on our $\alpha$ criterion and the df. For example, for a df of 9, $t_{crit}$ for the upper bound is `=T.INV(.975,9)`, which gives us`r qt(.975, df = 9)` and $t_{crit}$ for the lower bound is `=T.INV(.025,9)`, which gives us`r qt(.025, df = 9)`
- Note that the *t*-distribution is symmetrical
In short, if $t \ge |t_{crit}|$, we can reject the null hypothesis.

What about one-tailed t-tests?
===========================================================
- If we are absolutely sure of the direction of the effect, then we could use a *t*-test that only rejects the null hypothesis when the *t*-value is greater than $t_{crit}$ or if it is smaller than $t_{crit}$ (depending on what direction we want to test for).
- In this case, our $t_{crit}$ can be a little closer to 0, since the entire 5% rejection area is in one tail only: `=T.INV(.95,9)`, which gives us `r qt(.95, df = 9)`.
- But be careful, if the effect is in the wrong direction (even if it's ridiculously strong in the wrong direction), we can't reject the null hypothesis with that test.
- This is one of the weird cases in null hypothesis significance testing (NHST) where our intentions can determine the results of the test. Bayesian statisticians are right to complain about this.

Example
===========================================================
> I'm trying a new type of medication to help insomniac patients sleep better. Each of my 5 patients reports how much longer (or shorter) they have been sleeping (in hours) after taking the medication compared to before. The numbers are below. Based on this, can I conclude that the medication has changed my patients' sleep? Or are the variations that the patients observed random and unrelated to the intervention?

```{r, echo = FALSE}
(sleep_times <- round(rnorm(5,1,1),2))
```
Your turn. What is the null hypothesis?

Example solution
===========================================================
The $H_0$ is that the true mean of the population is 0.
```{r}
t.test(sleep_times)
```
-If p $\le$ .05: reject the null hypothesis.
- Try this in SPSS!

Power simulations
==========================================================
- For simple (and even more complex) designs, you can compute power analytically. I will show you how to do this using a program called GPower.
- But simulations are a lot more intuitive!
- Let's go back to the sleep example and assume that the true mean was 1 (that means that on average, people get one hour more sleep when using the medication) and the sd was 1.

Power simulations plot
==========================================================
- Remember, $t_{crit} = `r qt(.975, 4)`$

```{r, echo = FALSE}
t_test_sim <- function(n, mean = 1, sd = 1){
  t_results <- t.test(rnorm(n, mean, sd))
  t_results$statistic}
simulation_results <- replicate(1000, t_test_sim(5, 1, 1))
hist(simulation_results, main = paste0("Proportion of |t| > ", round(qt(.975, 4),2), ": ", sum(abs(simulation_results) > qt(.975, 4),2)/length(simulation_results)), 100)
```

Not so great!

How to increase power
=======================================================
- Let's try a higher true mean ($\mu = 2$):

```{r, echo =F}
simulation_results <- replicate(1000, t_test_sim(5, 2, 1))
hist(simulation_results, main = paste0("Proportion of |t| > ", round(qt(.975, 4),2), ": ", sum(abs(simulation_results) > qt(.975, 4),2)/length(simulation_results)), 100)
```

How to increase power (2)
=======================================================
- The standard deviation (i.e. the noise) in the population is lower (people don't vary as much in their response to the medication)
- Let's try a true value of $\sigma = 0.5$

```{r, echo = F}
simulation_results <- replicate(1000, t_test_sim(5, 1, .5))
hist(simulation_results, main = paste0("Proportion of |t| > ", round(qt(.975, 4),2), ": ", sum(abs(simulation_results) > qt(.975, 4),2)/length(simulation_results)), 100)

```

How to increase power (realistically!)
=======================================================
- You don't really have any direct control over population mean (i.e. effect size) or sd (i.e. noise). Let's focus on the one variable that you do have control over.
- The sample size is larger: let's try $n=10$

```{r, echo = F}
simulation_results <- replicate(1000, t_test_sim(10, 1, 1))
hist(simulation_results, main = paste0("Proportion of |t| > ", round(qt(.975, 4),2), ": ", sum(abs(simulation_results) > qt(.975, 4),2)/length(simulation_results)), 100)
```

Double-checking our results
=========================================================
- Let's just check analytically that we have this correctly: If we want to show in the one-sample *t*-test that a mean of 1 is different from 0 (when $sd = 1$), we need about 10 subjects. I will show you how to use GPower for that.

Setting yourself up for success (or failure)
=========================================================
- You don't want to run an underpowered study. Most likely, you'll get a null result that tells you nothing about the true state of the world.
- How can you avoid this? 
  - Run a realistic number of participants so you reach acceptable power (the APA recomments .8).


Exercise
========================================================
>An experimenter knows for a fact that the average number of friends people have on Facebook is 70, with an sd of 10. She knows this because she works for Facebook and has access to all your personal data. The experimenter wants to know if people who post lots of photos of cats have more or fewer friends than the average Facebook user. Automatically tagging cat photos is hard, so our experimenter just asks an unpaid intern to compile a sample of 100 cat-posting people and find out their friend numbers. How big does the effect (in friends gained/lost) have to be so it would be detectable at an acceptable power level of .8?

Solution
========================================================
- Effect size is defined as
$$d = \frac{\mu_1 - \mu_2}{\hat{\sigma}}$$
- We're doing a t-test where we want to know if the group mean (for a group size of $n=100$) comes from a known population ($\mu = 70, \sigma = 10$)
- We want to find the necessary effect size given the sample size and the standard error of the mean. The test would be two-tailed, since we don't know the direction of the effect. The power we want is $(1-\beta) = .8$

In GPower
========================================================
- In GPower, select `t tests` as `Test family` and `Means: Difference from constant (one sample case)` as `Statistical test`. As `Type of power analysis`, select `Sensitivity: Compute required effect size`
- In the `Input Parameters` area, select `Two` for `Tails`, leave the $\alpha$ at `.05`, set the `Power` to `0.8`, and set the `Total sample size` to `100`. You get an effect size of $d = `r power.t.test(n = 100, sd = 1, power = .8, type = "one.sample", alternative = "two.sided")$delta`$
- A difference in as little as $d \cdot \sigma = .283 \cdot 10=  2.83$ friends would be detectable.

Don't cheat!
========================================================
- How about the following strategy? 

> Just run the hypothesis test on the data after every new sample and stop as soon as you get a significant result.

- Let's see just what happens to $\alpha$ if you do that.
- Run a simulation where there is no effect (i.e. where we know the $H_0$ is true)

```{r, echo = FALSE}  
t_test_cheating_sim <- function(n_max = 30, n_increments = 2, sd = 1){
  samples <- NULL
  significant <- FALSE
  
  while(length(samples) <= n_max & significant == FALSE){
      samples <- c(samples, rnorm(n_increments, mean = 0, sd = sd))
      significant <- t.test(samples)$p.value <= .05
    }
  return(significant)
  }
```

The consequences of cheating
======================================================
Let's run this simulation 1000 times and make a plot with the results:

```{r, echo = F}
simulation_results <- replicate(1000, t_test_cheating_sim(n_max = 30, n_increments = 2, sd = 1))
barplot(table(simulation_results), names.arg = c("no","yes"),xlab= "Significant", ylab = "Number of simulations", main = paste0("Proportion of significant results: ", sum(simulation_results)/length(simulation_results)), 100)
```

The consequences of cheating (2)
======================================================
- Holy inflated Type I error rate, Batman!
  - $\alpha$ is at 25%, instead of 5% where it should be.
- Unfortunately, this strategy of using stopping rules ("data peeking") is quite common.
  - Solution: do a power analysis, set your sample size beforehand, and stick to it!

Testing more interesting hypotheses
==========================================================
- So far, we have been testing the null hypothesis that our sample mean is 0.
- This is not what we usually do in Psychology.
- Instead, we want to know if there is a significant difference between the means of two (or more) samples.
  - For example, you might give only one group an intervention against anxiety, with the other one serving as the control.
    - Does the intervention work? 
      - Do people in the treatment group report lower anxiety? 
      - Can we generalise this to the population?
      - Should we use this intervention in clinical practice?
  - A lot of effort and money may be wasted if you get these questions wrong.
  
The two-sample t-test
==========================================================
- Remember, we are comparing two samples now. We'll call the sample means $\mu_1$ and $\mu_2$.
- Our null hypothesis is $H_0: \mu_1 = \mu_2$
- We can rephrase this as $H_0: \mu_1 - \mu_2 = \delta = 0$
- We already know the logic of this: we just want to find out if $d$ ($\delta$ = true population difference, $d$ = sample difference)is extreme enough so we can reject the $H_0$.
- Let's see how $\delta$ is distributed.
- We could do this analytically, using the things we've learned about expected values, but I'll leave that to those of you who are really really interested and just give you the end results.

The two-sample t-test (4)
=========================================================
- We're looking for the distribution of sample differences $x_1 - x_2$:
- If we knew the population standard deviation, we could use the following formulas:
$$
\begin{aligned}
\mu_{\bar{x}_1 - \bar{x}_2} &= \mu_1 - \mu_2\\
\hat{\sigma}_{\bar{x}_1 - \bar{x}_2} &= \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}
\end{aligned}
$$
- Based on this, we could calculate CIs or just a *z*-value
- Remember our $H_0: \mu_1 - \mu_2 = 0$ 
- The *z*-value would then be:
$z = \frac{(\bar{x}_1 - \bar{x}_2) - (\mu_1- \mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}} = \frac{(\bar{x}_1 - \bar{x}_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}$

- (since our null hypothesis is that $\mu_1 - \mu_2 = 0$)

The two-sample t-test (5)
==========================================================
- Of course, in real life we don't know the population sd
- So we have to estimate it using $s^2$
- This would be a *t*-value, not a *z*-value

$t = \frac{(\bar{x}_1 - \bar{x}_2)}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$

- Only problem: what is the df of that test? If the variances are equal, it's easy: $df = n_1+n_2-2$
  - There are some shortcuts that we can take if the sample sizes and population variances are the same, but is there a general solution?
- We could just use the lower of the sample sizes, but this will cost us power
- This was actually a big problem in statistics, but B. L. Welch found an approximate solution (called *Welch's t-test*)
- You can look the details up on Wikipedia, but SPSS knows them and will apply them automatically.

The dependent t-test for paired samples
==========================================================
- This is actually a lot easier. Since we have two samples per person/group/analysis unit, we can simply compute the differences between measurements and then use the one-sample *t*-test to check if they are 0.
- First, we calculate the mean and the standard deviation of our sample of $n$ difference values $d_i = x_{i1}-x_{i2}$:
$$ 
\begin{aligned}
\hat{\mu}_d = \bar{d} &= \frac{\sum\limits_{i=1}^{n}d_i}{n}\\
\hat{\sigma}_d = s_d &= \sqrt{\frac{\sum\limits_{i=1}^{n}(d_i - \bar{d})^2}{n-1}} \\
\hat{\sigma}_{\bar{d}} &= \frac{\hat{\sigma}_d}{\sqrt{n}} = \frac{\hat{s}_d} {\sqrt{n}}\\
                      &= \frac{\sqrt{\frac{\sum\limits_{i=1}^{n}(d_i - \bar{d})^2}{n-1}}}{\sqrt{n}}
\end{aligned}
$$
- Here, $n$ is the number of sample *pairs*


The dependent t-test for paired samples (2)
==========================================================
- Then we can calculate the *t*-value:
$$t = \frac{\bar{d} - \mu_d}{\hat{\sigma}_d}$$, where $\mu_d$ is the population mean for the difference given that the $H_0$ is true. If the $H_0$ is that both samples are the same ($\mu_d = 0$), this simplifies to
$$t = \frac{\bar{d}}{\hat{\sigma}_d}$$
-We can estimate the standard error of the difference mean $\hat{\sigma}_{\bar{d}}$ from the standard deviation of the difference: $$\hat{\sigma}_{\bar{d}} = \frac{s_d}{\sqrt{n}}$$
    -Plugging this into the equation for *t*, we get: $$t_{n-1} =  \frac{\bar{d}}{\frac{s_d}{\sqrt{n}}}$$
- The resulting *t*-value will have a df of $n-1$, where $n$ is the number of sample pairs.
- Since the sd of the differences will be a lot lower than the overall sd, the power of this test is quite a bit higher.
